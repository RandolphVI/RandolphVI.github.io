<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>黃某人</title>
  
  <subtitle>痴</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://randolph.pro/"/>
  <updated>2019-03-17T10:10:30.890Z</updated>
  <id>http://randolph.pro/</id>
  
  <author>
    <name>Randolph</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>✎「匯文」 我与地坛</title>
    <link href="http://randolph.pro/2019/03/16/%E2%9C%8E%E3%80%8C%E5%8C%AF%E6%96%87%E3%80%8D%E6%88%91%E4%B8%8E%E5%9C%B0%E5%9D%9B%20copy/"/>
    <id>http://randolph.pro/2019/03/16/✎「匯文」我与地坛 copy/</id>
    <published>2019-03-15T16:00:00.000Z</published>
    <updated>2019-03-17T10:10:30.890Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm8.staticflickr.com/7877/33525214788_14d64fc575_o.jpg" alt></p><h1 id="我与地坛"><a href="# 我与地坛" class="headerlink" title="我与地坛"></a>我与地坛 </h1><p> 北平玉兰花虽然未开 <br> 但难得今日玻璃晴朗，橘子辉煌</p><p>便打算去地坛一览</p><p>想起年少时读史铁生先生「我与地坛」<br>是真真切切感受到了一个文人原来是可以同时拥有细腻而强烈的笔触的</p><p>我至今还可以想象得到当时文中句子所给我带来的震撼 <br> 并且每段文字所予以都是不同的感受</p><p>「就命运而言，休论公道。」</p><p>先生在他最狂妄的年纪被命运所抛弃，地坛包容了他，先生在地坛思考人生，思考罪孽与福祉 <br> 最后他找到了他的那杆枪，迎着不公的世界，开始了他的宣言</p><p>我驻足在地坛公园的中心，向着四周望去 <br> 地坛中人来人往，风从树林穿过，有孩童在或近或远处嬉戏，仿佛神灵也置身于此</p><p>「我已不在地坛，地坛在我」</p><p>地坛予以了先生对世界命数思考的能力 <br> 先生如同他所说的那般，已不在地坛，也不在这个世上了 <br> 但先生的人格会同地坛这般与他永在</p><p>「要是有些事我没说，地坛，你别以为是我忘了，为什么也没忘，但是有些事只适合收藏。不能说，也不能想，却又不能忘。」</p><p>年少读时不得其意，如今阅历稍长也变得信佛迷事，倒是听懂了先生写时的那份平静</p><p>如今先生已去，很久没有再读过了，在此缅怀先生</p>]]></content>
    
    <summary type="html">
    
      己亥年二月初十于北平地坛所记。
    
    </summary>
    
      <category term="匯文" scheme="http://randolph.pro/categories/%E5%8C%AF%E6%96%87/"/>
    
    
      <category term="匯文" scheme="http://randolph.pro/tags/%E5%8C%AF%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>✎「创作」 昆虫白与舞茉莉</title>
    <link href="http://randolph.pro/2018/10/26/%E2%9C%8E%E3%80%8C%E5%88%9B%E4%BD%9C%E3%80%8D%E6%98%86%E8%99%AB%E7%99%BD%E4%B8%8E%E8%88%9E%E8%8C%89%E8%8E%89/"/>
    <id>http://randolph.pro/2018/10/26/✎「创作」昆虫白与舞茉莉/</id>
    <published>2018-10-25T16:00:00.000Z</published>
    <updated>2019-03-18T02:54:17.861Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="昆虫白与舞茉莉"><a href="# 昆虫白与舞茉莉" class="headerlink" title="昆虫白与舞茉莉"></a>昆虫白与舞茉莉 </h1><p> 茉莉睁大了眼睛，冲着你，突然眯成了一条缝，一本正经地对你说道：「诶，你觉得人活着是个悲剧吗，如果是个悲剧，那么人生的意义是什么啊？」</p><p>你很诧异，放下手中的打鼓棍，鼓声也骤然停止。转而，你开始认真的思考着，「尼采不是说过所谓悲剧意义，是深知人与自然比微不足道…」</p><p>你还未说完，就被茉莉打断，她嘟着个嘴，像似不满意或者说她不想听你引经据典，长篇大论泛泛而谈。</p><p>不过你倒是不急不忙，希望能够耐心地讲完，「但是人在这样的基础上坚持，但也人因为有生命有创造力可以和自然一样伟大，人因为这样的无法改变的宿命悲剧，却衍生出了伟大，而伟大又诞生了美。」</p><p>茉莉明显地不开心了，把手插进牛仔衣的口袋，依靠在沙发上，「什么伟大啊什么美的，这跟我们有什么关系，况且…」，这次轮到你打断了茉莉的抱怨，突然奏起的鼓声将两人从各自的世界拉向了不知何处的远方。</p><p>茉莉倒也不再闹腾，胸口的那朵石茉莉刺青随着鼓声或者说是心跳一同起伏，仿佛跳脱于虚无，变得坚韧可爱起来。</p>]]></content>
    
    <summary type="html">
    
      戊戌年九月十八于合肥所作。
    
    </summary>
    
      <category term="创作" scheme="http://randolph.pro/categories/%E5%88%9B%E4%BD%9C/"/>
    
    
      <category term="创作" scheme="http://randolph.pro/tags/%E5%88%9B%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>✎「创作」 吃不胖的女孩</title>
    <link href="http://randolph.pro/2018/10/11/%E2%9C%8E%E3%80%8C%E5%88%9B%E4%BD%9C%E3%80%8D%E5%90%83%E4%B8%8D%E8%83%96%E7%9A%84%E5%A5%B3%E5%AD%A9/"/>
    <id>http://randolph.pro/2018/10/11/✎「创作」吃不胖的女孩/</id>
    <published>2018-10-10T16:00:00.000Z</published>
    <updated>2019-03-18T02:58:39.481Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="吃不胖的女孩"><a href="# 吃不胖的女孩" class="headerlink" title="吃不胖的女孩"></a>吃不胖的女孩 </h1><h2 id="一"><a href="# 一" class="headerlink" title="一"></a> 一</h2><p>「你真的分清了占有欲 喜欢 爱和新鲜感吗 还是不甘心” 两小时前子囡吵架时歇斯底里的喊叫仍然在你的脑袋里嗡嗡作响，像口腔上的脓疮，如果你不用舌头去舔，也许会很快痊愈，但是你总是忍不住去舔，就如同你和子囡摇摇欲坠的感情一样。」</p><p>你忍受不了这该死的一切，当着她的面将玻璃牙杯摔得粉碎，这清脆地声音同时震惊了你们，子囡不知道是因为害怕还是绝望，不再冲你喊叫，数十秒之后，看见了她从未有过的坚定眼神。</p><p>和她一起离开的还有重重的离开的摔门声，留下一地的碎玻璃，还有错愕的你。</p><p>你瘫坐下来，身体陷入了柔软到不恰当的沙发里，回想起你和子囡两个人在一起的时候，她真的是一个能吃的姑娘，和自己一样的能吃，可她却总是吃不胖，永远那么的苗条那么的好看。或许任何东西对她而言都只是匆匆过客吧，获得多少也能干净利落地流失多少，没有什么东西能在她的身体里留下丝毫痕迹。</p><h2 id="二"><a href="# 二" class="headerlink" title="二"></a>二</h2><p>「我像是被炙火烫伤的手指，我渴望冰，我渴望霜，我渴望风，我渴望一切能让我无论体内体外温度都冷却下来的种种一切。我们争吵过后的每一次都让我变得更加冷静脆弱，也如同我们之间的感情不断地从 4 摄氏度向零点靠近，从你口中最想要的爱情形态变成永远都不会化开的冰刀，插在你我两人的身上。」</p><p>子囡在那天晚上喝了不知道多少的酒，在写给你的这段话却异常的清醒。她渴望通过酒精解脱，却发现这该死的爱情竟然抵不过几句话。不知道是酒精作祟还是困意上头，子囡把头埋在了手臂，如同将爱情一起碾碎埋在不知名的何处。</p><p>下午的时候，你若无其事，觉得应该带煎饺一起去外面溜达溜达，煎饺是子囡和你在五年前一同收养的金毛，起初你们俩热恋，爱得你侬我侬的时候，煎饺常常成为你们朋友圈的重要配角之一，是你们以往感情的忠实见证者。煎饺在出门之前，一直低着头嘀咕着，兴许是看到了女主人气愤的样子，最近的频率越来越高，煎饺并不喜欢这样子。你拍着煎饺的头的时候，煎饺望着你，仿佛在问子囡在哪，你也看着煎饺的眼睛，不知为何，你好像看到煎饺的眼中看到了另外一个人的模样。</p><p>带煎饺出门的时候，煎饺仿佛很开心，很有活力，也许它以为是你要带它去找子囡，一会蹭着你的腿，一会绕着你转，你看着煎饺，说不定子囡这次也许也只是单纯的发牢骚呢，或许你和煎饺一样，期待着这段感情并没有到终点，毕竟子囡怎么会忍心抛下你和煎饺呢。想完，你牵着煎饺下了楼，准备去外面买点子囡最爱吃的食物，等着子囡晚上回家向她好好道歉一番。</p>]]></content>
    
    <summary type="html">
    
      戊戌年九月初三于合肥所作。
    
    </summary>
    
      <category term="创作" scheme="http://randolph.pro/categories/%E5%88%9B%E4%BD%9C/"/>
    
    
      <category term="创作" scheme="http://randolph.pro/tags/%E5%88%9B%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>✎「匯文」 水之湄</title>
    <link href="http://randolph.pro/2018/09/06/%E2%9C%8E%E3%80%8C%E5%8C%AF%E6%96%87%E3%80%8D%E6%B0%B4%E4%B9%8B%E6%B9%84/"/>
    <id>http://randolph.pro/2018/09/06/✎「匯文」水之湄/</id>
    <published>2018-09-05T16:00:00.000Z</published>
    <updated>2019-03-17T14:35:13.084Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm8.staticflickr.com/7905/47348684952_2be39b1d04_o.jpg" alt></p>        <div id="aplayer-NAIrlYmv" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;">            <pre class="aplayer-lrc-content"></pre>        </div>        <script>          var ap = new APlayer({element: document.getElementById("aplayer-NAIrlYmv"),            narrow: false,            autoplay: false,            showlrc: false,            music: {              title: "水之湄",              author: "惘闻",              url: "http://m10.music.126.net/20190317183217/5ab317588f0dc56376b65be62410ae17/ymusic/60b0/9f43/8114/555bc404980f9d283be4dcee83a943a1.mp3",              pic: "https://farm8.staticflickr.com/7905/32461469177_cf49b312c3_o.jpg",              lrc: ""            }          });          window.aplayers || (window.aplayers = []);          window.aplayers.push(ap);        </script><h1 id="水之湄"><a href="# 水之湄" class="headerlink" title="水之湄"></a>水之湄 </h1><p> 读过书少之又少</p><p>但卡尔维诺却是我印象中最深刻最最最喜欢的作家 <br> 我不止一次在何地何处和别人谈及他的一切</p><p>从最初接触的「不存在的骑士」到后来「分成两半的子爵」<br>我硬生生地被他轻盈而极富想象力的文笔及故事所折服</p><p>卡尔维诺依赖想象力，甚至到了沉溺的地步 <br> 就像托尔金所写的那些捡起众戒之王的人，无法摆脱 <br> 在「美国讲稿」中的「速度」一文中 <br> 他将暗夜中遭遇魔兽的马车夫和梦醒时分的恐龙这种言谈，都当成生活的基础 <br> 为了表达我对他的尊敬与爱戴，我在任何游戏里的昵称都会叫做「梦醒时分的恐龙」</p><p>但我今天想说的是他的「看不见的城市」<br>为什么呢？</p><p>原因与我今晚去听的一场后摇乐队有关</p><p>当马老师给我第一次推荐「水之湄」这首歌的时候 <br> 我激动地从甜梅号的「黄昏鹿场」的单曲循环中跳脱出来 <br> 疯狂地想要了解这支国内后摇乐队 <br> 而我最爱的“水之湄”便来自这支后摇乐队——惘闻的新专「看不见的城市」</p><p>现场的「水之湄」与平时在音乐软件上所听到的大有不同 <br> 不懂音乐的我也至少听出五六处不一样的地方 <br> 我想也许是因为现场即兴发挥或是其他缘由</p><p>在前去之前，小凡曾和我讨论 <br> 后摇乐队的现场到底该是怎样啊 <br> 没有人声的演奏难道不会稍显尴尬么 <br> 我当时也抱着存疑</p><p>但到了现场，各个观众像是被蛊惑住了心窍 <br> 摇头晃脑一头扎进后摇的酒中 </p><p>「记忆中的形象一旦被词语固定住，就给抹掉了。」<br>「看不见的城市」中的波罗曾经这样说道</p><p>听着「水之湄」，我的思绪回到当初趴在书桌上看书的时候 <br> 音乐像是被精细的刀工切成细小的一片片 <br> 夹着糅进了我的思绪中，也逐渐地唤醒了我听后摇入眠的那些日子</p><p>后摇和卡尔维诺给我带来的感受有相同的一点是 <br> 我可以自己随着音乐或文字随意建构想象力王国，并且往往很美但同样也让人孤独绝望</p><p>人生是该有多庆幸有能看书 能听后摇 随意做梦的时候</p><p>关于“水之湄”这首歌的命名，吉他手谢玉岗写下了这样一段耐人寻味的话：「冰岛的 Sundlaugin 录音棚外面有一条小河，山上融化的雪水不断的注入其中，录音的间歇，打开控制室靠近阳台的门，在屋里就能听到水流的声响。可能因为冰岛特殊的地热资源，即使我们录音的那些天，几乎是冰岛最冷的时候了，这条小河也没有结冰，远远看去还弥漫着水雾气。录音到现在半年过去了，我已经记不得录音棚外面其他的地貌特征了，但这条小河却始终清清楚楚的留在了脑海里，它的岸边除了白雪，空空荡荡……」</p><p>另外，小号真的是惘闻「看不见的城市」这张专辑的灵魂！</p><p><img src="https://farm8.staticflickr.com/7902/46678152434_a531b77ba6_o.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      戊戌年七月廿七于合肥 ON THE WAY 所记。
    
    </summary>
    
      <category term="匯文" scheme="http://randolph.pro/categories/%E5%8C%AF%E6%96%87/"/>
    
    
      <category term="匯文" scheme="http://randolph.pro/tags/%E5%8C%AF%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>♜「Datasets」About Khan Academy Data</title>
    <link href="http://randolph.pro/2018/03/19/%E2%99%9C%20%E3%80%8CDatasets%E3%80%8DAbout%20Khan%20Academy%20Data/"/>
    <id>http://randolph.pro/2018/03/19/♜ 「Datasets」About Khan Academy Data/</id>
    <published>2018-03-18T16:00:00.000Z</published>
    <updated>2019-03-18T08:15:08.170Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm1.staticflickr.com/797/27030261808_ebe2f72aa4_o.jpg" alt></p><p> 有关「Datasets」的其他数据集介绍系列：<a href="http://randolph.pro/categories/Datasets/">「Datasets」</a></p><h1 id="About-Khan-Academy-Data"><a href="#About-Khan-Academy-Data" class="headerlink" title="About Khan Academy Data"></a>About Khan Academy Data</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Khan-Academy-Knowledge-Structure"><a href="#Khan-Academy-Knowledge-Structure" class="headerlink" title="Khan Academy Knowledge Structure"></a>Khan Academy Knowledge Structure</h3><p><img src="https://farm1.staticflickr.com/815/40007475165_3f002df1fc_o.png" alt></p><p> 存在重复项，例如：</p><ol><li>「Computer science」 下的 「Hour of Code」 中的内容会重定位到同级的「 Computer science」 的内容中。</li><li>「Math by subject」 与 「Math by grade」存在交集。</li></ol><h2 id="Data-structure"><a href="#Data-structure" class="headerlink" title="Data structure"></a>Data structure</h2><p> 数据一共为 4 个 <code>.json</code> 文件，以及相关视频 <strong>Video</strong> 与相关图片 <strong>Picture</strong>。</p><ul><li>content.json<ul><li> 包含知识点逻辑结构信息 </li><li> 包含内容种类信息（主要用于指向对应的题目类型）</li></ul></li><li>practice.json<ul><li> 若数据在 <code>content.json</code> 中 <code>content_kind</code> 字段为 <strong>Exercise</strong> ，则会进一步根据 <code>practice</code> \<list> 中的唯一 <strong><code>practice_id</code></strong> 对应到 <code>practice.json</code> 当中。</list></li><li> 包含了试题题面文本信息以及图像信息 </li><li> 包含了试题答案文本信息以及图像信息 </li></ul></li><li>article.json<ul><li> 若数据在 <code>content.json</code> 中 <code>content_kind</code> 字段为 <strong>Article</strong> ，则会进一步根据唯一的 <code>article_url</code> 对应到 <code>article.json</code> 当中。</li></ul></li><li>code.json<ul><li> 属于可汗学院中专门的编程题，但是其实重定向后就是对应种类为 <strong>Scratchpad</strong> 内容，简而言之，就是属于 <strong>Scratchpad</strong> 的一小部分内容，但不是 <strong>Scratchpad</strong>  的全部内容。</li><li> 另外，若数据在 <code>content.json</code> 中 <code>content_kind</code> 字段为  <strong>Scratchpad</strong> ，并不存在唯一的字段对应到 <code>code.json</code> 当中，即不存在直接联系。</li></ul></li></ul><h3 id="Logical-Structure"><a href="#Logical-Structure" class="headerlink" title="Logical Structure"></a>Logical Structure</h3><p><img src="https://farm1.staticflickr.com/807/26028761257_c79edc4013_o.png" alt></p><h3 id="content-json"><a href="#content-json" class="headerlink" title="content.json"></a><code>content.json</code></h3><p><img src="https://farm1.staticflickr.com/815/40193083274_e7ac89c514_o.png" alt></p><h3 id="practice-json"><a href="#practice-json" class="headerlink" title="practice.json"></a><code>practice.json</code></h3><p><img src="https://farm5.staticflickr.com/4776/40007579025_a3160e419c_o.png" alt></p><h3 id="Different-types-of-content"><a href="#Different-types-of-content" class="headerlink" title="Different types of content"></a>Different types of content</h3><h4 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h4><p><img src="https://farm5.staticflickr.com/4776/26028894047_f866849115_o.png" alt></p><h4 id="Picture"><a href="#Picture" class="headerlink" title="Picture"></a>Picture</h4><p>Total <strong>14839</strong> items = <strong>4128</strong> <code>.png</code> + <strong>10709</strong> <code>.svg</code> (Exists duplicate items)</p><p><img src="https://farm1.staticflickr.com/813/40193098264_6ec4f5842e_o.png" alt></p><h4 id="Video"><a href="#Video" class="headerlink" title="Video"></a>Video</h4><p><img src="https://farm1.staticflickr.com/791/39091655630_73e8720a62_o.png" alt></p><h4 id="Article"><a href="#Article" class="headerlink" title="Article"></a>Article</h4><p><img src="https://farm1.staticflickr.com/814/40901331191_30e7694fdb_o.png" alt></p><h4 id="Scratchpad"><a href="#Scratchpad" class="headerlink" title="Scratchpad"></a>Scratchpad</h4><p><img src="https://farm1.staticflickr.com/802/40859462552_e6bdc986ee_o.png" alt></p><h2 id="Data-Analysis"><a href="#Data-Analysis" class="headerlink" title="Data Analysis"></a>Data Analysis</h2><h3 id="针对 -Domain- 为「Math-by-subject」"><a href="# 针对 -Domain- 为「Math-by-subject」" class="headerlink" title="针对 Domain 为「Math by subject」"></a> 针对 Domain 为「Math by subject」</h3><ul><li><p> 各个「subject」的 Video 数量分布图 </p><p><img src="https://farm1.staticflickr.com/806/40859498872_f765435403_o.png" alt></p></li><li><p> 各个「subject」的  Exercise 数量分布图 </p><p><img src="https://farm1.staticflickr.com/803/27030763818_a44ec0274e_o.png" alt></p></li><li><p> 各个「subject」的 「child_subject」 数量分布图 </p><p><img src="https://farm1.staticflickr.com/817/40859523822_64ec326b8b_o.png" alt></p></li><li><p> 各个「subject」的  「slug」 数量分布图 </p></li></ul><p><img src="https://farm1.staticflickr.com/813/27030780438_74e889defe_o.png" alt></p><h4 id="针对 -Video- 在所有各个「subject」的数量分布图"><a href="# 针对 -Video- 在所有各个「subject」的数量分布图" class="headerlink" title="针对 Video 在所有各个「subject」的数量分布图"></a> 针对 Video 在所有各个「subject」的数量分布图 </h4><p><img src="https://farm1.staticflickr.com/799/27030789618_113a7e0b9c_o.png" alt></p><h4 id="针对 -Exercise- 在所有各个「subject」的数量分布图"><a href="# 针对 -Exercise- 在所有各个「subject」的数量分布图" class="headerlink" title="针对 Exercise 在所有各个「subject」的数量分布图"></a> 针对 Exercise 在所有各个「subject」的数量分布图 </h4><p><img src="https://farm1.staticflickr.com/807/27030796068_3ee5504c9e_o.png" alt></p>]]></content>
    
    <summary type="html">
    
      本文主要介绍了可汗学院的内容逻辑结构以及进行了一些初步的数据分析。
    
    </summary>
    
      <category term="Datasets" scheme="http://randolph.pro/categories/Datasets/"/>
    
    
      <category term="Datasets" scheme="http://randolph.pro/tags/Datasets/"/>
    
  </entry>
  
  <entry>
    <title>♚「Kaggle」 Music Recommendation Challenge</title>
    <link href="http://randolph.pro/2017/12/17/%E2%99%9A%E3%80%8CKaggle%E3%80%8DMusic%20Recommendation%20Challenge/"/>
    <id>http://randolph.pro/2017/12/17/♚「Kaggle」Music Recommendation Challenge/</id>
    <published>2017-12-16T16:00:00.000Z</published>
    <updated>2019-03-17T15:02:50.751Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm8.staticflickr.com/7820/46680387594_4f5c2a631f_o.jpg" alt></p><p>有关「Machine Learning」的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/">「Machine Learning」</a><br>有关「Kaggle」的其他论文学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/Kaggle/">「Kaggle」</a><br>有关模型代码：<a href="https://github.com/RandolphVI/Music-Recommendation-System" target="_blank" rel="noopener">「Music Recommendation Challenge」</a></p><h1 id="「Music-Recommendation-Challenge」"><a href="#「Music-Recommendation-Challenge」" class="headerlink" title="「Music Recommendation Challenge」"></a>「Music Recommendation Challenge」</h1><h2 id="Step-1-Exploratory-Data-Analysis-EDA"><a href="#Step-1-Exploratory-Data-Analysis-EDA" class="headerlink" title="Step 1: Exploratory Data Analysis(EDA)"></a>Step 1: Exploratory Data Analysis(EDA)</h2><h3 id="原始特征"><a href="# 原始特征" class="headerlink" title="原始特征"></a>原始特征</h3><ul><li>用户特征<ul><li><code>user_id</code></li><li><code>city</code></li><li><code>gender</code></li><li><code>age</code></li></ul></li><li>音乐特征<ul><li><code>song_id</code></li><li><code>song_length</code></li><li><code>genre_ids</code></li><li><code>language</code></li><li><code>name</code></li><li><code>artist_name</code></li><li><code>composer</code></li><li><code>lyricist</code></li><li><code>isrc</code></li></ul></li><li>交互特征</li><li>上下文特征<ul><li><code>registered_via</code></li><li><code>registration_init_time</code></li><li><code>exipration_date</code></li><li><code>source_system_tab</code></li><li><code>source_screen_name</code></li><li><code>source_type</code></li></ul></li></ul><h2 id="Step-2-Feature-Engineering"><a href="#Step-2-Feature-Engineering" class="headerlink" title="Step 2:  Feature Engineering"></a>Step 2:  Feature Engineering</h2><p>Feature Engineering 是把 raw data 转换成 features 的整个过程的总称。基本上特征工程就是个手艺活，制作的好坏全凭人的功夫，往细了讲，便是创造力与经验。</p><p>以该比赛音乐推荐系统为例，数据集中的特征可以分成以下四种：</p><ul><li>用户特征：用户本身的各种属性，例如 <code>user id</code>、<code>gender</code>（性别）、<code>city</code>（所在的城市）等</li><li>音乐特征：音乐本身的各种属性，例如<code>song id</code>、<code>name</code>（歌曲名）、<code>artist</code>（演唱者）、<code>composer</code>（作曲家）、<code>lyricist</code>（作词家）、<code>genre_ids</code>（音乐风格分类）等</li><li>交互特征：用户对音乐做出的某项行为，该行为的 aggregation 或交叉特征，例如最近听的歌曲的曲风分布或 <code>most_like_artist_type</code> 喜爱的歌手的类型、<code>listen_count</code>听歌的次数等</li><li>上下文特征：用戶对音乐做出的某项行为，该行为的 metadata，例如 <code>registration_init_time</code> 注册的时间、<code>source_type</code> 使用的设备等</li></ul><p>有些特征是在资料 EDA 阶段就可以拿到，有些特征则需要额外的步骤（例如如透过外部的 API 或者其他模型）才能取得。</p><h3 id="联想特征"><a href="# 联想特征" class="headerlink" title="联想特征"></a>联想特征</h3><ul><li><p>用户特征</p><ul><li><code>registration_year</code>: 该用户的注册年份</li><li><code>registration_month</code>：该用户的注册月份</li><li><code>registration_date</code>：该用户的注册日</li><li><code>expiration_year</code>: 该用户的退订年份</li><li><code>expiration_month</code>：该用户的退订月份</li><li><code>expiration_date</code>：该用户的退订日</li><li><code>membership_days</code>：该用户从注册到退订的时间天数</li></ul></li><li><p>音乐特征</p><ul><li><code>is_featured</code>：判断 <code>artist</code> 中是否存在 <code>feat.</code> 信息（<strong><code>.feat</code></strong> 是 <strong>featuring</strong> 的缩写，如果直译的话，指「以……为特色、亮点」。<strong><code>feat.</code></strong> 在歌曲里面是指专辑表演者与另外（一个或者多个）的艺人 / 组合的合作，也就是请人在歌曲中客串。）</li><li><code>smaller_song</code>：判断 <code>song_length</code> 是否小于 <code>avg_song_length</code> （<code>avg_song_length</code> 是 <strong>train &amp; test</strong> 所出现的所有 <code>song_length</code> 的平均长度） </li><li><code>artist_composer</code>：判断 <code>artisit</code> 与 <code>composer</code> 中是否出现同样的艺人，如果有则记为 1，否则记为 0（间接反映艺人的有才程度）</li><li><code>artist_composer_lyricist</code>：判断 <code>artisit</code> 、 <code>composer</code> 以及 <code>lyricist</code> 中是否出现同样的艺人，如果有则记为 1，否则记为 0（间接反映艺人的有才程度）</li><li><code>genre_count</code>： 该歌曲的 <code>genre_ids</code> 的个数</li><li><code>artist_count</code>：该歌曲的 <code>artist</code> 的个数</li><li><code>composer_count</code>：该歌曲的 <code>composer</code> 的个数</li><li><code>lyricist_count</code>：该歌曲的 <code>lyricist</code> 的个数</li><li><code>count_song_played</code>：该歌曲 <code>song_id</code> 在 <strong>train &amp; test</strong> 中出现的次数</li><li><code>count_artist_played</code>：该艺人 <code>artist</code> 在 <strong>train &amp; test</strong> 中出现的次数</li><li><code>count_genre_played</code>：该曲风 <code>genre_ids</code> 在 <strong>train &amp; test</strong> 中出现的次数</li><li><code>count_genre_like</code>： 该曲风 <code>genre_ids</code> 在 <strong>train</strong> 中 <strong><code>target=1</code></strong>（即被喜欢）的次数</li><li><code>genre_like_ratio</code>：<code>count_genre_like</code> / <code>count_genre_played</code>（反映歌曲被喜爱的程度，或者说流行的程度）</li><li><code>song_country</code>：根据 <code>isrc</code> 信息得到的歌曲所属的国家信息</li><li><code>song_publisher</code>：根据 <code>isrc</code> 信息得到的歌曲所属的发布商信息</li><li><code>song_year</code>：根据 <code>isrc</code> 信息得到的歌曲所属的发布年份信息</li></ul></li><li><p>交互特征</p></li><li><p>上下文特征</p><ul><li><p><code>als_model_prediction</code>：来自 ALS 模型的预测值，该用户对某音乐的偏好程度</p></li><li><p><code>gbdt_model_index</code>: 来自 GBDT 模型的 tree index，某 observation 的自动特征</p></li></ul></li></ul><h3 id="2-1-Missing-Value-Imputation- 缺失值处理"><a href="#2-1-Missing-Value-Imputation- 缺失值处理" class="headerlink" title="2.1 Missing Value Imputation 缺失值处理"></a>2.1 Missing Value Imputation 缺失值处理 </h3><p> 最简单暴力的做法当然就是直接 drop 掉那些含有缺失值的 rows。</p><ul><li>针对 numerical 特征的缺失值，可以用以下方式取代：<ul><li><code>0</code>，缺点是可能会混淆其他本来就是 0 的数值</li><li><code>-999</code>，用某个正常情况下不会出现的数值代替，但是选得不好可能会变成异常值，要特别对待</li><li>Mean，平均数（例如用户年龄信息 <code>bd</code> 存在许多异常值（存在负数、零甚至超过一百），对于那些异常值可以用 <code>age</code> 的平均值来代替）</li><li>Median，中位数，跟平均数相比，不会被异常值干扰</li></ul></li><li>针对 categorical 特征的缺失值，可以用以下方式取代：<ul><li>Mode，众数，最常见的值</li><li>改成 <code>Others</code> 之类的值</li></ul></li></ul><p>假设你要填补 <code>age</code> 这个特征，然后你有其他例如 <code>gender</code> 这样的特征，你可以分别计算男性和女性的 <code>bd</code> 的平均数、中位数和众数来填补缺失值；更复杂一点的方式是，你可以把没有缺失值的数据挑出来，用它们来训练一个 regression 或 classification 模型，用这个模型来预测缺失值。</p><p>不过其实有些算法是可以容许缺失值的，这时候可以新增一个<code>has_missing_value</code> 栏位（称为 NA indicator column）。</p><h3 id="2-2-Outliers-Detection- 野点处理"><a href="#2-2-Outliers-Detection- 野点处理" class="headerlink" title="2.2 Outliers Detection 野点处理"></a>2.2 Outliers Detection 野点处理 </h3><p> 发现离群值最直观的方式就是画图表，针对单一特征可以使用 box plot；两两特征则可以使用 scatter plot。</p><p>处置离群值的方式通常是直接删除或是做变换（例如 log transformation 或 binning），当然你也可以套用处理缺失值的方式。</p><h3 id="2-3-Duplicate-Entries-Removal- 异常值处理"><a href="#2-3-Duplicate-Entries-Removal- 异常值处理" class="headerlink" title="2.3 Duplicate Entries Removal 异常值处理"></a>2.3 Duplicate Entries Removal 异常值处理</h3><p>Duplicate 或 redundant 尤其指的是那些 features 都一样，但是 target variable 却不同的数据。</p><h3 id="2-4-Feature-Scaling- 特征缩放"><a href="#2-4-Feature-Scaling- 特征缩放" class="headerlink" title="2.4 Feature Scaling 特征缩放"></a>2.4 Feature Scaling 特征缩放 </h3><h4 id="2-4-1-Standardization- 标准化"><a href="#2-4-1-Standardization- 标准化" class="headerlink" title="2.4.1 Standardization 标准化"></a><strong>2.4.1 Standardization 标准化</strong></h4><p> 原始数据集中，因为各个特征的含义和单位不同，每个特征的取值范围可能会差异很大。例如某个二元特征的范围是 0 或 1，另一个特征的范围可能是 $[0, 1000000]$，由于取值范围相差过大导致了模型可能会更偏向于取值范围较大的那个特征。解决的办法就是把各种不同 scale 的特征转换成同样的 scale，称为标准化或正规化。</p><p>狭义来说，标准化专门指的是通过计算 z-score，让数据的 mean 为 0、 variance 为 1。</p><h4 id="2-4-2-Normalization- 归一化"><a href="#2-4-2-Normalization- 归一化" class="headerlink" title="2.4.2 Normalization 归一化"></a><strong>2.4.2 Normalization 归一化 </strong></h4><p> 归一化是指把每个样本缩放到单位范数（每个样本的范数为 1），适用于计算 dot product 或者两个样本之间的相似性。除了标准化、归一化之外，其他还有通过最大、最小值，把数据的范围缩放到 $[0, 1]$ 或 $[-1, 1]$ 的区间缩放法，不过这个方法容易受异常值的影响。</p><p>标准化是分别对单一特征进行（针对 column）；归一化是对每个 observation 进行（针对 row）。</p><ul><li><strong>对 SVM、logistic regression 或其他使用 squared loss function 的演算法来说，需要 standardization；</strong></li><li><strong>对 Vector Space Model 来说，需要 normalization</strong>；</li><li><strong>对 tree-based 的算法，基本上都不需要标准化或归一化，它们对 scale 不敏感。</strong></li></ul><h3 id="2-5-Feature-Transformation- 特征变换"><a href="#2-5-Feature-Transformation- 特征变换" class="headerlink" title="2.5 Feature Transformation 特征变换"></a>2.5 Feature Transformation 特征变换 </h3><p> 针对 continuous 特征，可以进行 Feature Transformation 特征变换：</p><h4 id="2-5-1-Rounding"><a href="#2-5-1-Rounding" class="headerlink" title="2.5.1 Rounding"></a><strong>2.5.1 Rounding</strong></h4><p>某些精度有到小数点后第 n 位的特征，如果你其实不需要那么精确，可以考虑 <code>round(value * m)</code> 或 <code>round(log(value))</code> 这样的做法，甚至可以把 round 之后的数值当成 categorical 特征。</p><h4 id="2-5-2-Log-Transformation"><a href="#2-5-2-Log-Transformation" class="headerlink" title="2.5.2 Log Transformation"></a><strong>2.5.2 Log Transformation</strong></h4><p>因为 $x$ 越大，$log(x)$ 增长的速度就越慢，所以 <strong> 取 log 的意义是可以 compress 大数和 expand 小数</strong>，换句话说就是压缩 “long tail” 和展开 “head”。假设 x 原本的范围是 $[100, 1000]$，$log(x, 10)$ 之后的范围就变成 $[2, 3]$ 了。也常常使用 $log(1 + x)$ 或 $log(\frac{x}{1-x})$。</p><p>另外一种类似的做法是 square root 平方根或 cube root 立方根（可以用在负数）。</p><h4 id="2-5-3-Binarization"><a href="#2-5-3-Binarization" class="headerlink" title="2.5.3 Binarization"></a><strong>2.5.3 Binarization</strong></h4><p>对数值型的数据设定一个 threshold，大于就赋值为 1、小于就赋值为 0。例如 <code>score</code>，如果你只关心「及格」或「不及格」，可以直接把成绩对应到 1（<code>score &gt;= 60</code>）和 0（<code>score &lt; 60</code>）；或是你要做啤酒销量分析，你可以新增一个 <code>age &gt;= 18</code> 的特征来标示出已成年。</p><p>你有一个 <code>color</code> 的 categorical 特征，如果你不在乎实际上是什么颜色的话，其实也可以改成 <code>has_color</code>。</p><h4 id="2-5-4-Binning"><a href="#2-5-4-Binning" class="headerlink" title="2.5.4 Binning"></a><strong>2.5.4 Binning</strong></h4><p>也称为 <strong>bucketization</strong>。以 <code>age</code> 这样的特征为例，你可以把所有年龄拆分成 $n$ 段，0-20 岁、20-40 岁、40-60 岁等或是 0-18 岁、18-40 岁、40-70 岁等（等距或等量），然后把个别的年龄对应到某一段，假设 26 岁是对应到第二个 bucket，那新特征的值就是 2。这种方式是人为地指定每个 bucket 的边界值，还有另外一种拆分法是根据数据的分布来拆，称为 quantization 或 quantile binning，你只需要指定 bucket 的数量即可。</p><p>同样的概念应用到其他地方，可以把 datetime 特征拆分成上午、中午、下午和晚上；如果是 categorical 特征，可以把出现次数小于某个 threshold 的值改成 <code>Other</code> 之类的。或者是你有一个 occupation 特征，如果你其实不需要非常准确的职业资讯的话，可以把 “Web Developer”、”iOS Developer” 或 “DBA” 这些个别的资料都改成 “Software Engineer”。</p><p><strong>Binarization 和 Binning 都是对 continuous 特征离散化，增强模型的非线性泛化能力。</strong></p><h4 id="2-5-5-Integer-Encoding"><a href="#2-5-5-Integer-Encoding" class="headerlink" title="2.5.5 Integer Encoding"></a><strong>2.5.5 Integer Encoding</strong></h4><p>也称为 <strong>label encoding</strong>。把每个 category 对应到数字，一种做法是随机对应到 0, 1, 2, 3, 4 等数字；另外一种做法是依照该值出现的频率大小的顺序来给值，例如最常出现的值给 0，依序给 1, 2, 3 等等。如果是针对一些在某种程度上有次序的 categorical 特征（称为 ordinal），例如「钻石会员」「白金会员」「黄金会员」「普通会员」，直接 mapping 成数字可能没什么问题，但是如果是类似 <code>color</code> 或 <code>city</code> 这样的没有明显大小的特征的话，还是用 one-hot encoding 比较合适。不过如果用的是 tree-based 的算法就无所谓了。</p><p>有些 categorical 特征也可能会用数字表示（例如 id），跟 continuous 特征的差别是，数值的差异或大小对 categorical 特征来说没有太大的意义。</p><h4 id="2-5-6-One-hot-Encoding-OHE"><a href="#2-5-6-One-hot-Encoding-OHE" class="headerlink" title="2.5.6 One-hot Encoding(OHE)"></a><strong>2.5.6 One-hot Encoding(OHE)</strong></h4><p>如果某个特征有 $m$ 种值（例如 Taipei, Beijing, Tokyo），那它 one-hot encode 之后就会变成长度为 $m$ 的向量：</p><ul><li>Taipei: [1, 0 ,0]</li><li>Beijing: [0, 1, 0]</li><li><p>Tokyo: [0, 0, 1]</p><p>你也可以改用 Dummy coding，这样就只需要产生长度为 $m -1$ 的向量：</p></li><li><p>Taipei: [1, 0]</p></li><li>Beijing: [0, 1]</li><li>Tokyo: [0, 0]</li></ul><p>One-hot Encoding 的缺点是容易造成特征的维度大幅增加和没办法处理之前没见过的值。</p><h4 id="2-5-7-Bin-counting"><a href="#2-5-7-Bin-counting" class="headerlink" title="2.5.7 Bin-counting"></a><strong>2.5.7 Bin-counting</strong></h4><p>例如在 Computational Advertising 中，如果你有针对每个 user 的「广告曝光数（包含点击和未点击）」和「广告点击数」，你就可以算出每个 user 的「点击率」，然后用这个机率来表示每个 user，反之也可以对 ad id 使用类似的做法。</p>  <figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ad_id   ad_views  ad_clicks  ad_ctr</span><br><span class="line"><span class="number">412533</span>  <span class="number">18339</span>     <span class="number">1355</span>       <span class="number">0.074</span></span><br><span class="line"><span class="number">423334</span>  <span class="number">335</span>       <span class="number">12</span>         <span class="number">0.036</span></span><br><span class="line"><span class="number">345664</span>  <span class="number">1244</span>      <span class="number">132</span>        <span class="number">0.106</span></span><br><span class="line"><span class="number">349833</span>  <span class="number">35387</span>     <span class="number">1244</span>       <span class="number">0.035</span></span><br></pre></td></tr></table></figure><p>换个思路，如果你有一个 brand 的特征，然后你可以从 user 的购买记录中找出购买 A 品牌的人，有 70% 的人会购买 B 品牌、有 40% 的人会购买 C 品牌；购买 D 品牌的人，有 10% 的人会购买 A 品牌和 E 品牌，你可以每个品牌表示成这样：</p>  <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">brand</span>  <span class="selector-tag">A</span>    <span class="selector-tag">B</span>    <span class="selector-tag">C</span>    <span class="selector-tag">D</span>    <span class="selector-tag">E</span></span><br><span class="line"><span class="selector-tag">A</span>      1<span class="selector-class">.0</span>  0<span class="selector-class">.7</span>  0<span class="selector-class">.4</span>  0<span class="selector-class">.0</span>  0<span class="selector-class">.0</span></span><br><span class="line"><span class="selector-tag">B</span>      ...</span><br><span class="line"><span class="selector-tag">C</span>      ...</span><br><span class="line"><span class="selector-tag">D</span>      0<span class="selector-class">.1</span>  0<span class="selector-class">.0</span>  0<span class="selector-class">.0</span>  1<span class="selector-class">.0</span>  0<span class="selector-class">.1</span></span><br><span class="line"><span class="selector-tag">E</span>      ...</span><br></pre></td></tr></table></figure><h4 id="2-5-8-LabelCount-Encoding"><a href="#2-5-8-LabelCount-Encoding" class="headerlink" title="2.5.8 LabelCount Encoding"></a><strong>2.5.8 LabelCount Encoding</strong></h4><p>类似 Bin-cunting 的做法，一样是利用现有的 count 或其他统计上的资料，差别在于 LabelCount Encoding 最后用的是次序而不是数值本身。优点是对异常值不敏感。</p>  <figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ad_id   ad_clicks  ad_rank</span><br><span class="line"><span class="number">412533</span>  <span class="number">1355</span>       <span class="number">1</span></span><br><span class="line"><span class="number">423334</span>  <span class="number">12</span>         <span class="number">4</span></span><br><span class="line"><span class="number">345664</span>  <span class="number">132</span>        <span class="number">3</span></span><br><span class="line"><span class="number">349833</span>  <span class="number">1244</span>       <span class="number">2</span></span><br></pre></td></tr></table></figure><h4 id="2-5-9-Count-Vectorization"><a href="#2-5-9-Count-Vectorization" class="headerlink" title="2.5.9 Count Vectorization"></a><strong>2.5.9 Count Vectorization</strong></h4><p>除了可以用在 text 特征之外，如果你有 comma-seperated 的 categorical 特征也可以使用这个方法。例如电影类型 <code>genre</code>，里头的值为 <code>Action, Sci-Fi, Drama</code>，就可以先用 <code>RegexTokenizer</code> 转成 <code>Array(&quot;action&quot;, &quot;sci-fi&quot;, &quot;drama&quot;)</code>，再用 <code>CountVectorizer</code> 转成 vector。</p><h4 id="2-5-10-Feature-Hashing"><a href="#2-5-10-Feature-Hashing" class="headerlink" title="2.5.10 Feature Hashing"></a><strong>2.5.10 Feature Hashing</strong></h4><p>以 user id 为例，透过一个 hash function 把每一个 user id 映射到 <code>(hashed_1, hashed_2, ..., hashed_m)</code> 的某个值。指定 m &lt;&lt; user id 的取值范围，所以缺点是会有 collision（如果你的 model 足够 robust，倒也是可以不管），优点是可以良好地处理之前没见过的值和罕见的值。当然不只可以 hash 单一值，也可以 hash 一个 vector。</p><p>你可以把 feature hashing 表示为单一栏位的数值（例如 2）或是类似 one-hot encoding 那样的多栏位的 binary 表示法（例如 [0, 0, 1]）。</p><h4 id="2-5-11-Category-Embedding"><a href="#2-5-11-Category-Embedding" class="headerlink" title="2.5.11 Category Embedding"></a><strong>2.5.11 Category Embedding</strong></h4><h4 id="2-5-12-User-Profile"><a href="#2-5-12-User-Profile" class="headerlink" title="2.5.12 User Profile"></a><strong>2.5.12 User Profile</strong></h4><p>使用用户画像来表示每个 user id，例如用户的年龄、性别、职业、收入、居住地、偏好的各种 tag 等，把每个 user 表示成一个 feature vector。除了单一维度的特征之外，也可以建立「用户听过的歌都是哪些曲风」、「用户（30 天内）浏览过的文章都是什么分类」，以 TF-IDF 的方式表达。或者是把用户所有喜欢文章对应的向量的平均值作为此用户的 profile。比如某个用户经常关注与推荐系统有关的文章，那么他的 profile 中 “CB”、”CF” 和 “推荐系统” 对应的权重值就会较高。</p><h4 id="2-5-13-Rare-Categorical-Varibales"><a href="#2-5-13-Rare-Categorical-Varibales" class="headerlink" title="2.5.13 Rare Categorical Varibales"></a><strong>2.5.13 Rare Categorical Varibales</strong></h4><p>先计算好每一种 category 的数量，然后把小于某个 threshold 的 category 都改成 <code>Others</code> 之类的值。或是使用 clustering 算法来达到同样的目的。你也可以直接建立一个新的 binary feature 叫做 <code>rare</code>，要来标示那些相对少见的类别。</p><h4 id="2-5-14-Unseen-Categorical-Variables"><a href="#2-5-14-Unseen-Categorical-Variables" class="headerlink" title="2.5.14 Unseen Categorical Variables"></a><strong>2.5.14 Unseen Categorical Variables</strong></h4><p>当你用 training set 的资料 fit 了一个 <code>StringIndexer</code>（和 <code>OneHotEncoder</code>），把它拿去用在 test set 上时，有一定的机率你会遇到某些 categorical 特征的值只在 test set 出现，所以对只见过 training set 的 transformer 来说，这些就是所谓的 unseen values。</p><p>对付 unseen values 通常有几种做法：</p><pre><code>1. 用整个 training set + test set 来编码 categorical 特征2. 直接舍弃含有 unseen values 的那条记录3. 把 unseen values 改成 `Others` 之类的已知值。</code></pre><p>如果采用第一种方式，一但你把这个 transformer 拿到 production 去用时，无可避免地还是会遇到 unseen values。不过通常线上的 feature engineering 会有别的做法，例如事先把 user 或 item 的各项特征都算好（定期更新或是 data 产生的时候触发），然后以 id 为 key 存进 Redis 之类的 NoSQL 里，model 要用的时候直接用 user id / item id 拿到处理好的 feature vector。</p><h4 id="2-5-15-Large-Categorical-Variables"><a href="#2-5-15-Large-Categorical-Variables" class="headerlink" title="2.5.15 Large Categorical Variables"></a><strong>2.5.15 Large Categorical Variables</strong></h4><p>针对那种非常大的 categorical 特征（例如 id 类的特征），如果你用的是 logistic regression，其实可以硬上 one-hot encoding。不然就是利用上面提到的 feature hashing 或 bin counting 等方式；如果是 GBDT 的话，甚至可以直接用 id 硬上，只要 tree 足够多。</p><h3 id="2-6-Feature-Construction- 特征构建"><a href="#2-6-Feature-Construction- 特征构建" class="headerlink" title="2.6 Feature Construction 特征构建"></a>2.6 Feature Construction 特征构建 </h3><p> 特征构建指的是从原有的特征中，人工地创造出新的特征，通常用来解决一般的线性模型没办法学到非线性特征的问题。其中一个重点是能否通过某些办法，在特征中加入某些「额外的资讯」，虽然也得小心数据偏见的问题。</p><p>如果你有很多 user 购物的资料，除了可以 aggregate 得到 <code>total spend</code> 这样的 feature 之外，也可以变换一下，变成 <code>spend in last week</code>、<code>spend in last month</code> 和 <code>spend in last year</code> 这种可以表示「趋势」的特征。</p><p>例如：</p><ol><li><p><code>author_avg_page_view</code>: 该文章作者的所有文章的平均浏览数</p></li><li><p><code>user_visited_days_since_doc_published</code>: 该文章发布到该用户访问经过了多少天</p></li><li><p><code>user_history_doc_sim_categories</code>: 用户读过的所有文章的分类和该篇文章的分类的 TF-IDF 的相似度</p></li><li><p><code>user_history_doc_sim_topics</code>: 用户读过的所有文章的内文和该篇文章的内文的 TF-IDF 的相似度</p></li></ol><h4 id="2-6-1-Temporal-Features- 时间特征"><a href="#2-6-1-Temporal-Features- 时间特征" class="headerlink" title="2.6.1 Temporal Features 时间特征"></a><strong>2.6.1 Temporal Features 时间特征 </strong></h4><p> 对于 date / time 类型的资料，除了转换成 timestamp 和取出 <code>day</code>、<code>month</code> 和 <code>year</code> 做成新的栏位之外，也可以对 hour 做 binning（分成上午、中午、晚上之类的）或是对 day 做 binning（分成工作日、周末）；或是想办法查出该日期当天的天气、节日或活动等讯息，例如 <code>is_national_holiday</code> 或 <code>has_sport_events</code>。</p><p>更进一步，用 datetime 类的资料通常也可以做成 <code>spend_hours_last_week</code> 或 <code>spend_money_last_week</code> 这种可以用来表示「趋势」的特征。</p><h4 id="2-6-2-Text-Features- 文字特征"><a href="#2-6-2-Text-Features- 文字特征" class="headerlink" title="2.6.2 Text Features 文字特征"></a><strong>2.6.2 Text Features 文字特征 </strong></h4><h4 id="2-6-3-Spatial-Features- 地理特征"><a href="#2-6-3-Spatial-Features- 地理特征" class="headerlink" title="2.6.3 Spatial Features 地理特征"></a><strong>2.6.3 Spatial Features 地理特征</strong></h4><h4 id="2-6-4-Cyclical-Features"><a href="#2-6-4-Cyclical-Features" class="headerlink" title="2.6.4 Cyclical Features"></a><strong>2.6.4 Cyclical Features</strong></h4><h3 id="2-7-Feature-Interaction- 特征交互"><a href="#2-7-Feature-Interaction- 特征交互" class="headerlink" title="2.7 Feature Interaction 特征交互"></a>2.7 Feature Interaction 特征交互</h3><p> 假设你有 <code>A</code> 和 <code>B</code> 两个 continuous 特征，你可以用 <code>A + B</code>、<code>A - B</code>、<code>A * B</code> 或 <code>A / B</code> 之类的方式建立新的特征。例如 <code>house_age_at_purchase = house_built_date - house_purchase_date</code> 或是 <code>click_through_rate = n_clicks / n_impressions</code>。</p><p>还有一种类似的作法叫 Polynomial Expansion 多项式展开，当 degree 为 2 时，可以把 <code>(x, y)</code> 两个特征变成 <code>(x, x * x, y, x * y, y * y)</code> 五个特征。</p><h3 id="2-8-Feature-Combination- 特征组合"><a href="#2-8-Feature-Combination- 特征组合" class="headerlink" title="2.8 Feature Combination 特征组合"></a>2.8 Feature Combination 特征组合 </h3><p><strong> 特征组合主要是针对 categorical 特征，特征交互则是适用于连续值特征</strong>。但是两者的概念是差不多的，就是把两个以上的特征透过某种方式结合在一起，变成新的特征。通常用来解决一般的线性模型没办法学到非线性特征的问题。</p><p>假设有 <code>gender</code> 和 <code>wealth</code> 两个特征，分别有 2 和 3 种取值，最简单的方式就是直接 string concatenation 组合出一个新的特征 <code>gender_wealth</code>，共有 2 x 3 = 6 种取值。因为是 categorical 特征，可以直接对 <code>gender_wealth</code>使用 <code>StringIndexer</code> 和 <code>OneHotEncoder</code>。你当然也可以一起组合 continuous 和 categorical 特征，例如 <code>age_wealth</code> 这样的特征，只是 vector 里的值就不是 0 1 而是 <code>age</code> 本身了。</p><p>假设 C 是 categorical 特征，N 是 continuous 特征，以下有几种有意义的组合：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">user_id  age  gender  wealth  gender_wealth  gender_wealth_ohe   age_wealth</span><br><span class="line"><span class="number">1</span>        <span class="number">56</span>   male    rich    male_rich      [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]  [<span class="number">56</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="number">2</span>        <span class="number">30</span>   male    middle  male_middle    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]  [<span class="number">0</span>, <span class="number">30</span>, <span class="number">0</span>]</span><br><span class="line"><span class="number">3</span>        <span class="number">19</span>   female  rich    female_rich    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]  [<span class="number">19</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="number">4</span>        <span class="number">62</span>   female  poor    female_poor    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]  [<span class="number">0</span>, <span class="number">0</span>, <span class="number">62</span>]</span><br><span class="line"><span class="number">5</span>        <span class="number">78</span>   male    poor    male_poor      [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]  [<span class="number">0</span>, <span class="number">0</span>, <span class="number">78</span>]</span><br><span class="line"><span class="number">6</span>        <span class="number">34</span>   female  middle  female_middle  [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]  [<span class="number">0</span>, <span class="number">34</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="2-9-Feature-Extraction- 特征提取"><a href="#2-9-Feature-Extraction- 特征提取" class="headerlink" title="2.9 Feature Extraction 特征提取"></a>2.9 Feature Extraction 特征提取 </h3><p> 通常就是指 dimensionality reduction。</p><ul><li><strong>Principal Component Analysis (PCA)</strong></li><li><strong>Latent Dirichlet Allocation (LDA)</strong></li><li><strong>Latent Semantic Analysis (LSA)</strong></li></ul><h3 id="2-10-Feature-Selection- 特征选择"><a href="#2-10-Feature-Selection- 特征选择" class="headerlink" title="2.10 Feature Selection 特征选择"></a>2.10 Feature Selection 特征选择 </h3><p> 特征选择是指通过某些方法自动地从所有的特征中挑选出有用的特征。</p><h4 id="2-10-1-Filter-Method"><a href="#2-10-1-Filter-Method" class="headerlink" title="2.10.1 Filter Method"></a>2.10.1 <strong>Filter Method</strong></h4><p>采用某一种评估指标（发散性、相关性或 Information Gain 等），单独地衡量个别特征跟 target variable 之间的关系，常用的方法有 Chi Square Test（卡方检验）。这种特征选择方式没有任何模型的参与。</p><p>以相关性来说，也不见得跟 target variable 的相关性越高就越好。</p><h4 id="2-10-2-Wrapper-Method"><a href="#2-10-2-Wrapper-Method" class="headerlink" title="2.10.2 Wrapper Method"></a>2.10.2 <strong>Wrapper Method</strong></h4><p>会采用某个模型来预测你的 target variable，把特征选择想成是一个组合优化的问题，想办法找出一组特征子集能够让模型的评估结果最好。缺点是太耗时间了，实际上不常用。</p><h4 id="2-10-3-Embedded-Method"><a href="#2-10-3-Embedded-Method" class="headerlink" title="2.10.3 Embedded Method"></a>2.10.3 <strong>Embedded Method</strong></h4><p>通常会采用一个会为特征赋予 coefficients 或 importances 的演算法，例如 Logistic Regression（特别是使用 L1 penalty）或 GBDT，直接用权重或重要性对所有特征排序，然后取前 n 个作为特征子集。</p><h3 id="2-11-Feature-Learning- 特征学习"><a href="#2-11-Feature-Learning- 特征学习" class="headerlink" title="2.11 Feature Learning 特征学习"></a>2.11 Feature Learning 特征学习 </h3><p> 也称为 Representation Learning 或 Automated Feature Engineering。</p><ul><li><strong>GBDT</strong></li><li><strong>Neural Network: Restricted Boltzmann Machines</strong></li><li><strong>Deep Learning: Autoencoder</strong></li></ul><h3 id="Step-3-Choose-the-Model"><a href="#Step-3-Choose-the-Model" class="headerlink" title="Step 3: Choose the Model"></a>Step 3: Choose the Model</h3><h4 id="LIBFFM-GBDT"><a href="#LIBFFM-GBDT" class="headerlink" title="LIBFFM + GBDT"></a>LIBFFM + GBDT</h4><p>This model is called <strong>Field-aware Factorization Machines</strong>. If you want to use this model, please download <a href="http://www.csie.ntu.edu.tw/~r01922136/libffm" target="_blank" rel="noopener">LIBFFM</a> first.</p><h4 id="CatBoost"><a href="#CatBoost" class="headerlink" title="CatBoost"></a>CatBoost</h4><h4 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h4><h4 id="LightBGM"><a href="#LightBGM" class="headerlink" title="LightBGM"></a>LightBGM</h4>]]></content>
    
    <summary type="html">
    
      本文主要介绍了一些本人在 Kaggle 上首战的经验，包括数据的 EDA 、FE 特征工程、所选择的模型、模型融合的以及其他。
    
    </summary>
    
      <category term="Kaggle" scheme="http://randolph.pro/categories/Kaggle/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Kaggle" scheme="http://randolph.pro/tags/Kaggle/"/>
    
  </entry>
  
  <entry>
    <title>♝「Papers」 About Field-aware Factorization Machines</title>
    <link href="http://randolph.pro/2017/11/11/%E2%99%9D%E3%80%8CPapers%E3%80%8DAbout%20Field-aware%20Factorization%20Machines/"/>
    <id>http://randolph.pro/2017/11/11/♝「Papers」About Field-aware Factorization Machines/</id>
    <published>2017-11-10T16:00:00.000Z</published>
    <updated>2019-03-18T05:53:51.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm5.staticflickr.com/4373/35507009504_3298ce3029_o.jpg" alt></p><p>有关「Machine Learning」的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/">「Machine Learning」</a><br>有关「Papers」的其他论文学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/Papers/">「Papers」</a></p><h2 id="「Field-aware-Factorization-Machines-for-CTR-Prediction-」"><a href="#「Field-aware-Factorization-Machines-for-CTR-Prediction-」" class="headerlink" title="「Field-aware Factorization Machines for CTR Prediction 」"></a>「Field-aware Factorization Machines for CTR Prediction 」</h2><p>FM 和 FFM 模型都是最近几年提出的模型，凭借其在数据量比较大并且特征稀疏的情况下，仍然能够得到优秀的性能和效果的特性，屡次在各大公司举办的 CTR 预估比赛中获得不错的战绩。</p><p>FFM（Field-aware Factorization Machine）最初的概念来自 Yu-Chin Juan（阮毓钦，毕业于中国台湾大学，现在美国 Criteo 工作）与其比赛队员，是他们借鉴了来自 Michael Jahrer 的论文 <a href="https://kaggle2.blob.core.windows.net/competitions/kddcup2012/2748/media/Opera.pdf" target="_blank" rel="noopener">「Ensemble of Collaborative Filtering and Feature Engineered Models for Click Through Rate Prediction」</a> 中的 field 概念提出了 FM 的升级版模型，该篇于 2016 年发布。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>在 CTR 任务中，大部分样本数据特征是非常稀疏的，使用  One-hot 编码会造成特征空间剧增。而通过对大量样本数据的观察发现某些特征经过关联之后，与最后预测的 label  的相关性就会提高。</p><p>因此使用多项式模型，考虑特征之间的组合会更加直观。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Poly-2"><a href="#Poly-2" class="headerlink" title="Poly 2"></a>Poly 2</h3><p>在多项式模型中，特征 $x_i$ 和 $x_j$ 的组合采用 $x_ix_j$ 表示，即 $x_i$ 和 $x_j$ 都非零时，组合特征 $x_ix_j$ 才有意义。典型的二阶多项式模型的表达式如下：</p><script type="math/tex; mode=display">y(\mathbf{x}) = w_0+ \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n w_{ij} x_i x_j</script><p>其中，$n$ 代表样本的特征数量，<script type="math/tex">x_ i</script> 是第 <script type="math/tex">i</script>个特征的值，<script type="math/tex">w_0</script>、<script type="math/tex">w_i</script>、<script type="math/tex">w_{ij}</script> 是模型参数。这也是 Poly2 （Degree-2 Polynomial Model ）模型的表达式。</p><p>从公式（1）可以看出，组合特征的参数一共有 $\frac{n(n−1)}{2}$ 个，任意两个参数都是独立的。然而，在数据稀疏性普遍存在的实际应用场景中，二次项参数的训练是很困难的。其原因是，每个参数 <script type="math/tex">w_{ij}</script> 的训练需要大量 <script type="math/tex">x_i</script> 和 <script type="math/tex">x_j</script> 都非零的样本；由于样本数据本来就比较稀疏，满足“<script type="math/tex">x_i</script> 和 <script type="math/tex">x_j</script> 都非零”的样本将会非常少。训练样本的不足，很容易导致参数 <script type="math/tex">w_{ij}</script> 不准确，最终将严重影响模型的性能。</p><h3 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h3><p>因此，为了解决二次项参数的训练问题，FM 模型基于矩阵分解的思想，在原有表达式对其稍微进行了一点改动：</p><script type="math/tex; mode=display">y(\mathbf{x}) = w_0+ \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \left \langle v_{i},v_{j} \right \rangle x_i x_j</script><p>其中，$n$ 代表样本的特征数量，<script type="math/tex">x_ i</script> 是第 $i$ 个特征的值，<script type="math/tex">w_0</script>、<script type="math/tex">w_i</script>、<script type="math/tex">v_{i}</script>、<script type="math/tex">v_{j}</script> 是模型参数。<script type="math/tex">v_{i}</script>、<script type="math/tex">v_{j}</script> 表示长度为 $k $ 的隐向量，包含 $k$ 个描述特征的因子，$k$ 为超参数，<script type="math/tex">\left \langle v_{i},v_{j} \right \rangle</script> 表示向量点积。</p><p>根据公式，二次项的参数数量减少为 $kn$ 个，远小于 Poly2 模型的参数数量。而且，最重要的是，参数的因子化使得 <script type="math/tex">x_ix_j</script> 的参数和 <script type="math/tex">x_ix_t</script> 的参数不再是相互独立的，因此我们可以在样本稀疏的情况下相对合理地估计 FM 的二次项参数。具体来说，<script type="math/tex">x_ix_j</script> 和 <script type="math/tex">x_ix_t</script> 的系数分别为 <script type="math/tex">\left \langle v_{i},v_{j} \right \rangle</script> 和 <script type="math/tex">\left \langle v_{i},v_{t} \right \rangle</script>，它们之间有共同项 <script type="math/tex">v_i</script>。也就是说，所有包含“<script type="math/tex">x_i</script> 的非零组合特征”（存在某个 $j≠i$，使得 <script type="math/tex">x_ix_j≠0</script>）的样本都可以用来学习隐向量 <script type="math/tex">v_i</script>，这很大程度上避免了数据稀疏性造成的影响。而在 Poly2 模型中，<script type="math/tex">w_iw_j</script> 和 <script type="math/tex">w_iw_t</script> 是相互独立的。</p><p>FM 模型比起 Poly 2 模型，其优点显而易见：</p><ul><li>通常 CTR 任务的数据量都是十分庞大的，FM 的参数数量比起 Poly 2 要明显减少，减少了模型的训练时间。</li><li>FM 的参数并不是相互独立，可以从其他的参数学习得到，在样本稀疏性非常明显的情况下，能够的得到更好更准确的参数，提高了模型的精度。</li></ul><h3 id="FFM"><a href="#FFM" class="headerlink" title="FFM"></a>FFM</h3><p>而我们今天的主角 FFM 模型，在 FM 模型表达式的基础上，更进一步：</p><script type="math/tex; mode=display">y(\mathbf{x}) = w_0+ \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \left \langle v_{i,f_{2}},v_{j,f_{1}} \right \rangle x_i x_j</script><p>其中，$n$ 代表样本的特征数量，<script type="math/tex">x_ i</script> 是第 $i$ 个特征的值，<script type="math/tex">w_0</script>、<script type="math/tex">w_i</script>、<script type="math/tex">v_{i,f_{2}}</script>、<script type="math/tex">v_{j,f_{1}}</script> 是模型参数。<script type="math/tex">v_{i,f_{2}}</script>、<script type="math/tex">v_{j,f_{1}}</script> 表示长度为 $k$ 的隐向量，包含 $k$ 个描述特征的因子，$k$ 为超参数，<script type="math/tex">\left \langle v_{i},v_{j} \right \rangle</script> 表示向量点积。<script type="math/tex">\left \langle v_{i},v_{j} \right \rangle</script> 变成了 <script type="math/tex">\left \langle v_{i,f_{2}},v_{j,f_{1}} \right \rangle</script> ，其中 <script type="math/tex">f_1</script>、<script type="math/tex">f_2</script> 表示 <script type="math/tex">x_i</script> 与 <script type="math/tex">x_j</script> 所属的 field。</p><p>举个例子：</p><div class="table-container"><table><thead><tr><th>Clicked</th><th>Publisher(P)</th><th>Advertiser(A)</th><th>Gender(G)</th></tr></thead><tbody><tr><td>Yes</td><td>ESPN</td><td>Nike</td><td>Male</td></tr></tbody></table></div><p>对于 FM 模型而言，$\phi_{FM}(w,x)$ 为：</p><script type="math/tex; mode=display">\phi_{FM}(w,x) = w_{ESPN} \cdot w_{Nike} + w_{ESPN} \cdot w_{Male} + w_{Nike} \cdot w_{Male}</script><p>而对于 FFM 模型而言，$\phi_{FFM}(w, x)$ 为：</p><script type="math/tex; mode=display">\phi_{FM}(w,x) = w_{ESPN,A} \cdot w_{Nike,P} + w_{ESPN,G} \cdot w_{Male,P} + w_{Nike,G} \cdot w_{Male,A}</script><p>在 FM 模型中，每个特征只有一个隐向量需要学习，而 FFM 则需要学习多个隐向量，取决于与其组合的特征的 field。例如对于特征 ESPN 的参数 <script type="math/tex">w_{ESPN}</script> ，它可以通过与特征 Nike 组合（<script type="math/tex">w_{ESPN} \cdot w_{Nike}</script>）或者 特征 Male 的组合（<script type="math/tex">w_{ESPN} \cdot w_{Male}</script>）来学习。然而，由于 Nike 与 Male 分别属于不同的 field ，因此（ESPN, NIKE）与（ESPN, Male）所造成的影响是不一样。</p><p>可以看到，<script type="math/tex">w_{ESPN, A}</script> 是因为 Nike 的 field 为 Advertiser（A），<script type="math/tex">w_{ESPN,G}</script> 是因为 Male 的 field 为 Gender（G）。</p><h4 id="FFM- 模型数据格式"><a href="#FFM- 模型数据格式" class="headerlink" title="FFM 模型数据格式"></a>FFM 模型数据格式</h4><script type="math/tex; mode=display">label \quad field_1:feat_1:val_1 \quad field_2:feat_2:val_2 \quad ...</script><p> 对于大多数特征都可以用这样的方法表示，但是对于一些特征：</p><ol><li><p>类别型特征</p><p>例如上表的数据，处理成 FFM 数据格式的话：</p><script type="math/tex; mode=display">1 \quad P:ESPN:1 \quad A:Nike:1 \quad G:Male:1</script></li><li><p>数值型特征</p><p>对于数值型特征，例如下表数据：</p></li></ol><div class="table-container"><table><thead><tr><th>Accepted</th><th>AR</th><th>Hidx</th><th>Cite</th></tr></thead><tbody><tr><td>Yes</td><td>45.73</td><td>2</td><td>3</td></tr><tr><td>No</td><td>1.04</td><td>100</td><td>50000</td></tr></tbody></table></div><p>   有两种处理成 FFM 数据格式的方式：</p><ul><li><p>Treat each feature as a dummy field:</p><script type="math/tex; mode=display">1 \quad AR:AR:45.73 \quad Hidx:Hidx:2 \quad Cite:Cite:3</script></li><li><p>Discretize each numerical feature to a categorical one:</p><script type="math/tex; mode=display">1 \quad AR:45:1 \quad Hidx:2:1 \quad Cite:3:1</script><p>注意到第二种处理方法，将 AR 这 field 中的特征值进行了处理，将 $45.73$ 处理成 $45.7$、$45$、$40$ 甚至是 $int(log(45.73))$ 都是可行的。</p></li></ul><ol><li><p>单 field 特征</p><p>经常在 NLP 任务上出现，例如如下表数据：</p></li></ol><div class="table-container"><table><thead><tr><th>good mood</th><th>sentence</th></tr></thead><tbody><tr><td>Yes</td><td>Hooray!  Our paper is accepted!</td></tr><tr><td>No</td><td>Well, our paper is rejected..</td></tr></tbody></table></div><p>   所有的特征都属于同一个 field — <code>sentence</code>，那么我们的做法就是将 <code>sentence</code> 这一 field 放置在每个分词特征之前，相当于是从 FFM 降低到 FM。 </p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><h4 id="Experiment-Setting"><a href="#Experiment-Setting" class="headerlink" title="Experiment Setting"></a>Experiment Setting</h4><p><strong>1. 数据集</strong></p><p>数据集为 Kaggle 两个比赛的数据集：</p><ul><li>Criteo</li><li>Avazu</li></ul><div class="table-container"><table><thead><tr><th>Data Set</th><th>instances</th><th>features</th><th>fields</th></tr></thead><tbody><tr><td>Criteo</td><td>45,840,617</td><td>$10^6$</td><td>39</td></tr><tr><td>Avazu</td><td>40,428,968</td><td>$10^6$</td><td>33</td></tr></tbody></table></div><p><strong>2. 模型训练及参数</strong></p><p>模型的优化方法为普通的 SG （Stochastic Gradient），再加上 FFM 中需要我们设定的超参数 $k$ ，因此模型的参数为： </p><ul><li>$k$ 隐向量的长度；</li><li>$\lambda$ 学习率；</li><li>$\eta$ 步长；</li></ul><p><strong>3. 实验结果</strong></p><p><img src="https://farm5.staticflickr.com/4552/24489629838_2654e8c9e6_o.png" alt="FFM"></p><h4 id="Results-and-Discussion"><a href="#Results-and-Discussion" class="headerlink" title="Results and Discussion"></a>Results and Discussion</h4><p>相较于 LM、Poly 2 以及 FM 模型，FFM 在该两个数据集上表现更好，拥有更高的准确率。</p>]]></content>
    
    <summary type="html">
    
      本文主要介绍了 Field-aware Factorization Machines 在 广告点击率（CTR）任务上的一篇论文。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
      <category term="CTR" scheme="http://randolph.pro/categories/Machine-Learning/CTR/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Papers" scheme="http://randolph.pro/tags/Papers/"/>
    
  </entry>
  
  <entry>
    <title>♛「Machine Learning」 How to Generate a Good Word Embedding?</title>
    <link href="http://randolph.pro/2017/10/14/%E2%99%9B%E3%80%8CMachine%20Learning%E3%80%8D%20How%20to%20Generate%20a%20Good%20Word%20Embedding/"/>
    <id>http://randolph.pro/2017/10/14/♛「Machine Learning」 How to Generate a Good Word Embedding/</id>
    <published>2017-10-13T16:00:00.000Z</published>
    <updated>2019-03-17T15:14:36.142Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm5.staticflickr.com/4300/35460446383_aafc34ca3c_o.jpg" alt></p><p> 有关「Machine Learning」的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/">「Machine Learning」</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h1 id="Compared-with-Other-Models"><a href="#Compared-with-Other-Models" class="headerlink" title="Compared with Other Models"></a>Compared with Other Models</h1><h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><p>  ​</p>]]></content>
    
    <summary type="html">
    
      本文介绍了如何生成简单有效的词向量的方法。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>♣︎「TensorFlow」 Cross Entropy in TensorFlow</title>
    <link href="http://randolph.pro/2017/09/25/%E2%99%A3%EF%B8%8E%E3%80%8CTensorFlow%E3%80%8DCross%20Entropy%20Function%20in%20TensorFlow%20/"/>
    <id>http://randolph.pro/2017/09/25/♣︎「TensorFlow」Cross Entropy Function in TensorFlow /</id>
    <published>2017-09-24T16:00:00.000Z</published>
    <updated>2019-03-18T08:04:06.723Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm5.staticflickr.com/4399/36206153111_6662041dd1_o.png" alt></p><p>有关「TensorFlow」的其他学习笔记系列：<a href="http://randolph.pro/categories/TensorFlow/">「TensorFlow」</a></p><h1 id="Cross-Entropy-Introduction"><a href="#Cross-Entropy-Introduction" class="headerlink" title="Cross Entropy Introduction"></a>Cross Entropy Introduction</h1><p>交叉熵（Cross Entropy）是 Loss 函数的一种（也称为损失函数或代价函数），用于描述模型预测值与真实值的差距大小，常见的 Loss 函数就是均方平方差（Mean Squared Error），定义如下：</p><script type="math/tex; mode=display">C = \frac {(y-a)^2}{2}</script><p>平方差很好理解，预测值与真实值直接相减，为了避免得到负数取绝对值或者平方，再做平均就是均方平方差。注意这里预测值需要经过 sigmoid 激活函数，得到取值范围在 0 到 1 之间的预测值。</p><p>平方差可以表达预测值与真实值的差异，但在分类问题种效果并不如交叉熵好，原因是：</p><ol><li>非负性。交叉熵输出的值是非负的。</li><li>当真实值 $a$ 与期望输出 $y$ 的误差越大，权值更新就快；误差越小，权值更新就慢。而使用平方差作为 Loss 函数，其权值更新往往很慢。</li><li>其中用于计算的 $a$ 也是经过 sigmoid 激活的，取值范围在 0 到 1。如果 label 是 1，预测值也是 1 的话，前面一项 <script type="math/tex">y * ln(a)</script> 就是 <script type="math/tex">1 * ln(1)</script> 等于 0，后一项 <script type="math/tex">(1 – y) * ln(1 – a)</script> 也就是 <script type="math/tex">0 * ln(0)</script> 等于 0，Loss 函数为 0，反之 Loss 函数为无限大非常符合我们对 Loss 函数的定义。</li></ol><p>交叉熵的定义如下：</p><script type="math/tex; mode=display">C = -\frac{1}{n}\sum_{x}[yln(a)+(1-y)ln(1-a)]</script><p>其中，$a = \sigma(z)$，<script type="math/tex">z=\sum_{j}w_{j}x_{j}+b</script>，$n$ 是训练数据的个数。</p><p>这里多次强调 sigmoid 激活函数，是因为在多标签或者多分类的问题下有些函数是不可用的，而 TensorFlow 本身也提供了多种交叉熵算法的实现。</p><h1 id="Cross-Entropy-in-TensorFlow"><a href="#Cross-Entropy-in-TensorFlow" class="headerlink" title="Cross Entropy in TensorFlow"></a>Cross Entropy in TensorFlow</h1><p>TensorFlow 针对分类问题，实现了四个交叉熵函数，分别是：</p><ul><li><a href="https://www.tensorflow.org/versions/master/api_docs/python/nn.html#sigmoid_cross_entropy_with_logits" target="_blank" rel="noopener">tf.nn.sigmoid_cross_entropy_with_logits</a></li><li><a href="https://www.tensorflow.org/versions/master/api_docs/python/nn.html#softmax_cross_entropy_with_logits" target="_blank" rel="noopener">tf.nn.softmax_cross_entropy_with_logits</a></li><li><a href="https://www.tensorflow.org/versions/master/api_docs/python/nn.html#sparse_softmax_cross_entropy_with_logits" target="_blank" rel="noopener">tf.nn.sparse_softmax_cross_entropy_with_logits</a></li><li><a href="https://www.tensorflow.org/versions/master/api_docs/python/nn.html#weighted_cross_entropy_with_logits" target="_blank" rel="noopener">tf.nn.weighted_cross_entropy_with_logits</a></li></ul><p><a href="https://www.tensorflow.org/versions/master/api_docs/python/nn.html#sparse_softmax_cross_entropy_with_logits" target="_blank" rel="noopener">详细内容参考 API 文档</a></p><h2 id="sigmoid-cross-entropy-with-logits"><a href="#sigmoid-cross-entropy-with-logits" class="headerlink" title="sigmoid_cross_entropy_with_logits"></a>sigmoid_cross_entropy_with_logits</h2><p>我们先看 <code>sigmoid_cross_entropy_with_logits</code>，为什么呢，因为它的实现和前面的交叉熵算法定义是一样的，也是 TensorFlow 最早实现的交叉熵算法。这个函数的输入是 logits 和 targets，logits 就是神经网络模型中的 $W * X$ 矩阵，注意不需要经过 sigmoid，而 targets 的 shape 和 logits 相同，就是正确的 label 值，例如这个模型一次要判断 100 张图是否包含 10 种动物，这两个输入的 shape 都是 $[100, 10]$。注释中还提到这 10 个分类之间是独立的、不要求是互斥，这种问题我们称之为多目标或者多标签分类问题，例如判断图片中是否包含 10 种动物，label 值可以包含多个 1 或 0 个 1。这种问题，我们可以使用 <code>sigmoid_cross_entropy_with_logits</code> 函数。</p><p>另外，还有一种问题是多分类问题，例如我们对年龄特征分为 5 段，只允许 5 个值有且只有 1 个值为 1，这种问题可以直接用这个函数吗？答案是不可以，我们先来看看 <code>sigmoid_cross_entropy_with_logits</code> 的代码实现吧。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">For brevity, let `x = logits`, `z = labels`.  The logistic loss is</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))</span></span><br><span class="line"><span class="string">    = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))</span></span><br><span class="line"><span class="string">    = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))</span></span><br><span class="line"><span class="string">    = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))</span></span><br><span class="line"><span class="string">    = (1 - z) * x + log(1 + exp(-x))</span></span><br><span class="line"><span class="string">    = x - x * z + log(1 + exp(-x))</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>可以看到这就是标准的 Cross Entropy 算法实现，对 $W * X$ 得到的值进行 sigmoid 激活，保证取值在 0 到 1 之间，然后放在交叉熵的函数中计算 Loss。对于二分类问题这样做没问题，但对于前面提到的多分类问题，例如年龄取值范围在 $[0, 4]$，目标值也在 $[0, 4]$，这里如果经过 sigmoid 后预测值就限制在 0 到 1 之间，而且公式中的 $(1 – z)$ 就会出现负数，仔细想一下 0 到 4 之间还不存在线性关系，如果直接把 label 值带入计算肯定会有非常大的误差。但是对于这多分类问题， TensorFlow 又提供了基于 Softmax 的交叉熵函数。</p><p>因此对于多分类问题是不能直接代入的，但是对于多标签分类问题，我们可以灵活变通，把 5 个年龄段的预测用 <strong>onehot encoding</strong> 变成 5 维的 label，训练时当做 5 个不同的目标来训练即可，但不保证只有一个为 1。</p><h2 id="softmax-cross-entropy-with-logits"><a href="#softmax-cross-entropy-with-logits" class="headerlink" title="softmax_cross_entropy_with_logits"></a>softmax_cross_entropy_with_logits</h2><p>Softmax 本身的算法很简单，就是把所有值用 $e^n$ 计算出来，求和后算每个值占的比率，保证总和为 1，一般我们可以认为 Softmax 出来的就是 confidence，也就是概率。</p><script type="math/tex; mode=display">softmax = \frac{exp(logits)}{reduce\_sum(exp(logits),  dim)}</script><p><code>softmax_cross_entropy_with_logits</code> 和 <code>sigmoid_cross_entropy_with_logits</code> 很不一样，输入是类似的 logits 和 lables 的 shape 一样，但这里要求分类的结果是互斥的，保证只有一个字段有值，例如 CIFAR-10 中图片只能分一类而不像前面判断是否包含多类动物。</p><p>想一下问什么会有这样的限制？在函数头的注释中我们看到，这个函数传入的 logits 是 unscaled 的，既不做 sigmoid 也不做 softmax，因为函数实现会在内部更高效得使用 softmax，对于任意的输入经过 softmax 都会变成和为 1 的概率预测值，这个值就可以代入变形的 Cross Entroy 算法：</p><script type="math/tex; mode=display">- y * ln(a) – (1 – y) * ln(1 – a)</script><p>从而得到有意义的 Loss 值了。如果是多标签多目标问题，经过 softmax 就不会得到多个和为 1 的概率，而且 label 有多个 1 也无法计算交叉熵，因此这个函数只适合单目标的二分类或者多分类问题。</p><p>再补充一点，对于多分类问题，例如我们的年龄分为 5 类，并且人工编码为 0、1、2、3、4，因为输出值是 5 维的特征，因此我们需要人工做 <strong>onehot encoding</strong> 分别编码为 00001、00010、00100、01000、10000，才可以作为这个函数的输入。理论上我们不做 onehot encoding 也可以，做成和为 1 的概率分布也可以，但需要保证是和为 1，和不为 1 的实际含义不明确，TensorFlow 的 C++ 代码实现计划检查这些参数，可以提前提醒用户避免误用。</p><h2 id="sparse-softmax-cross-entropy-with-logits"><a href="#sparse-softmax-cross-entropy-with-logits" class="headerlink" title="sparse_softmax_cross_entropy_with_logits"></a>sparse_softmax_cross_entropy_with_logits</h2><p><code>sparse_softmax_cross_entropy_with_logits</code> 是 <code>softmax_cross_entropy_with_logits</code>的易用版本，除了输入参数不同，作用和算法实现都是一样的。前面提到 <code>softmax_cross_entropy_with_logits</code> 的输入必须是类似 <strong>onehot encoding</strong> 的多维特征，但 CIFAR-10、ImageNet 和大部分分类场景都只有一个分类目标，label 值都是从 0 编码的整数，每次转成 onehot encoding 比较麻烦，有没有更好的方法呢？答案就是用 <code>sparse_softmax_cross_entropy_with_logits</code>：</p><ul><li>它的第一个参数 logits 和前面一样，shape 是 <code>[batch_size, num_classes]</code>。</li><li>而第二个参数 labels，<code>softmax_cross_entropy_with_logits</code>规定 shape 必须是 <code>[batch_size, num_classes]</code>，否则无法做 Cross Entropy。这意味着，labels 的值必须是从 0 开始编码的 int32 或 int64，而且值范围是 <code>[0, num_class)</code>，如果我们从 1 开始编码或者步长大于 1，会导致某些 label 值超过这个范围，代码会直接报错退出，这也很好理解，TensorFlow 通过这样的限制才能知道用户传入的 3、6 或者 9 对应是哪个 class。而 <strong><code>sparse</code></strong> 版本，则允许 labels 的 shape 是 <code>[batch_size]</code>，通过在内部高效实现类似的 <strong>onehot encoding</strong>，从而简化用户的输入。</li></ul><p>因此，如果用户已经做了 <strong>onehot encoding</strong> 那可以直接使用不带 <code>softmax_cross_entropy_with_logits</code> 函数，如果还没有进行 <strong>onehot encoding</strong>，则可以选择使用 <code>sparse_softmax_cross_entropy_with_logits</code> 函数。</p><h2 id="weighted-cross-entropy-with-logits"><a href="#weighted-cross-entropy-with-logits" class="headerlink" title="weighted_cross_entropy_with_logits"></a>weighted_cross_entropy_with_logits</h2><p><code>weighted_cross_entropy_with_logits</code> 是 <code>sigmoid_cross_entropy_with_logits</code> 的拓展版，输入参数和实现和后者差不多，只是可以多支持一个 <code>pos_weight</code> 参数，目的是可以增加或者减小正样本在算 Cross Entropy 时的 Loss。实现原理很简单，在传统基于 Sigmoid 的交叉熵算法上，正样本算出的值乘以某个系数 <strong><code>pos_weight</code></strong>，算法实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">The usual cross-entropy cost is defined as:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    targets * -log(sigmoid(logits)) +</span></span><br><span class="line"><span class="string">        (1 - targets) * -log(1 - sigmoid(logits))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">The argument `pos_weight` is used as a multiplier for the positive targets:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    targets * -log(sigmoid(logits)) * pos_weight +</span></span><br><span class="line"><span class="string">        (1 - targets) * -log(1 - sigmoid(logits))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">For brevity, let `x = logits`, `z = targets`, `q = pos_weight`.</span></span><br><span class="line"><span class="string">The loss is:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      qz * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))</span></span><br><span class="line"><span class="string">    = qz * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))</span></span><br><span class="line"><span class="string">    = qz * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))</span></span><br><span class="line"><span class="string">    = qz * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))</span></span><br><span class="line"><span class="string">    = (1 - z) * x + (qz +  1 - z) * log(1 + exp(-x))</span></span><br><span class="line"><span class="string">    = (1 - z) * x + (1 + (q - 1) * z) * log(1 + exp(-x))</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>这就是 TensorFlow 目前提供的有关 Cross Entropy 的函数实现，用户需要理解多标签和多分类的场景，根据业务需求（分类目标是否独立和互斥）来选择基于 Sigmoid 或者 Softmax 的实现，如果使用 Sigmoid 目前还支持加权的实现，如果使用 Softmax 我们可以自己做 onehot coding 或者使用更易用的 <code>sparse_softmax_cross_entropy_with_logits</code> 函数。</p><p>TensorFlow 提供的 Cross Entropy 函数基本涵盖了多目标和多分类的问题，但如果同时是多目标多分类的场景，肯定是无法使用 <code>softmax_cross_entropy_with_logits</code>，如果使用<code>sigmoid_cross_entropy_with_logits</code> 我们就把多分类的特征都认为是独立的特征，而实际上他们有且只有一个为 1 的非独立特征，计算 Loss 时不如 Softmax 有效。这里可以预测下，未来 TensorFlow 社区将会实现更多的 op 解决类似的问题，我们也期待更多人参与 TensorFlow 贡献算法和代码。</p>]]></content>
    
    <summary type="html">
    
      交叉熵（Cross Entropy）是 Loss 函数的一种（也称为损失函数或代价函数），用于描述模型预测值与真实值的差距大小，TensorFlow 本身也提供了多种交叉熵算法的实现，本文详细了 TensorFlow 中各个交叉熵函数的用法。
    
    </summary>
    
      <category term="TensorFlow" scheme="http://randolph.pro/categories/TensorFlow/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
      <category term="TensorFlow" scheme="http://randolph.pro/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>♛「Machine Learning」 LSTM Introduction</title>
    <link href="http://randolph.pro/2017/09/08/%E2%99%9B%E3%80%8CMachine%20Learning%E3%80%8D%20LSTM%20Introduction/"/>
    <id>http://randolph.pro/2017/09/08/♛「Machine Learning」 LSTM Introduction/</id>
    <published>2017-09-07T16:00:00.000Z</published>
    <updated>2019-03-17T15:14:49.219Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm5.staticflickr.com/4300/35460446383_aafc34ca3c_o.jpg" alt></p><p>有关「Machine Learning」的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/">「Machine Learning」</a></p><h1 id="循环神经网络 -RNN"><a href="# 循环神经网络 -RNN" class="headerlink" title="循环神经网络 (RNN)"></a> 循环神经网络 (RNN)</h1><p> 人们的每次思考并不都是从零开始的。比如说你在阅读这篇文章时，你基于对前面的文字的理解来理解你目前阅读到的文字，而不是每读到一个文字时，都抛弃掉前面的思考，从头开始。你的记忆是有持久性的。</p><p>传统的神经网络并不能如此，这似乎是一个主要的缺点。例如，假设你在看一场电影，你想对电影里的每一个场景进行分类。传统的神经网络不能够基于前面的已分类场景来推断接下来的场景分类。</p><p>循环神经网络 (Recurrent Neural Networks) 解决了这个问题。这种神经网络带有环，可以将信息持久化。</p><p><img src="http://7xqwhn.com1.z0.glb.clouddn.com/2016-10-19-14585451475265.jpg" alt="Recurrent Neural Networks have loops. -c100"></p><p>在上图所示的神经网络 $A$ 中，输入为 $X_t$，输出为 $h_t$。$A$ 上的环允许将每一步产生的信息传递到下一步中。环的加入使得 RNN 变得神秘。不过，如果你多思考一下的话，其实 RNN 跟普通的神经网络也没有那么不同。一个 RNN 可以看作是同一个网络的多份副本，每一份都将信息传递到下一个副本。如果我们将环展开的话：</p><p><img src="http://7xqwhn.com1.z0.glb.clouddn.com/2016-10-19-14585464218639.png" alt="An unrolled recurrent neural network. -c600"></p><p>这种链式结构展示了 RNN 与序列和列表的密切关系。RNN 的这种结构能够非常自然地使用这类数据。而且事实的确如此。在过去的几年里，RNN 在一系列的任务中都取得了令人惊叹的成就，比如语音识别，语言建模，翻译，图片标题等等。关于 RNN 在各个领域所取得的令人惊叹的成就，参见 <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener"> 这篇文章</a>。</p><p>LSTM 是这一系列成功中的必要组成部分。LSTM(Long Short Term Memory)是一种特殊的循环神经网络，在许多任务中，LSTM 表现得比标准的 RNN 要出色得多。几乎所有基于 RNN 的令人赞叹的结果都是 LSTM 取得的。本文接下来将着重介绍 LSTM。</p><h2 id="长期依赖 -Long-Term-Dependencies- 的问题"><a href="# 长期依赖 -Long-Term-Dependencies- 的问题" class="headerlink" title="长期依赖 (Long Term Dependencies) 的问题"></a>长期依赖 (Long Term Dependencies) 的问题</h2><p>RNN 的一个核心思想是将以前的信息连接到当前的任务中来，例如，通过前面的视频帧来帮助理解当前帧。如果 RNN 真的能够这样做的话，那么它们将会极其有用。但是事实真是如此吗？未必。</p><p>有时候，我们只需要看最近的信息，就可以完成当前的任务。比如，考虑一个语言模型，通过前面的单词来预测接下来的单词。如果我们想预测句子“the clouds are in the <em>sky</em>”中的最后一个单词，我们不需要更多的上下文信息——很明显下一个单词应该是 sky。在这种情况下，当前位置与相关信息所在位置之间的距离相对较小，RNN 可以被训练来使用这样的信息。</p><p><img src="http://7xqwhn.com1.z0.glb.clouddn.com/2016-10-19-14585476990034.jpg" alt="-c400"></p><p>然而，有时候我们需要更多的上下文信息。比如，我们想预测句子“I grew up in France… I speak fluent <em>French</em>”中的最后一个单词。最近的信息告诉我们，最后一个单词可能是某种语言的名字，然而如果我们想确定到底是哪种语言的话，我们需要 France 这个更远的上下文信息。实际上，相关信息和需要该信息的位置之间的距离可能非常的远。</p><p>不幸的是，随着距离的增大，RNN 对于如何将这样的信息连接起来无能为力。</p><p><img src="http://7xqwhn.com1.z0.glb.clouddn.com/2016-10-19-14585481509571.png" alt="-c600"></p><p>理论上说，RNN 是有能力来处理这种长期依赖 (Long Term Dependencies) 的。人们可以通过精心调参来构建模型处理一个这种玩具问题 (Toy Problem)。不过，在实际问题中，RNN 并没有能力来学习这些。<a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="noopener">Hochreiter (1991) German</a> 更深入地讲了这个问题，<a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="noopener">Bengio, et al. (1994)</a>发现了 RNN 的一些非常基础的问题。</p><p>幸运的是，LSTM 并没有上述问题！</p><h2 id="LSTM 网络"><a href="#LSTM 网络" class="headerlink" title="LSTM 网络"></a>LSTM 网络 </h2><p>LSTM，全称为长短期记忆网络(Long Short Term Memory networks)，是一种特殊的 RNN，能够学习到长期依赖关系。LSTM 由<a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="noopener">Hochreiter &amp; Schmidhuber (1997)</a> 提出，许多研究者进行了一系列的工作对其改进并使之发扬光大。LSTM 在许多问题上效果非常好，现在被广泛使用。</p><p>LSTM 在设计上明确地避免了长期依赖的问题。记住长期信息是小菜一碟！所有的循环神经网络都有着重复的神经网络模块形成链的形式。在普通的 RNN 中，重复模块结构非常简单，例如只有一个 tanh 层。</p><p><img src="http://7xqwhn.com1.z0.glb.clouddn.com/2016-10-19-14585498578776.jpg" alt="The repeating module in a standard RNN contains a single layer. -c600"></p><p>LSTM 也有这种链状结构，不过其重复模块的结构不同。LSTM 的重复模块中有 4 个神经网络层，并且他们之间的交互非常特别。</p><p><img src="http://7xqwhn.com1.z0.glb.clouddn.com/2016-10-19-14585500063294.png" alt="The repeating module in an LSTM contains four interacting layers. -c600"></p><p>现在暂且不必关心细节，稍候我们会一步一步地对 LSTM 的各个部分进行介绍。开始之前，我们先介绍一下将用到的标记。</p><p><img src="http://7xqwhn.com1.z0.glb.clouddn.com/2016-10-19-14585501719114.jpg" alt="-c500"></p><p>在上图中，每条线表示向量的传递，从一个结点的输出传递到另外结点的输入。粉红圆表示向量的元素级操作，比如相加或者相乘。黄色方框表示神经网络的层。线合并表示向量的连接，线分叉表示向量复制。</p><h2 id="LSTM 核心思想"><a href="#LSTM 核心思想" class="headerlink" title="LSTM 核心思想"></a>LSTM 核心思想</h2><p>LSTM 的关键是元胞状态(Cell State)，下图中横穿整个元胞顶部的水平线。</p><p>元胞状态有点像是传送带，它直接穿过整个链，同时只有一些较小的线性交互。上面承载的信息可以很容易地流过而不改变。</p><p><img src="http://7xqwhn.com1.z0.glb.clouddn.com/2016-10-19-14585507601229.jpg" alt="-c600"></p><p>LSTM 有能力对元胞状态添加或者删除信息，这种能力通过一种叫门的结构来控制。</p><p>门是一种选择性让信息通过的方法。它们由一个 Sigmoid 神经网络层和一个元素级相乘操作组成。</p><p><img src="http://7xqwhn.com1.z0.glb.clouddn.com/2016-10-19-14585511223368.jpg" alt="-c100"></p><p>Sigmoid 层输出 0~1 之间的值，每个值表示对应的部分信息是否应该通过。0 值表示不允许信息通过，1 值表示让所有信息通过。一个 LSTM 有 3 个这种门，来保护和控制元胞状态。</p><h2 id="LSTM 分步详解"><a href="#LSTM 分步详解" class="headerlink" title="LSTM 分步详解"></a>LSTM 分步详解 </h2><p>LSTM 的第一步是决定我们将要从元胞状态中扔掉哪些信息。该决定由一个叫做“遗忘门(Forget Gate)”的 Sigmoid 层控制。遗忘门观察(h<em>{t-1}) 和(x</em>{t})，对于元胞状态 (C_{t-1}) 中的每一个元素，输出一个 0~1 之间的数。1 表示“完全保留该信息”，0 表示“完全丢弃该信息”。</p><p>回到之前的预测下一个单词的例子。在这样的一个问题中，元胞状态可能包含当前主语的性别信息，以用来选择正确的物主代词。当我们遇到一个新的主语时，我们就需要把旧的性别信息遗忘掉。</p><p><img src="http://7xqwhn.com1.z0.glb.clouddn.com/2016-10-19-14585517843913.jpg" alt="-c600"></p><p>下一步是决定我们将会把哪些新信息存储到元胞状态中。这步分为两部分。首先，有一个叫做“输入门(Input Gate)”的 Sigmoid 层决定我们要更新哪些信息。接下来，一个 tanh 层创造了一个新的候选值，$\tilde{C_t}$，该值可能被加入到元胞状态中。在下一步中，我们将会把这两个值组合起来用于更新元胞状态。</p><p>在语言模型的例子中，我们可能想要把新主语的性别加到元胞状态中，来取代我们已经遗忘的旧值。</p><p><img src="http://7xqwhn.com1.z0.glb.clouddn.com/2016-10-19-14585522130400.jpg" alt="-c600"></p><p>现在我们该更新旧元胞状态 $C_{t-1}$ 到新状态 $C_t$ 了。上面的步骤中已经决定了该怎么做，这一步我们只需要实际执行即可。</p><p>我们把旧状态 $C_{t-1}$ 乘以 $f_t$，忘掉我们已经决定忘记的内容。然后我们再加上 $i_t * \tilde{C_t}$，这个值由新的候选值（$\tilde{C_t}$）乘以候选值的每一个状态我们决定更新的程度（$i_t$）构成。</p><p>还是语言模型的例子，在这一步，我们按照之前的决定，扔掉了旧的主语的性别信息，并且添加了新的信息。</p><p><img src="http://7xqwhn.com1.z0.glb.clouddn.com/2016-10-19-14585647039038.jpg" alt="-c600"></p><p>最后，我们需要决定最终的输出。输出将会基于目前的元胞状态，并且会加入一些过滤。首先我们建立一个 Sigmoid 层的输出门(Output Gate)，来决定我们将输出元胞的哪些部分。然后我们将元胞状态通过 tanh 之后（使得输出值在 -1 到 1 之间），与输出门相乘，这样我们只会输出我们想输出的部分。</p><p>对于语言模型的例子，由于刚刚只输出了一个主语，因此下一步可能需要输出与动词相关的信息。举例来说，可能需要输出主语是单数还是复数，以便于我们接下来选择动词时能够选择正确的形式。</p><p><img src="http://7xqwhn.com1.z0.glb.clouddn.com/2016-10-19-14585652046323.jpg" alt="-c600"></p><h2 id="LSTM 的变种"><a href="#LSTM 的变种" class="headerlink" title="LSTM 的变种"></a>LSTM 的变种 </h2><p> 本文前面所介绍的 LSTM 是最普通的 LSTM，但并非所有的 LSTM 模型都与前面相同。事实上，似乎每一篇 paper 中所用到的 LSTM 都是稍微不一样的版本。不同之处很微小，不过其中一些值得介绍。</p><p>一个流行的 LSTM 变种，由 <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="noopener">Gers &amp; Schmidhuber (2000)</a> 提出，加入了“窥视孔连接(peephole connection)”。也就是说我们让各种门可以观察到元胞状态。</p><p><img src="http://7xqwhn.com1.z0.glb.clouddn.com/2016-10-19-14585655553885.jpg" alt="-c600"></p><p>上图中，对于所有的门都加入了“窥视孔”，不过也有一些 paper 中只加一部分。</p><p>另一种变种是使用 <strong> 对偶 </strong> 的遗忘门和输入门。我们不再是单独地决定需要遗忘什么信息，需要加入什么新信息；而是一起做决定：我们只会在需要在某处放入新信息时忘记该处的旧值；我们只会在已经忘记旧值的位置放入新值。</p><p><img src="http://7xqwhn.com1.z0.glb.clouddn.com/2016-10-19-14585661398239.jpg" alt="-c600"></p><p>另一个变化更大一些的 LSTM 变种叫做 Gated Recurrent Unit，或者 GRU，由 <a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="noopener">Cho, et al. (2014)</a> 提出。GRU 将遗忘门和输入门合并成为单一的“更新门 (Update Gate)”。GRU 同时也将元胞状态(Cell State) 和隐状态 (Hidden State) 合并，同时引入其他的一些变化。该模型比标准的 LSTM 模型更加简化，同时现在也变得越来越流行。</p><p><img src="http://7xqwhn.com1.z0.glb.clouddn.com/2016-10-19-14585667357562.png" alt="-c600"></p><p>另外还有很多其他的模型，比如 <a href="http://arxiv.org/pdf/1508.03790v2.pdf" target="_blank" rel="noopener">Yao, et al. (2015)</a> 提出的 Depth Gated RNNs。同时，还有很多完全不同的解决长期依赖问题的方法，比如 <a href="http://arxiv.org/pdf/1402.3511v1.pdf" target="_blank" rel="noopener">Koutnik, et al. (2014)</a> 提出的 Clockwork RNNs。</p><p>不同的模型中哪个最好？这其中的不同真的有关系吗？<a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="noopener">Greff, et al. (2015)</a>对流行的变种做了一个比较，发现它们基本相同。<a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="noopener">Jozefowicz, et al. (2015)</a>测试了一万多种 RNN 结构，发现其中的一些在特定的任务上效果比 LSTM 要好。</p><h2 id="结论"><a href="# 结论" class="headerlink" title="结论"></a>结论 </h2><p> 前文中，我提到了人们使用 RNN 所取得的出色的成就。本质上，几乎所有的成就都是由 LSTM 取得的。对于大部分的任务，LSTM 表现得非常好。</p><p>由于 LSTM 写在纸上是一堆公式，因此看起来很吓人。希望本文的分步讲解能让读者更容易接受和理解。</p><p>LSTM 使得我们在使用 RNN 能完成的任务上迈进了一大步。很自然，我们会思考，还会有下一个一大步吗？研究工作者们的共同观点是：“是的！还有一个下一步，那就是注意力 (Attention)！”注意力机制的思想是，在每一步中，都让 RNN 从一个更大的信息集合中去选择信息。举个例子，假如你使用 RNN 来生成一幅图片的说明文字，RNN 可能在输出每一个单词时，都会去观察图片的一部分。事实上，<a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="noopener">Xu, et al.(2015)</a> 做的正是这个工作！如果你想探索注意力机制的话，这会是一个很有趣的起始点。现在已经有很多使用注意力的令人兴奋的成果，而且似乎更多的成果马上将会出来……</p><p>注意力并不是 RNN 研究中唯一让人兴奋的主题。举例说，由 <a href="http://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="noopener">Kalchbrenner, et al. (2015)</a> 提出的 Grid LSTM 似乎极有前途。在生成式模型中使用 RNN 的工作——比如 <a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="noopener">Gregor, et al. (2015)</a>、<a href="http://arxiv.org/pdf/1506.02216v3.pdf" target="_blank" rel="noopener">Chung, et al. (2015)</a> 以及<a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank" rel="noopener">Bayer &amp; Osendorfer (2015)</a>——看起来也非常有意思。最近的几年对于 RNN 来说是一段非常令人激动的时间，接下来的几年也必将更加使人振奋！</p>]]></content>
    
    <summary type="html">
    
      本文主要介绍 Deep Learning 中的 LSTM 神经网络。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>♛「Machine Learning」 Deal with Imbalanced Data</title>
    <link href="http://randolph.pro/2017/09/06/%E2%99%9B%E3%80%8CMachine%20Learning%E3%80%8DDeal%20with%20Imbalanced%20Data/"/>
    <id>http://randolph.pro/2017/09/06/♛「Machine Learning」Deal with Imbalanced Data/</id>
    <published>2017-09-05T16:00:00.000Z</published>
    <updated>2019-03-18T05:56:53.975Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm5.staticflickr.com/4300/35460446383_aafc34ca3c_o.jpg" alt></p><p>有关「Machine Learning」的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/">「Machine Learning」</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>在机器学习中，常常会遇到样本比例不平衡的问题，如对于一个二分类问题，正负样本的比例是 10:1。这种现象往往是由于本身数据来源决定的，如信用卡的征信问题中往往就是正样本居多。样本比例不平衡往往会带来不少问题，但是实际获取的数据又往往是不平衡的，因此本文主要讨论面对样本不平衡时的解决方法。</p><p>样本不平衡往往会导致模型对样本数较多的分类造成过拟合，即总是将样本分到了样本数较多的分类中；除此之外，一个典型的问题就是 <a href="https://en.wikipedia.org/wiki/Accuracy_paradox" target="_blank" rel="noopener">Accuracy Paradox</a>，这个问题指的是模型的对样本预测的准确率很高，但是模型的泛化能力差。其原因是模型将大多数的样本都归类为样本数较多的那一类，如下表所示:</p><div class="table-container"><table><thead><tr><th style="text-align:center">category</th><th style="text-align:center">Predicted Negative</th><th style="text-align:center">Predicted Positive</th></tr></thead><tbody><tr><td style="text-align:center">Negative Cases</td><td style="text-align:center">9700</td><td style="text-align:center">150</td></tr><tr><td style="text-align:center">Positive Cases</td><td style="text-align:center">50</td><td style="text-align:center">100</td></tr></tbody></table></div><p>准确率为：</p><script type="math/tex; mode=display">\frac{9700+100}{9700 + 150 + 50 + 100} = 0.98</script><p>而假如将所有的样本都归为预测为负样本，准确率会进一步上升，但是这样的模型显然是不好的，实际上，模型已经对这个不平衡的样本过拟合了。</p><p>针对样本的不平衡问题，可以从两个方面考虑：</p><ul><li>Data Level：<ul><li>Over-Sampling<ul><li>Random Over-Sampling</li><li>SMOTE</li><li>Borderline-SMOTE</li><li>ASASYN</li></ul></li><li>Under-Sampling<ul><li>Random Under-Samling</li><li>Easy Ensemble</li><li>Balance Cascade</li><li>NearMiss</li></ul></li><li>Integration of<ul><li>SMOTE + Tomek Links</li><li>SMOTE + ENN</li></ul></li><li>Different resampled ratios</li></ul></li><li>Algorithm Level：<ul><li>Try Different Algorithms</li><li>Try Penalized Models</li><li>Change the Performance Measures</li><li>Change the Model Training Process</li></ul></li></ul><p>但是，搜集更多的数据，从而让正负样本的比例平衡，这种方法往往是最被忽视的方法，然而实际上，当搜集数据的代价不大时，这种方法是最有效的。</p><p>但是需要注意，当搜集数据的场景本来产生数据的比例就是不平衡时，这种方法并不能解决数据比例不平衡问题。</p><h1 id="Data-Level"><a href="#Data-Level" class="headerlink" title="Data Level"></a>Data Level</h1><p>在数据层面上，改变数据分布，从数据层面使得类别分布更为平衡。</p><p>对于一个不平衡样本，其样本数多的为 <strong>majority</strong>，样本数少的为 <strong>minority</strong>。整个 trainingset 为 $S$，<strong>majority</strong> 记为 <script type="math/tex">S_{maj}</script>，<strong>minority</strong> 记为 <script type="math/tex">S_{min}</script>。用 <script type="math/tex">|S_{maj}|</script> 表示 <strong>majority</strong> 的样本数量，用 <script type="math/tex">|S_{min}|</script> 表示 <strong>minority</strong> 的样本数量。</p><p>对于随机采取部分 <strong>majority</strong> 的样本记为 <script type="math/tex">E_{maj}</script>，其样本数量用 <script type="math/tex">|E_{maj}|</script> 表示。<br>对于随机采取部分 <strong>minority</strong> 的样本记为 <script type="math/tex">E_{min}</script>，其样本数量用 <script type="math/tex">|E_{min}|</script> 表示。</p><p>对数据重采样可以有针对性地改变数据中样本的比例，采样一般有两种方式：<strong>Over-Sampling</strong> 和 <strong>Under-Sampling</strong>。</p><p><strong>Over-Sampling</strong> 是增加样本数较少的样本，其方式是直接复制原来的样本，通常简单的做法就是随机挑选 <script type="math/tex">S_{min}</script>，然后加入到 <script type="math/tex">S_{min}</script>当中，增加的量通常是 <script type="math/tex">|S_{maj}|-|S_{min}|</script>，因为增加了重复的样本，所以容易造成过拟合。</p><p><strong>Under-Sampling</strong> 是减少样本数较多的样本，其方式是丢弃这些多余的样本，通过简单的做法就是随机挑选 <script type="math/tex">S_{maj}</script>，然后将其去除，减少的量通常是 <script type="math/tex">|S_{maj}|-|S_{min}|</script>，由于去除了原本样本中可能存在的重要信息，所以会导致欠拟合。</p><p>通常来说，当总样本数目较少的时候考虑 <strong>Over-Sampling</strong>，而样本数数目较多的时候考虑 <strong>Under-Sampling</strong>。</p><h2 id="Over-Sampling"><a href="#Over-Sampling" class="headerlink" title="Over-Sampling"></a>Over-Sampling</h2><p>过采样方法是针对少数的正样本，增加正样本的数量，从而提高整体 $F$ 值。最简单的过采样方法是简单地复制一些正样本。过采样的缺点是没有给正样本增加任何新的信息，这样训练得到的模型会有出现一定的过拟合问题。</p><p>另外，过采样方法对 SVM 算法是无效的。因为 SVM 算法是找支持向量，复制正样本并不能改变数据的支持向量。</p><h3 id="Random-Over-Sampling"><a href="#Random-Over-Sampling" class="headerlink" title="Random Over-Sampling"></a>Random Over-Sampling</h3><p>随机过采样是增加少数类样本数量，可以事先设置多数类与少数类最终的数量比例，在保留多数类样本不变的情况下，根据比例随机复制少数类样本，在使用的过程中为了保证所有的少数类样本信息都会被包含，可以先完全复制一份全量的少数类样本，再随机复制少数样本使得满足数量比例，具体步骤如下：</p><ol><li>首先在少数类 $S_{min}$ 集合中随机选中一些少数类样本</li><li>然后通过复制所选样本生成样本集合 $E$</li><li>将它们添加到 <script type="math/tex">S_{min}</script> 中来扩大原始数据集从而得到新的少数类集合 <script type="math/tex">S_{min-new}</script></li></ol><p>最后，<script type="math/tex">S_{min}</script> 中的总样本数增加了 <script type="math/tex">|E|</script> 个新样本，且 <script type="math/tex">S_{min-new}</script> 的类分布均衡度进行了相应的调整，如此操作可以改变类分布平衡度从而达到所需水平。当然，重复的样本过多，容易造成分类器的过拟合。</p><p><img src="https://farm5.staticflickr.com/4362/37118453045_b9e9c4a044_o.png" alt></p><h3 id="SMOTE（Synthetic-Minority-Oversampling-Technique）"><a href="#SMOTE（Synthetic-Minority-Oversampling-Technique）" class="headerlink" title="SMOTE（Synthetic Minority Oversampling Technique）"></a>SMOTE（Synthetic Minority Oversampling Technique）</h3><p>在合成采样技术方面，Chawla NY 等人提出的 SMOTE 过采样技术是基于随机过采样算法的一种改进方案，由于随机过采样简单复制样本的策略来增加少数类样本，这样容易产生模型过拟合的问题，即使模型学习到的信息过于特别（Specific）而不够泛化(General)。</p><p>SMOTE 的主要思想是利用特征空间中现存少数类样本之间的相似性来建立人工数据，特别是，对于子集 <script type="math/tex">S_{min} \subset S</script>，对于每一个样本 <script type="math/tex">x_i \subset S_{min}</script> 使用 KNN 算法，其中 KNN 算法被定义为考虑 $S_{min}$ 中的 K 个元素本身与 <script type="math/tex">x_i</script> 的欧氏距离在 $n$ 维特征空间 $X$ 中表现为最小幅度值的样本。<strong>由于不是简单地复制少数类样本，因此可以在一定程度上避免分类器的过度拟合，实践证明此方法可以提高分类器的性能。但是由于对每个少数类样本都生成新样本，因此容易发生生成样本重叠（overlapping）的问题</strong>。算法流程如下：</p><ol><li>对于少数类中的每一个样本 <script type="math/tex">x_i</script>，以欧氏距离为标准计算它到少数类样本集 <script type="math/tex">S_{min}</script> 中所有样本的距离，得到 K 近邻；</li><li>根据样本不平衡比例设置一个采样比例以确定采样倍率 <script type="math/tex">N</script>，对于每一个少数类样本 <script type="math/tex">x_i</script>，从其 K 近邻中随机选择若干个样本，假设选择的近邻为 <script type="math/tex">\tilde{x}</script>；</li><li>对于每一个随机选出的近邻 <script type="math/tex">\tilde{x}</script>，分别与原样本按照如下的公式构建新的样本:</li></ol><script type="math/tex; mode=display">x_{new}=x+rand\left(0,1\right)\times\left(\tilde{x}-x\right)</script><p><img src="https://farm5.staticflickr.com/4369/37075604245_e8b49bcb5e_o.png" alt></p><h3 id="Borderline-SMOTE"><a href="#Borderline-SMOTE" class="headerlink" title="Borderline-SMOTE"></a>Borderline-SMOTE</h3><p>原始的 SMOTE 算法对所有的少数类样本都是一视同仁的，<strong>但实际建模过程中发现那些处于边界位置的样本更容易被错分，因此利用边界位置的样本信息产生新样本可以给模型带来更大的提升</strong>。Borderline-SMOTE 便是将原始 SMOTE 算法和边界信息算法结合的算法。算法流程如下：</p><ol><li>首先，对于每个 <script type="math/tex">x_{i} \subset S_{min}</script> 确定一系列 KNN 样本集，称该数据集为 <script type="math/tex">S_{i-kNN}</script>，且 <script type="math/tex">S_{i-kNN} \subset S</script>；</li><li>然后，对每个样本 <script type="math/tex">x_{i}</script>，判断出最近邻样本集中属于多数类样本的个数，即：<script type="math/tex">|S_{i-kNN} \cap S_{maj}|</script>；</li><li>最后，选择满足下面不等式的 <script type="math/tex">\frac{k}{2} <|S_{i-kNN} \cap S_{maj}| <k</script> 的 <script type="math/tex">x_i</script>，将其加入危险集 <strong>DANGER</strong>，对危险集中的每一个样本点（最容易被错分的样本），采用普通的 SMOTE 算法生成新的少数类样本。</li></ol><p><strong>NOISE</strong>：如果一个 <script type="math/tex">x_i</script> 的 <script type="math/tex">|S_{i-kNN} \cap S_{maj}|</script> 等于 $k$（即设定的近邻个数），表示这个 <strong>minority</strong> 点的周围全都是 <script type="math/tex">S_{maj}</script> 的点，那么这个点就很有可能只是个干扰，或者是一个错误的 sample。</p><p><strong>DANGER</strong>：如果一个 <script type="math/tex">x_i</script> 的 <script type="math/tex">|S_{i-kNN} \cap S_{maj}|</script> 在 <script type="math/tex">\frac{k}{2}</script> 与 <script type="math/tex">k</script> 之间，表示这个 <strong>minority</strong> 点很有可能在 <strong>minority</strong> 与 <strong>majority</strong> 的边界边，或者已经到了 <script type="math/tex">S_{maj}</script> 的范围内，因此有点“危险”。</p><p><strong>SAFE</strong>：如果一个 <script type="math/tex">x_i</script> 的 <script type="math/tex">|S_{i-kNN} \cap S_{maj}|</script> 少于 <script type="math/tex">\frac{k}{2}</script>，表示这个 <strong>minority</strong> 点还在 <script type="math/tex">S_{min}</script> 的范围内，算是比较“安全”。</p><p>在划分了三种级别的 <strong>minority</strong> 点之后，就开始采用普通的 SMOTE 算法产生新的少数类样本，但是只要是 <strong>NOISE</strong> 级别的，就不予产生，反而是 <strong>Danger</strong> 的才产生新的样本， <strong>SAFE</strong> 的产生意义就不是特别大，大多数机器学习算法的假设大多数会涵盖到 <strong>SAFE</strong> 该级别的附近，多了反而会产生过拟合。</p><p><img src="https://farm5.staticflickr.com/4416/36680155240_e56c93ec1d_o.png" alt></p><h3 id="ADASYN"><a href="#ADASYN" class="headerlink" title="ADASYN"></a>ADASYN</h3><p>实际效果如下：</p><p><img src="https://farm5.staticflickr.com/4407/36721703920_7ee63f5633_o.png" alt></p><h2 id="Under-Sampling"><a href="#Under-Sampling" class="headerlink" title="Under-Sampling"></a>Under-Sampling</h2><p>欠采样方法是针对多数的负样本，减少负样本的数量，反而提高整体 $F$ 值。最简单的欠采样方法是随机地删掉一些负样本。欠采样的缺点很明显，就是会丢失负样本的一些重要信息，不能够充分利用已有的信息，这样训练得到的模型只学到了总体模式的一部分。</p><h3 id="Random-Under-Sampling"><a href="#Random-Under-Sampling" class="headerlink" title="Random Under-Sampling"></a>Random Under-Sampling</h3><p>减少多数类样本数量最简单的方法便是随机剔除多数类样本，可以事先设置多数类与少数类最终的数量比例，在保留少数类样本不变的情况下，根据比例随机选择多数类样本。</p><ol><li><p>首先我们从 <script type="math/tex">S_{maj}</script> 中随机选取一些多数类样本 <script type="math/tex">E</script></p></li><li><p>将这些样本从 <script type="math/tex">S_{maj}</script> 中移除，就有<script type="math/tex">|S_{maj-new}|=|S_{maj}|-|E|</script></p></li></ol><p><img src="https://farm5.staticflickr.com/4391/36948259812_969cfd4d20_o.png" alt></p><p>优点在于操作简单，只依赖于样本分布，不依赖任何距离信息，属于非启发式方法；缺点在于会丢失一部分多数类样本的信息，无法充分利用已有信息。</p><h3 id="Tomek-Links"><a href="#Tomek-Links" class="headerlink" title="Tomek Links"></a>Tomek Links</h3><h3 id="Informed-Under-Samling"><a href="#Informed-Under-Samling" class="headerlink" title="Informed Under-Samling"></a>Informed Under-Samling</h3><p>Informed 欠采样算法可以解决传统随机欠采样造成的数据信息丢失问题，且表现出较好的不均衡数据分类性能。其中含有一些集成（Ensemble）的思想，主要的方法是 <strong>EasyEnsemble</strong> 算法和 <strong>BalanceCascade</strong> 算法。</p><h4 id="Easy-Ensemble"><a href="#Easy-Ensemble" class="headerlink" title="Easy Ensemble"></a>Easy Ensemble</h4><p>它把数据划分为两部分，分别是多数类样本和少数类样本，对于多数类样本 <script type="math/tex">S_{maj}</script>，通过 $n$ 次 <strong> 有放回 </strong> 采样生成 $n$ 份子集，每份子集的大小为 <script type="math/tex">S_{min}</script>。然后，让少数类样本 <script type="math/tex">S_{min}</script> 分别和这 <script type="math/tex">n</script> 份样本合并训练 AdaBoost 分类器，这样可以得到 <script type="math/tex">n</script> 个模型，最终的模型采用加权多数表决的方法，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。这里假设多数类样本为 $N$，少数类样本为 $P$，算法伪代码如下：</p><p><img src="https://farm5.staticflickr.com/4349/36924720872_f6dc5d18bc_o.png" alt></p><p>EasyEnsemble 的想法是多次随机欠采样，尽可能全面地涵盖所有信息，算法特点是利用 <strong>boosting</strong> 减小偏差（Adaboost）、<strong>bagging</strong> 减小方差（Ensemble Classifier）。实际应用的时候也可以尝试选用不同的分类器来提高分类的效果。算法的流程如下：</p><p><img src="https://farm5.staticflickr.com/4409/36281899963_45beb23bde_o.png" alt></p><p>实际的效果如下：</p><p><img src="https://farm5.staticflickr.com/4362/36282852314_18efd6065f_o.png" alt></p><h4 id="Balance-Cascade"><a href="#Balance-Cascade" class="headerlink" title="Balance Cascade"></a>Balance Cascade</h4><p><strong>EasyEnsemble</strong> 算法训练的子过程是独立的，<strong>BalanceCascade</strong> 则是一种级联算法，这种级联的思想在图像识别中用途非常广泛。算法伪代码如下：</p><p><img src="https://farm5.staticflickr.com/4423/36924728072_b27ef319a3_o.png" alt></p><p><strong>BalanceCascade</strong> 和 <strong>EasyEnsemble</strong> 方法接近，但并不是对于多数类样本 <script type="math/tex">S_{maj}</script> 一次性直接生成多个子集，而是先生成一个子集 <script type="math/tex">E_{maj,1}</script>，让少数类样本 <script type="math/tex">S_{min}</script> 和这个子集 <script type="math/tex">E_{maj,1}</script> 样本合并训练出一个分类器，通过这个分类器判断整个多数类 <script type="math/tex">S_{maj}</script>，将判断错误的挑出来。用判断错误的 <script type="math/tex">S_{maj}</script> 中生成一个新的子集 <script type="math/tex">E_{maj,2}</script>，并且再让少数类样本和该子集 <script type="math/tex">E_{maj,2}</script> 样本合并训练出一个分类器，然后继续通过这个新的分类器继续判断整个多数类 <script type="math/tex">S_{maj}</script>，将判断错误的挑出来，如此不停地用这种流程进行下去，直到先用完 sample 或者判断准确，程序停止。</p><p>可以看出，<strong>BalanceCascade</strong> 算法每次循环的前置条件都是根据上一次判断错误的结果来生成 sample 。</p><p><strong>BalanceCascade</strong> 算法得到的是一个级联分类器，将若干个强分类器由简单到复杂排列，只有和少数类样本特征比较接近的才有可能输入到后面的分类器，比如边界点，因此能更充分地利用多数类样本的信息，一定程度上解决随机欠采样的信息丢失问题。</p><p>实际效果如下：</p><p><img src="https://farm5.staticflickr.com/4390/36976962741_8a22144772_o.png" alt></p><h4 id="NearMiss"><a href="#NearMiss" class="headerlink" title="NearMiss"></a>NearMiss</h4><p><strong>NearMiss</strong> 方法是利用距离远近剔除多数类样本的一类方法，实际操作就是依据 KNN 算法，有以下三种方法：</p><ul><li><strong>NearMiss 1</strong>：对于多数样本类 <script type="math/tex">S_{maj}</script> 中的某个点，计算该点和离自己最近的 <script type="math/tex">S_{min}</script> 少数类的 3 个点的总距离，按值从小到大排序，根据指定个数 $n$， 保留 $n$ 个样本。</li></ul><p><img src="https://farm5.staticflickr.com/4392/36976824531_e1ecb1b0d1_o.png" alt></p><ul><li><strong>NearMiss 2</strong>：对于多数样本类 <script type="math/tex">S_{maj}</script> 中的某个点，计算该点和离自己最远的 <script type="math/tex">S_{min}</script> 少数类的 3 个点的总距离，按值从小到大排序，根据指定个数 $n$， 保留 $n$ 个样本。</li></ul><p><img src="https://farm5.staticflickr.com/4442/36976823751_dd2854ca84_o.png" alt></p><ul><li><strong>NearMiss 3</strong>：首先事前给定一个值 $a$，对于少数样本类 <script type="math/tex">S_{min}</script> 中的每个点，保留和自己距离最近的 $a$ 个多数类点。</li></ul><p><img src="https://farm5.staticflickr.com/4361/36976822951_d483540f26_o.png" alt></p><p><strong>NearMiss-1</strong> 和 <strong>NearMiss-2</strong> 方法的描述仅有一字之差，但其含义是完全不同的：</p><p><strong>NearMiss-1</strong> 考虑的是与最近的 3 个少数类样本的平均距离，是局部的；<strong>NearMiss-2</strong> 考虑的是与最远的 3 个少数类样本的平均距离，是全局的。</p><p><strong>NearMiss-1</strong> 方法得到的多数类样本分布也是”不均衡“的，它倾向于在比较集中的少数类附近找到更多的多数类样本，而在孤立的（或者说是离群的）少数类附近找到更少的多数类样本，原因是 <strong>NearMiss-1</strong> 方法考虑的局部性质和平均距离。</p><p><strong>NearMiss-3</strong> 方法则会使得每一个少数类样本附近都有足够多的多数类样本，显然这会使得模型的精确度高、召回率低。</p><p><strong>NearMiss</strong> 算法实际效果如下：</p><p><img src="https://farm5.staticflickr.com/4336/37117835945_006294f37c_o.png" alt></p><h3 id="CNN-Condensed-Nearest-Neighbours"><a href="#CNN-Condensed-Nearest-Neighbours" class="headerlink" title="CNN(Condensed Nearest Neighbours)"></a>CNN(Condensed Nearest Neighbours)</h3><p>这里的 CNN 不是卷积神经网络的简称，而是 Condensed Nearest Neighbours 压缩最近邻算法，通过 CNN 算法来进行 Under-Sampling。</p><h2 id="Integration-of"><a href="#Integration-of" class="headerlink" title="Integration of"></a>Integration of</h2><p>将过采样和欠采样结合。</p><h3 id="SMOTE-Tomek-Links"><a href="#SMOTE-Tomek-Links" class="headerlink" title="SMOTE + Tomek Links"></a>SMOTE + Tomek Links</h3><p>实际效果如下：</p><p><img src="https://farm5.staticflickr.com/4363/36305865973_b4c970d546_o.png" alt></p><h3 id="SMOTE-ENN"><a href="#SMOTE-ENN" class="headerlink" title="SMOTE + ENN"></a>SMOTE + ENN</h3><p>实际效果如下：</p><p><img src="https://farm5.staticflickr.com/4411/36305863773_0780f863b0_o.png" alt></p><h2 id="Different-resampled-ratios"><a href="#Different-resampled-ratios" class="headerlink" title="Different resampled ratios"></a>Different resampled ratios</h2><p>尝试不同的采样比例，有些时候 1:1 的比例并不是最好的，因为与现实情况相差甚远。</p><h1 id="Algorithm-Level"><a href="#Algorithm-Level" class="headerlink" title="Algorithm Level"></a>Algorithm Level</h1><p>在算法层面上，改变分类算法，在传统分类算法的基础上对不同类别采取不同的加权方式，使得模型更看重少数类。</p><h2 id="Try-Different-Algorithms"><a href="#Try-Different-Algorithms" class="headerlink" title="Try Different Algorithms"></a>Try Different Algorithms</h2><p>强烈建议不要对待每一个分类都使用自己喜欢而熟悉的分类算法。应该使用不同的算法对其进行比较，因为不同的算法适用于不同的任务与数据。</p><p>例如，<strong>决策树</strong> 往往在类别不均衡数据上表现不错。它使用基于类变量的划分规则去创建分类树，因此可以强制地将不同类别的样本分开。目前流行的决策树算法有：<strong><code>C4.5</code></strong>、<strong><code>C5.0</code></strong>、<strong><code>CART</code></strong> 和 <strong><code>Random Forest</code></strong> 等。</p><h2 id="Try-Penalized-Models"><a href="#Try-Penalized-Models" class="headerlink" title="Try Penalized Models"></a>Try Penalized Models</h2><p>可以使用相同的分类算法，但是使用一个不同的角度，比如你的分类任务是识别那些小类，那么可以对分类器的小类样本数据增加权值，降低大类样本的权值，从而使得分类器将重点集中在小类样本身上。一个具体做法就是，在训练分类器时，若分类器将小类样本分错时额外增加分类器一个小类样本分错代价，这个额外的代价可以使得分类器更加“关心”小类样本。如 <code>penalized-SVM</code> 和 <code>penalized-LDA</code> 算法。另外，刚开始，可以设置每个类别的权值与样本个数比例的倒数，然后可以使用过采样进行调优。</p><p>如果你锁定一个具体的算法时，并且无法通过使用重采样来解决不均衡性问题，此时你便可以使用惩罚模型来解决不平衡性问题。但是，设置惩罚矩阵是一个复杂的事，因此你需要根据你的任务尝试不同的惩罚矩阵，并选取一个较好的惩罚矩阵。</p><h2 id="Change-the-Performance-Measures"><a href="#Change-the-Performance-Measures" class="headerlink" title="Change the Performance Measures"></a>Change the Performance Measures</h2><p>改变评判指标，也就是不用准确率来评判和选择模型，原因就是我们上面提到的 Accuracy Paradox 问题。从开头的分析可以看出，准确度这个评价指标在类别不均衡的分类任务中并不能很好的适用，甚至还会进行误导（尽管单从这个指标的数值上而言很高，但是模型的泛化能力很差）。因此在样本不均衡的分类任务中，需要使用更有说服力的评价指标来对分类器进行评价。</p><p>实际上有一些评判指标就是专门解决样本不平衡时的评判问题的，如：</p><ul><li>混淆矩阵（Confusion Matrix）</li><li>准确率（Precision）</li><li>召回率（Recall）</li><li>F1 值</li></ul><p>特别是：</p><ul><li><a href="http://alexkong.net/2013/06/introduction-to-auc-and-roc/" target="_blank" rel="noopener">ROC（AUC）</a></li><li><a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa" target="_blank" rel="noopener">Kappa</a></li></ul><p>如何针对不同的问题选择有效的评价指标，可以阅读这篇文章：<a href="http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/" target="_blank" rel="noopener">Classification Accuracy is Not Enough: More Performance Measures You Can Use</a>。</p><h2 id="Change-the-Model-Training-Process"><a href="#Change-the-Model-Training-Process" class="headerlink" title="Change the Model Training Process"></a>Change the Model Training Process</h2><p>更改模型的训练过程。假设超大类中样本的个数是极小类中样本个数的 $L$ 倍，那么在随机梯度下降（SGD，stochastic gradient descent）算法中，每次遇到一个极小类中样本进行训练时，训练 $L$ 次。</p><p>参考：</p><blockquote><ul><li><a href="http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/" target="_blank" rel="noopener">8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset</a></li><li><a href="http://www.ele.uri.edu/faculty/he/PDFfiles/ImbalancedLearning.pdf" target="_blank" rel="noopener"> Learning from Imbalanced Data</a></li><li><a href="http://www.jair.org/media/953/live-953-2037-jair.pdf" target="_blank" rel="noopener">SMOTE: Synthetic Minority Over-sampling Technique</a></li><li><a href="http://sci2s.ugr.es/keel/pdf/specific/congreso/han_borderline_smote.pdf" target="_blank" rel="noopener">Borderline-SMOTE: A New Over-Sampling Method inImbalanced Data Sets Learning</a></li><li><a href="https://pdfs.semanticscholar.org/4823/4756b7cf798bfeb47328f7c5d597fd4838c2.pdf" target="_blank" rel="noopener">ADASYN: Adaptive Synthetic Sampling Approach for ImbalancedLearning</a></li><li><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4309452" target="_blank" rel="noopener">Two Modifications of CNN</a></li><li><a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tsmcb09.pdf" target="_blank" rel="noopener">Exploratory Undersampling for Class-Imbalance Learning</a></li><li><a href="https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf" target="_blank" rel="noopener">KNN Approach to Unbalanced Data Distributions: A Case Study Involving Information Extraction</a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      本文主要介绍在机器学习中样本比例不平衡的处理方法。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>♟「Tools」 MongoDB and PyMongo</title>
    <link href="http://randolph.pro/2017/07/15/%E2%99%9F%E3%80%8CTools%E3%80%8D%20MongoDB%20and%20PyMongo/"/>
    <id>http://randolph.pro/2017/07/15/♟「Tools」 MongoDB and PyMongo/</id>
    <published>2017-07-14T16:00:00.000Z</published>
    <updated>2019-03-18T08:45:05.944Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm8.staticflickr.com/7874/32461566277_401e85a69d_o.jpg" alt></p><p>有关「Tools」的其他笔记系列：<a href="http://randolph.pro/categories/Tools/">「Tools」</a></p><h1 id="Chapter-1-Getting-Started"><a href="#Chapter-1-Getting-Started" class="headerlink" title="Chapter 1: Getting Started"></a>Chapter 1: Getting Started</h1><h2 id="Installing-MongoDB-on-Mac-OS"><a href="#Installing-MongoDB-on-Mac-OS" class="headerlink" title="Installing MongoDB on Mac OS"></a>Installing MongoDB on Mac OS</h2><p>Mac OS 安装 <code>MongoDB</code> 的方法有多种多样，这里使用 <code>Homebrew</code> 安装 <code>MongoDB</code>，在安装之前首先确保 <code>Homebrew</code> 更新至最新版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew update</span><br></pre></td></tr></table></figure><p>接着，使用 <code>Homebrew</code> 安装 <code>MongoDB</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install mongodb</span><br></pre></td></tr></table></figure><p>安装完成之后，在使用 <code>MongoDB</code> 之前，我们需要进行相应的环境配置。</p><h2 id="Setting-up-a-Environment-with-MongoDB"><a href="#Setting-up-a-Environment-with-MongoDB" class="headerlink" title="Setting up a Environment with MongoDB"></a>Setting up a Environment with MongoDB</h2><p>修改 <code>MongoDB</code> 配置文件，配置文件默认在 <code>/usr/local/etc</code> 下的 <code>mongod.conf</code>（可以参考我的配置文件）：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 日志</span></span><br><span class="line"><span class="attr">systemLog:</span></span><br><span class="line"><span class="attr">    destination:</span> <span class="string">file</span><span class="comment"># 日志为文件</span></span><br><span class="line"><span class="attr">    path:</span> <span class="string">/Users/XXX/MongoDB/log/mongo.log</span><span class="comment"># 文件位置</span></span><br><span class="line"><span class="attr">    logAppend:</span> <span class="literal">true</span><span class="comment"># 是否追加</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进程</span></span><br><span class="line"><span class="attr">processManagement:</span></span><br><span class="line"><span class="attr">    fork:</span> <span class="literal">true</span><span class="comment"># 守护进程方式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据读写</span></span><br><span class="line"><span class="attr">storage:</span></span><br><span class="line"><span class="attr">    dbPath:</span> <span class="string">/Users/XXX/MongoDB/db</span><span class="comment"># 数据读写目录</span></span><br><span class="line"></span><br><span class="line"><span class="attr">net:</span></span><br><span class="line"><span class="attr">    bindIp:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span><span class="comment"># 绑定 IP，默认 127.0.0.1，只能本机访问</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">27017</span><span class="comment"># 端口</span></span><br></pre></td></tr></table></figure><p><code>MongoDB</code> 默认的数据读写目录为 <code>/data/db</code>，默认的日志存储目录为 <code>/data/log</code>。</p><p>我们可以根据需要将其修改成自己需要的目录路径，其中 <code>XXX</code> 为你的电脑的用户名。</p><p>同样，我们也可以修改绑定的 IP 地址以及对应的端口号。</p><p>修改完 <code>mongod.conf</code> 配置文件后，根据自己配置内容，创建数据读写目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir -p /Users/XXX/MongoDB/db</span><br></pre></td></tr></table></figure><p>并要为其提供可读可写的权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ chown -R /Users/XXX/MongoDB/db</span><br></pre></td></tr></table></figure><p>最后一步，手动添加 <code>MongoDB</code> 安装目录到环境变量中。</p><p>如果使用的是 <code>bash</code>，请修改 <code>~/.bash_profile</code> 配置文件；<br>如果使用的是 <code>zsh</code>，请修改 <code>~/.zshrc</code> 配置文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/.bash_profile</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> mongod=<span class="string">'mongod --config /usr/local/etc/mongod.conf'</span></span><br></pre></td></tr></table></figure><p>这里将 <code>mongod</code> 命令指定为执行<code>mongod --config /usr/local/etc/mongod.conf</code>，即按照 <code>MongoDB</code> 的配置文件<code>mongod.conf</code>（即刚才修改的）的配置信息来启动 <code>MongoDB</code> 服务器端。</p><p>最后不要忘记让添加的环境变量生效：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">source</span> ~/.bash_profile</span><br></pre></td></tr></table></figure><h2 id="Running-with-MongoDB"><a href="#Running-with-MongoDB" class="headerlink" title="Running with MongoDB"></a>Running with MongoDB</h2><p>输入命令测试 <code>MongoDB</code> 的环境变量是否生效，尝试启动 <code>MongoDB</code> 服务器端：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mongod</span><br></pre></td></tr></table></figure><p>若未出现报错，则说明 <code>MongoDB</code> 服务器端启动成功，会出现如下的信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">about to fork child process, waiting until server is ready <span class="keyword">for</span> connections.</span><br><span class="line">forked process: 15751</span><br><span class="line">child process started successfully, parent exiting</span><br></pre></td></tr></table></figure><p>我们会得到相应的进程号。</p><p>接着，我们就可以进行 <code>MongoDB</code> 客户端数据库的连接：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mongo</span><br></pre></td></tr></table></figure><p>若未出现报错，则进入 <code>MonogDB</code> 的使用界面。</p><h2 id="Restart-the-MongoDB"><a href="#Restart-the-MongoDB" class="headerlink" title="Restart the MongoDB"></a>Restart the MongoDB</h2><p>如果有一天你发现你的数据库突然启动不了了，很可能是你没有正常关闭 <code>MongoDB</code> 导致的。</p><p>你可以先尝试删除掉 <code>mongod.lock</code> 文件，然后重新启动。</p><p>如果还是仍然启动不了，可以先通过命令查看所有进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ps -A</span><br></pre></td></tr></table></figure><p>找到 <code>MongoDB</code> 服务器端对应的进程号，进行关闭：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">kill</span> mongodb-id</span><br></pre></td></tr></table></figure><p>之后再重新启动 <code>MongoDB</code> 服务端：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mongod</span><br></pre></td></tr></table></figure><h2 id="MongoDB-GUI"><a href="#MongoDB-GUI" class="headerlink" title="MongoDB GUI"></a>MongoDB GUI</h2><p>存在诸多 MongoDB GUI，我推荐使用的是<a href="https://robomongo.org" target="_blank" rel="noopener">Robo 3T</a>。</p><hr><h1 id="Chapter-2-Reading-and-Writing-to-MongoDB-with-Python"><a href="#Chapter-2-Reading-and-Writing-to-MongoDB-with-Python" class="headerlink" title="Chapter 2: Reading and Writing to MongoDB with Python"></a>Chapter 2: Reading and Writing to MongoDB with Python</h1><h2 id="Create-and-Delete-database"><a href="#Create-and-Delete-database" class="headerlink" title="Create and Delete database"></a>Create and Delete database</h2><ul><li><code>MongoDB</code>创建数据库的语法格式如下：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; use DATABASE_NAME</span><br></pre></td></tr></table></figure><p>如果数据库不存在，则创建数据库，否则切换到指定数据库。</p><p>以下实例我们创建了数据库 <strong><code>randolph</code></strong>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; use randolph</span><br><span class="line">switched to db randolph</span><br><span class="line">&gt; db</span><br><span class="line">randolph</span><br></pre></td></tr></table></figure><p>如果你想查看所有数据库，可以使用 <code>show dbs</code> 命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; show dbs</span><br><span class="line"><span class="built_in">local</span>  0.078GB</span><br><span class="line"><span class="built_in">test</span>   0.078GB</span><br></pre></td></tr></table></figure><p>可以看到，我们刚创建的数据库 <strong><code>randolph</code></strong> 并不在数据库的列表中，要显示它，我们需要向 <strong><code>randolph</code></strong> 数据库插入一些数据：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; db.randolph.insert(&#123;<span class="string">"name"</span>:<span class="string">"MongoDB"</span>&#125;)</span><br><span class="line">WriteResult(&#123; <span class="string">"nInserted"</span> : 1 &#125;)</span><br><span class="line">&gt; show dbs</span><br><span class="line"><span class="built_in">local</span>   0.078GB</span><br><span class="line">randolph  0.078GB</span><br><span class="line"><span class="built_in">test</span>    0.078GB</span><br></pre></td></tr></table></figure><p><strong><code>MongoDB</code> 中默认的数据库为<code>test</code>，如果你没有创建新的数据库，集合将存放在 <code>test</code> 数据库中。</strong></p><ul><li><code>MongoDB</code> 删除数据库的语法格式如下：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; db.dropDatabase()</span><br></pre></td></tr></table></figure><p>删除当前数据库，默认为 <code>test</code>，你可以使用 <code>db</code> 命令查看当前数据库名。</p><p>以下实例我们删除了数据库 <strong><code>randolph</code></strong>。</p><p>首先，查看所有数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; show dbs</span><br><span class="line"><span class="built_in">local</span>   0.078GB</span><br><span class="line">randolph  0.078GB</span><br><span class="line"><span class="built_in">test</span>    0.078GB</span><br><span class="line">接下来我们切换到数据库 randolph：</span><br><span class="line">&gt; use randolph</span><br><span class="line">switched to db randolph</span><br></pre></td></tr></table></figure><p>执行删除命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; db.dropDatabase()</span><br><span class="line">&#123; <span class="string">"dropped"</span> : <span class="string">"randolph"</span>, <span class="string">"ok"</span> : 1 &#125;</span><br><span class="line">最后，我们再通过 show dbs 命令数据库是否删除成功：</span><br><span class="line">&gt; show dbs</span><br><span class="line"><span class="built_in">local</span>  0.078GB</span><br><span class="line"><span class="built_in">test</span>   0.078GB</span><br></pre></td></tr></table></figure><p>以上是数据库的删除操作，而删除集合的语法格式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; db.collection.drop()</span><br></pre></td></tr></table></figure><p>以下实例删除了 <strong><code>randolph</code></strong> 数据库中的集合<strong>site</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; use randolph</span><br><span class="line">switched to db randolph</span><br><span class="line">&gt; show tables</span><br><span class="line">site</span><br><span class="line">&gt; db.site.drop()</span><br><span class="line"><span class="literal">true</span></span><br><span class="line">&gt; show tables</span><br></pre></td></tr></table></figure><h2 id="Connecting-to-MongoDB-with-Python"><a href="#Connecting-to-MongoDB-with-Python" class="headerlink" title="Connecting to MongoDB with Python"></a>Connecting to MongoDB with Python</h2><p><code>PyMongo</code> 是 Python 中用来操作 <code>MongoDB</code> 的一个库。首先通过 <code>pip3</code> 下载 <code>PyMongo</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip3 install pymongo</span><br></pre></td></tr></table></figure><p>安装完 <code>PyMongo</code> 之后，我们直接来看一个使用例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> pymongo <span class="keyword">import</span> MongoClient</span><br><span class="line"><span class="keyword">from</span> pymongo.errors <span class="keyword">import</span> ConnectionFailure</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 建立连接到默认主机（localhost）和端口（27017）。还可以指定主机和 / 或使用端口：</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        client = MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line">        print(<span class="string">'Connected Successfully!'</span>)</span><br><span class="line">    <span class="keyword">except</span> ConnectionFailure <span class="keyword">as</span> e:</span><br><span class="line">        sys.stderr.write(<span class="string">'Could not connect to MongoDB: %s'</span> % e)</span><br><span class="line">        sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    db = client.local</span><br><span class="line">    collection = db[<span class="string">'your_collection'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h2 id="ObjectId"><a href="#ObjectId" class="headerlink" title="ObjectId"></a>ObjectId</h2><p>在 <code>MongoDB</code> 中存储的 <code>Document</code> 必须有一个 <strong><code>_id</code></strong> 键。这个键的值可以是任何类型的，默认是个 <code>ObjectId</code> 对象。</p><p>在一个 <code>Collection</code> 里面，每个 <code>Document</code> 都有唯一的 <strong><code>_id</code></strong> 值，来确保 <code>Collection</code> 里面每个 <code>Document</code> 都能被唯一标识。</p><p><code>MongoDB</code> 采用 <code>ObjectId</code>，而不是其他比较常规的做法（比如自动增加的主键）的主要原因，因为在多个服务器上同步自动增加主键值既费力还费时。</p><p><code>MongoDB</code> 的这种设计，就是体现 <strong> 空间换时间 </strong> 的思想。</p><p><code>ObjectId</code> 是一个 <strong>12</strong> 字节 <code>BSON</code> 类型数据，有以下格式：</p><ul><li>前 4 个字节表示时间戳</li><li>接下来的 3 个字节是机器标识码</li><li>紧接的 2 个字节由进程 id 组成（PID）</li><li>最后 3 个字节是随机数</li></ul><p>举个例子，<strong><code>4e7020cb7cac81af7136236b</code></strong> 这个 24 位的字符串，虽然看起来很长，也很难理解，但实际上它是由一组十六进制的字符构成，每个字节两位的十六进制数字，总共用了 12 字节的存储空间。</p><h3 id="Time"><a href="#Time" class="headerlink" title="Time"></a>Time</h3><p>对其前四个字节进行提取 <strong><code>4e7020cb</code></strong>，然后按照十六进制转为十进制，变为 <strong><code>1315971275</code></strong>，这个数字就是一个时间戳。通过时间戳的转换，就成了易看清的时间格式。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ date -r 1315971275</span><br><span class="line">Mer 14 Set 2011 11:34:35 CST</span><br></pre></td></tr></table></figure><h3 id="Machine"><a href="#Machine" class="headerlink" title="Machine"></a>Machine</h3><p>接下来的三个字节就是 <strong><code>7cac81</code></strong>，这三个字节是所在主机的唯一标识符，一般是机器主机名的散列值，这样就确保了不同主机生成不同的机器 hash 值。</p><p>这样确保在分布式中不造成冲突，这也就是在同一台机器生成的 <code>ObjectId</code> 中间的字符串都是一模一样的原因。</p><h3 id="PID"><a href="#PID" class="headerlink" title="PID"></a>PID</h3><p>上面的三个字节的 <code>Machine</code> 是为了确保在不同机器产生的 <code>ObjectId</code> 不冲突，而 <code>PID</code> 就是为了在同一台机器不同的 <code>MongoDB</code> 进程产生了 <code>ObjectId</code> 不冲突。</p><p>接下来的两个字节就是 <strong><code>af71</code></strong>，它是产生<code>ObjectId</code> 的进程标识符。</p><h3 id="INC"><a href="#INC" class="headerlink" title="INC"></a>INC</h3><p>自增计数器。前面的九个字节是保证了一秒内不同机器不同进程生成 <code>ObjectId</code> 不冲突，这后面的三个字节 <strong><code>36236b</code></strong> 是一个自动增加的计数器，用来确保在同一秒内产生的 <code>ObjectId</code> 也不会发现冲突，允许 $256^3=16777216$ 条记录的唯一性。</p><p>总的来看，<code>ObjectId</code> 的前 4 个字节时间戳，记录了文档创建的时间；接下来 3 个字节代表了所在主机的唯一标识符，确定了不同主机间产生不同的<code>ObjectId</code>；后 2 个字节的进程 id，决定了在同一台机器下，不同 <code>MongoDB</code> 进程产生不同的 <code>ObjectId</code>；最后通过 3 个字节的自增计数器，确保同一秒内产生 <code>ObjectId</code> 的唯一性。</p><p><code>ObjectId</code> 的这个主键生成策略，很好地解决了在分布式环境下高并发情况主键唯一性问题，值得学习借鉴。</p><h3 id="Source-Code-Analysis"><a href="#Source-Code-Analysis" class="headerlink" title="Source Code Analysis"></a>Source Code Analysis</h3><p><code>MongoDB</code> 可以通过自身的服务来产生 <code>ObjectId</code>，也可以通过客户端的驱动程序来生成 <code>ObjectId</code>。</p><p>虽然 <code>ObjectId</code> 是轻量级的，但如果全部在服务端生成肯定会花费一点开销。所以，能从服务器端转移到客户端驱动程序完成的，就尽量转移到客户端来完成，减少服务器端的开销。</p><p>我们来看一下，客户端的驱动程序是如何来生成 <code>ObjectId</code> 的。首先下载 <a href="https://github.com/mongodb/mongo-python-driver" target="_blank" rel="noopener">mongo-python-driver</a> 源码。</p><p>在源码的 <code>/bson</code> 文件夹中找到<code>objectid.py</code>，进行分析。默认构建的 <code>ObjectId</code> 代码如下代码所示:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ObjectId</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""A MongoDB ObjectId."""</span></span><br><span class="line">    </span><br><span class="line">    _inc = random.randint(<span class="number">0</span>, <span class="number">0xFFFFFF</span>)</span><br><span class="line">    _inc_lock = threading.Lock()</span><br><span class="line">    _machine_bytes = _machine_bytes()</span><br><span class="line">    __slots__ = (<span class="string">'__id'</span>)</span><br><span class="line">    _type_marker = <span class="number">7</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, oid=None)</span>:</span></span><br><span class="line">        <span class="string">"""Initialize a new ObjectId.</span></span><br><span class="line"><span class="string">        An ObjectId is a 12-byte unique identifier consisting of:</span></span><br><span class="line"><span class="string">            - a 4-byte value representing the seconds since the Unix epoch,</span></span><br><span class="line"><span class="string">            - a 3-byte machine identifier,</span></span><br><span class="line"><span class="string">            - a 2-byte process id, and</span></span><br><span class="line"><span class="string">            - a 3-byte counter, starting with a random value.</span></span><br><span class="line"><span class="string">            By default, ``ObjectId()`` creates a new unique identifier. The</span></span><br><span class="line"><span class="string">            optional parameter `oid` can be an :class:`ObjectId`, or any 12</span></span><br><span class="line"><span class="string">            :class:`bytes` or, in Python 2, any 12-character :class:`str`.</span></span><br><span class="line"><span class="string">            For example, the 12 bytes b'foo-bar-quux'do not follow the ObjectId</span></span><br><span class="line"><span class="string">            specification but they are acceptable input::</span></span><br><span class="line"><span class="string">                &gt;&gt;&gt; ObjectId(b'foo-bar-quux')</span></span><br><span class="line"><span class="string">                ObjectId('666f6f2d6261722d71757578')</span></span><br><span class="line"><span class="string">            `oid` can also be a :class:`unicode` or :class:`str` of 24 hex digits::</span></span><br><span class="line"><span class="string">                &gt;&gt;&gt; ObjectId('0123456789ab0123456789ab')</span></span><br><span class="line"><span class="string">                ObjectId('0123456789ab0123456789ab')</span></span><br><span class="line"><span class="string">                &gt;&gt;&gt;</span></span><br><span class="line"><span class="string">                &gt;&gt;&gt; # A u-prefixed unicode literal:</span></span><br><span class="line"><span class="string">                &gt;&gt;&gt; ObjectId(u'0123456789ab0123456789ab')</span></span><br><span class="line"><span class="string">                ObjectId('0123456789ab0123456789ab')</span></span><br><span class="line"><span class="string">            Raises :class:`~bson.errors.InvalidId` if `oid` is not 12 bytes nor</span></span><br><span class="line"><span class="string">            24 hex digits, or :class:`TypeError` if `oid` is not an accepted type.</span></span><br><span class="line"><span class="string">            :Parameters:</span></span><br><span class="line"><span class="string">                - `oid` (optional): a valid ObjectId.</span></span><br><span class="line"><span class="string">            .. mongodoc:: objectids"""</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> oid <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            self.__generate()</span><br><span class="line">        <span class="keyword">elif</span> isinstance(oid, bytes) <span class="keyword">and</span> len(oid) == <span class="number">12</span>:</span><br><span class="line">            self.__id = oid</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.__validate(oid)</span><br><span class="line">...</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__generate</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Generate a new value for this ObjectId."""</span></span><br><span class="line">        <span class="comment"># 4 bytes current time</span></span><br><span class="line">        oid = struct.pack(<span class="string">"&gt;i"</span>, int(time.time()))</span><br><span class="line">        <span class="comment"># 3 bytes machine</span></span><br><span class="line">        oid += ObjectId._machine_bytes</span><br><span class="line">        <span class="comment"># 2 bytes pid</span></span><br><span class="line">        oid += struct.pack(<span class="string">"&gt;H"</span>, os.getpid() % <span class="number">0xFFFF</span>)</span><br><span class="line">        <span class="comment"># 3 bytes inc</span></span><br><span class="line">        <span class="keyword">with</span> ObjectId._inc_lock:</span><br><span class="line">            oid += struct.pack(<span class="string">"&gt;i"</span>, ObjectId._inc)[<span class="number">1</span>:<span class="number">4</span>]</span><br><span class="line">            ObjectId._inc = (ObjectId._inc + <span class="number">1</span>) % <span class="number">0xFFFFFF</span></span><br><span class="line">        self.__id = oid</span><br></pre></td></tr></table></figure><ul><li><strong>time</strong></li></ul><p><strong><code>oid = struct.pack(&quot;&gt;i&quot;, int(time.time()))</code></strong></p><p>先通过 <code>time.time()</code> 计算出时间，然后 <code>int()</code> 强制类型转换成整数型，然后调用 <code>struct.pack()</code> 计算得出时间戳。</p><p><code>struct.pack(fmt, v1, v2, ...)</code></p><p>按照给定的格式 <code>fmt</code>，把数据封装成字符串(实际上是类似于 C 结构体的字节流)。</p><p>有的时候需要用 python 处理二进制数据，比如，存取文件，<code>socket</code> 操作时。这时候，可以使用 python 的 <code>struct</code> 模块来完成，可以用 <code>struct</code> 来处理 C 语言中的结构体。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys  </span><br><span class="line"><span class="keyword">import</span> struct  </span><br><span class="line">  </span><br><span class="line">a = <span class="number">20</span>  </span><br><span class="line">b = <span class="number">400</span>   </span><br><span class="line">str = struct.pack(<span class="string">"ii"</span>, a, b)  </span><br><span class="line"><span class="keyword">print</span> <span class="string">'length:'</span>, len(str)          <span class="comment"># length:  8  </span></span><br><span class="line"><span class="keyword">print</span> str                           <span class="comment"># 乱码：   </span></span><br><span class="line"><span class="keyword">print</span> repr(str)                     <span class="comment"># '\x14\x00\x00\x00\x90\x01\x00\x00'</span></span><br></pre></td></tr></table></figure><p><strong>格式符 <code>i</code> 表示转换为 int，<code>ii</code> 表示有两个 int 变量。</strong></p><p>进行转换后的结果长度为 8 个字节（int 类型占用 4 个字节，两个 int 为 8 个字节）。</p><p>可以看到输出的结果是乱码，因为结果是二进制数据，所以显示为乱码。</p><p>可以使用 python 的内置函数 <code>repr()</code> 来获取可识别的字符串，其中十六进制的 <strong><code>0x00000014</code></strong>, <strong><code>0x00001009</code></strong> 分别表示 <strong><code>20</code></strong> 和<strong><code>400</code></strong>。</p><ul><li><strong>machine</strong></li></ul><p><strong><code>oid += ObjectId._machine_bytes</code></strong></p><p>根据 <code>ObjectId</code> 类中的 <code>_machine_bytes</code> 属性。而 <code>_machine_bytes = _machine_bytes()</code>，下面看一下定义的 <code>_machine_bytes()</code> 函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_machine_bytes</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""Get the machine portion of an ObjectId."""</span></span><br><span class="line">    </span><br><span class="line">    machine_hash = hashlib.md5()</span><br><span class="line">    <span class="keyword">if</span> PY3:</span><br><span class="line">        <span class="comment"># gethostname() returns a unicode string in python 3.x</span></span><br><span class="line">        <span class="comment"># while update() requires a byte string.</span></span><br><span class="line">        machine_hash.update(socket.gethostname().encode())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Calling encode() here will fail with non-ascii hostnames</span></span><br><span class="line">        machine_hash.update(socket.gethostname())</span><br><span class="line">    <span class="keyword">return</span> machine_hash.digest()[<span class="number">0</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure><p>根据函数定义的那样，Python 3.x 获得 <code>_machine_bytes</code> 的方式是先通过 <code>socket.gethostname().encode()</code> 获得 encode 之后的 <code>string</code> 字节流。</p><p>之后再通过 <code>machine_hash.update()</code> 更新之后，最后再取前面三个字节<code>machine_hash.digest()[0:3]</code>。</p><ul><li><strong>pid</strong></li></ul><p><strong><code>oid += struct.pack(&quot;&gt;H&quot;, os.getpid() % 0xFFFF)</code></strong></p><p>根据 <code>os.getpid()</code> 获取进程号，之后再通过 <code>struct.pack()</code> 获得两个字节的进程标记符。</p><ul><li><strong>inc</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> ObjectId._inc_lock:</span><br><span class="line">    oid += struct.pack(<span class="string">"&gt;i"</span>, ObjectId._inc)[<span class="number">1</span>:<span class="number">4</span>]</span><br><span class="line">    ObjectId._inc = (ObjectId._inc + <span class="number">1</span>) % <span class="number">0xFFFFFF</span></span><br></pre></td></tr></table></figure><p><strong><code>oid += struct.pack(&quot;&gt;i&quot;, ObjectId._inc)[1:4]</code></strong> 先获得三个字节，之后 <code>ObjectId._inc</code> 自增，它能保证每次得到的值是一个递增并不重复的值。</p><h3 id="Get-Timestamp-on-MongoDB"><a href="#Get-Timestamp-on-MongoDB" class="headerlink" title="Get Timestamp on MongoDB"></a>Get Timestamp on MongoDB</h3><p>由于 <code>ObjectId</code> 中存储了四个字节的时间戳，所以我们不需要为我们的 <code>Document</code> 保存时间戳字段，我们可以通过 <code>getTimestamp()</code> 函数来获取文档的创建时间（将返回 ISO 格式的文档创建时间）:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; ObjectId(<span class="string">"5349b4ddd2781d08c09890f4"</span>).getTimestamp()</span><br><span class="line">ISODate(<span class="string">"2014-04-12T21:49:17Z"</span>)</span><br></pre></td></tr></table></figure><h3 id="ObjectId-to-String"><a href="#ObjectId-to-String" class="headerlink" title="ObjectId to String"></a>ObjectId to String</h3><p>在某些情况下，我们可能需要将 <code>ObjectId</code> 转换为字符串格式。我们可以使用下面的代码（将返回 <code>Guid</code> 格式的字符串）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; new ObjectId().str</span><br><span class="line">5349b4ddd2781d08c09890f3</span><br></pre></td></tr></table></figure><h2 id="Operators"><a href="#Operators" class="headerlink" title="Operators"></a>Operators</h2><p>所有存储在 <code>MongoDB</code> 集合中的数据都是 <code>BSON</code> 格式。</p><p><code>BSON</code> 是一种类 <code>JSON</code> 的一种二进制形式的存储格式，简称 <code>Binary JSON</code>。</p><h3 id="Insert"><a href="#Insert" class="headerlink" title="Insert()"></a>Insert()</h3><p><code>MongoDB</code> 的 <code>save()</code> 和<code>insert()</code>函数都可以向 <code>collection</code> 里插入数据，但两者是有两个区别：</p><ul><li><p><code>save()</code> 函数实际就是根据参数条件，调用了 <code>insert()</code> 或<code>update()</code>函数。</p><ul><li>如果想插入的数据对象存在，<code>insert()</code> 函数会报错，而 <code>save()</code> 函数则是相当于使用 <code>update()</code> 函数，改变原来的对象；</li><li>如果想插入的对象不存在, 那么它们执行相同的 <code>insert()</code> 函数插入操作。<br>这里可以用几个字来概括它们两的区别，即所谓：<strong>“有则改之, 无则加之”</strong>。</li></ul></li><li><p><code>insert()</code> 可以一次性插入一个列表，而不用遍历，效率高；<code>save()</code>则需要遍历列表，一个个插入。</p></li><li><p>另外，还有 <code>insertOne()</code> 和 <code>insertMany()</code> 插入方法。还是建议直接使用 <code>insert()</code> 函数。</p><ul><li><code>insertOne()</code>：向指定集合中插入一条文档数据；</li><li><code>insertMany()</code>：向指定集合中插入多条文档数据。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">data_record = &#123;<span class="string">'attribute1'</span>: value1, <span class="string">'attribute2'</span>: value2, <span class="string">'attribute3'</span>: value3&#125;</span><br><span class="line"><span class="comment">#  插入单条数据 </span></span><br><span class="line">db.collection.insert(data_record)</span><br><span class="line">db.collection.insert_one(data_record)</span><br><span class="line"></span><br><span class="line"><span class="comment">#  插入多条数据</span></span><br><span class="line">✗ db.collection.insert_many(data_record) // 会报错，insert_many() 时参数必须为 list 形式</span><br><span class="line"></span><br><span class="line">record_list = []</span><br><span class="line">record_list.append(data_record)</span><br><span class="line">db.collection.insert_many(record_list)</span><br></pre></td></tr></table></figure><h3 id="Remove"><a href="#Remove" class="headerlink" title="Remove()"></a>Remove()</h3><p><strong><code>remove(spec, multi=True)</code></strong></p><ol><li><strong><code>spec</code></strong>：查询文档，用于定位需要删除的目标文档。</li><li><strong><code>multi</code></strong>：是否更新多个文档。默认是删除多条符合条件的文档。</li></ol><p><strong>同样，和 <code>insert()</code> 插入操作一样，除了 <code>remove()</code> 删除操作外，还有 <code>delete_one()</code> 以及<code>delete_many()</code>（需要注意输入参数的形式，单条记录还是记录列表）。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">db.collection.remove() <span class="comment"># 表示删除集合里的所有记录</span></span><br><span class="line">db.collection.remove(&#123;<span class="string">'attribute'</span>: value&#125;) <span class="comment"># 表删除某属性 attribute=value 的所有记录</span></span><br><span class="line"></span><br><span class="line">id = db.collection.find_one(&#123;<span class="string">'attribute'</span>: value&#125;)[<span class="string">'_id'</span>]</span><br><span class="line">db.collection.remove(id) <span class="comment"># 查找到某属性 attribute=value 的记录，并根据记录的 id 删除该记录</span></span><br><span class="line">db.collection.drop() <span class="comment"># 表示删除整个集合</span></span><br></pre></td></tr></table></figure><p>删除文档通常很快，但是如果要清空整个集合，那么使用 <code>drop()</code> 直接删除集合会更快（然后在这个空集合上重建各项索引）。 </p><p>如果 <code>Collection</code> 有一些 <code>metadata</code>，例如 <code>index</code>，那么<code>db.collection.remove()</code> 将删除所有的<code>Document</code>，但并不会删除 <code>index</code> 信息，而 <code>drop()</code> 则会删除掉这些 <code>metadata</code>。</p><h3 id="Update"><a href="#Update" class="headerlink" title="Update()"></a>Update()</h3><p><strong><code>update(spec, doucument, upsert=False, multi=False)</code></strong></p><ol><li><strong><code>spec</code></strong>：查询文档，用于定位需要更新的目标文档。</li><li><strong><code>document</code></strong>：修改器文档，用于说明要对找到的文档进行哪些修改。</li><li><strong><code>upsert</code></strong>：如目标记录不存在，是否插入新文档。</li><li><strong><code>multi</code></strong>：<strong>是否更新多个文档。默认是只会修改第一条发现的文档，如果你要修改多条符合条件的文档，则需要设置参数<code>multi=true</code>。</strong></li></ol><p><strong>同样，和 <code>insert()</code> 插入操作一样，除了 <code>update()</code> 更新操作外，还有 <code>replace_one()</code>、<code>update_one()</code> 以及<code>update_many()</code>（需要注意输入参数的形式，单条记录还是记录列表）。</strong></p><h4 id="Fields"><a href="#Fields" class="headerlink" title="Fields"></a>Fields</h4><div class="table-container"><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td>$inc</td><td>Increments the value of the field by the specified amount.</td></tr><tr><td>$mul</td><td>Multiplies the value of the field by the specified amount.</td></tr><tr><td>$min</td><td>Only updates the field if the specified value is less than the existing field value.</td></tr><tr><td>$max</td><td>Only updates the field if the specified value is greater than the existing field value.</td></tr><tr><td>$set</td><td>Sets the value of a field in a document.</td></tr><tr><td>$unset</td><td>Removes the specified field from a document.</td></tr><tr><td>$rename</td><td>Renames a field.</td></tr></tbody></table></div><ul><li><strong><code>$inc</code></strong></li></ul><p><strong><code>$inc</code></strong>表示对原来的记录中的指定属性进行自增或自减：</p><p>例如，存在这样的条数据 <code>{&#39;_id&#39;: 1, &#39;attribute1&#39;: 8, &#39;attribute2&#39;: 6}</code>，对其进行<strong><code>$inc</code></strong> 更新操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">1</span>&#125;, &#123;<span class="string">'$inc'</span>: &#123;<span class="string">'attribute1'</span>: <span class="number">-2</span>, <span class="string">'attribute2'</span>: <span class="number">3</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><p>更新后的数据为<code>{&#39;_id&#39;: 1, &#39;attribute1&#39;: 6, &#39;attribute2&#39;: 9}</code>。</p><ul><li><strong><code>$mul</code></strong></li></ul><p><strong><code>$mul</code></strong>表示对原来的记录中的指定属性的值乘以给定的数值，分为以下几种情况：</p><p>例如，存在这样的三条数据：</p><ol><li><code>{&#39;_id&#39;: 1, &#39;item&#39;: &#39;ABC&#39;, &#39;price&#39;: 10.99}</code></li><li><code>{&#39;_id&#39;: 2, &#39;item&#39;: &#39;Unknown&#39;}</code></li><li><code>{&#39;_id&#39;: 3, &#39;item&#39;: &#39;XYZ&#39;, &#39;price&#39;: NumberLong(10)}</code></li></ol><p>◎ <strong>Multiply the Value of a Field</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">1</span>&#125;, &#123;<span class="string">'$mul'</span>: &#123;price: <span class="number">1.25</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><p>更新后的数据 1 为<code>{&#39;_id&#39;: 1, &#39;item&#39;: &#39;ABC&#39;, &#39;price&#39;: 13.7375}</code>。</p><p>◎ <strong>Apply <code>$mul</code> Operator to a Non-existing Field</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">2</span>&#125;, &#123;<span class="string">'$mul'</span>: &#123;<span class="string">'price'</span>: NumberLong(<span class="number">100</span>)&#125;&#125;)</span><br></pre></td></tr></table></figure><p>更新后的数据 2 为<code>{&#39;_id&#39;: 2, &#39;item&#39;: &#39;Unknown&#39;, &#39;price&#39;: NumberLong(0)}</code>。</p><p>◎ <strong>Multiply Mixed Numeric Types</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">3</span>&#125;, &#123;<span class="string">'$mul'</span>: &#123;<span class="string">'price'</span>: NumberInt(<span class="number">5</span>)&#125;&#125;)</span><br></pre></td></tr></table></figure><p>更新后的数据 3 为<code>{&#39;_id&#39;: 3, &#39;item&#39; : &#39;XYZ&#39;, &#39;price&#39;: NumberLong(50)}</code>。</p><ul><li><strong><code>$min</code></strong> &amp; <strong><code>$max</code></strong></li></ul><p><strong><code>$min</code></strong>表示将原来记录中指定属性的值与给定的数值进行比较，属性的新值选择两者之间更小的值，之后更新新记录至数据库。<br><strong><code>$max</code></strong>表示将原来记录中指定属性的值与给定的数值进行比较，属性的新值选择两者之间更大的值，之后更新新记录至数据库。</p><p>例如，存在这样的条数据 <code>{&#39;_id&#39;: 1, &#39;highScore&#39;: 800, &#39;lowScore&#39;: 200}</code>，对其进行<strong><code>$min</code></strong> 以及 <strong><code>$max</code></strong> 更新操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">1</span>&#125;, &#123;<span class="string">'$min'</span>: &#123;<span class="string">'lowScore'</span>: <span class="number">50</span>&#125;, <span class="string">'$max'</span>: &#123;<span class="string">'highSoce'</span>: <span class="number">750</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><p>更新之后的数据为<code>{&#39;_id&#39;: 1, &#39;highScore&#39;: 800, &#39;lowScore&#39;: 50}</code>。</p><ul><li><strong><code>$set</code></strong> &amp; <strong><code>$unSet</code></strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'attribute1'</span>: value1, <span class="string">'attribute2'</span>: value2&#125;, \</span><br><span class="line">                    &#123;<span class="string">'$set'</span>: &#123;<span class="string">'attribute1'</span>: new_value1&#125;, <span class="string">'$set'</span>: &#123;<span class="string">'attribute2'</span>: new_value2&#125;&#125;, multi=<span class="keyword">True</span>)</span><br><span class="line">db.collection.update(&#123;<span class="string">'attribute1'</span>: value1&#125;, &#123;<span class="string">'$unset'</span>: &#123;<span class="string">'attribute2'</span>: value2&#125;&#125;, multi=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>其中，<strong><code>$set</code></strong>表示对原来的记录进行修改，<strong><code>$unset</code></strong>表示移除指定属性。</p><p><strong>注意，使用 <code>&#39;$set&#39;: {&#39;attribute1&#39;: new_value1}</code> 只会对 <code>attribute1</code> 的部分进行修改，如果使用 <code>db.collection.update({&#39;attribute1&#39;: value1}, {&#39;attribute1&#39;: new_value1})</code>，即不使用 <code>$set</code>，则会将整个 <code>Document</code> 记录替换成 <code>{&#39;attribute1&#39;: new_value1}</code>。</strong></p><ul><li><strong><code>$rename</code></strong></li></ul><p><strong><code>$rename</code></strong>表示对原来的记录中的指定属性的 <strong>key</strong> 值进行修改：</p><p>例如，存在这样的数据 <code>{&#39;_id&#39;: 1, &#39;nmae&#39;: {&#39;first&#39;: &#39;george&#39;, &#39;last&#39;: &#39;washington&#39;}}</code>，对其进行<strong><code>$rename</code></strong> 更新操作，分为以下几种情况：<br>​<br>◎ <strong>Rename a Field</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">1</span>&#125;, &#123;<span class="string">'$rename'</span>: &#123;<span class="string">'nmae'</span>: <span class="string">'name'</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><p>更新后的数据为<code>{&#39;_id&#39;: 1, &#39;name&#39;: {&#39;first&#39;: &#39;george&#39;, &#39;last&#39;: &#39;washington&#39;}}</code>。</p><p>◎ <strong>Rename a Field in an Embedded Document</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">1</span>&#125;, &#123;<span class="string">'$rename'</span>: &#123;<span class="string">'name.first'</span>: <span class="string">'name.fname'</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><p>更新后的数据为<code>{&#39;_id&#39;: 1, &#39;name&#39;: {&#39;fname&#39;: &#39;george&#39;, &#39;last&#39;: &#39;washington&#39;}}</code>。<br>​<br>◎ <strong>Rename a Field That Does Not Exist</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">1</span>&#125;, &#123;<span class="string">'$rename'</span>: &#123;<span class="string">'job'</span>: <span class="string">'student'</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><p>因为不存在该字段，所以并不会进行修改，更新后的数据仍为<code>{&#39;_id&#39;: 1, &#39;name&#39;: {&#39;fname&#39;: &#39;george&#39;, &#39;last&#39;: &#39;washington&#39;}}</code>。</p><h4 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h4><div class="table-container"><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td>$addToSet</td><td>Adds elements to an array only if they do not already exist in the set.</td></tr><tr><td>$push</td><td>Adds an item to an array.</td></tr><tr><td>$pushAll</td><td>Deprecated. Adds several items to an array.</td></tr><tr><td>$pop</td><td>Removes the first or last item of an array.</td></tr><tr><td>$pull</td><td>Removes all array elements that match a specified query.</td></tr><tr><td>$pullAll</td><td>Removes all matching values from an array.</td></tr><tr><td>$each</td><td>Modifies the $push and $addToSet operators to append multiple items for array updates.</td></tr><tr><td>$position</td><td>Modifies the $push operator to specify the position in the array to add elements.</td></tr><tr><td>$slice</td><td>Modifies the $push operator to limit the size of updated arrays.</td></tr><tr><td>$sort</td><td>Modifies the $push operator to reorder documents stored in an array.</td></tr></tbody></table></div><ul><li><strong><code>$addToSet</code></strong></li></ul><p><strong><code>$addToSet</code></strong>表示添加给定的字段到原来的记录中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'attribute1'</span>: value1, <span class="string">'attribute2'</span>: value2&#125;, \</span><br><span class="line">                    &#123;<span class="string">'$addToSet'</span>: &#123;<span class="string">'attribute3'</span>: value3&#125;&#125;, multi=<span class="keyword">True</span>)</span><br><span class="line">db.collection.update</span><br></pre></td></tr></table></figure><p><strong><code>$addToSet</code></strong>和 <strong><code>$push</code></strong> 类似，不过仅在该元素不存在时才添加，相当于将 <code>array</code> 当成 <code>set</code> 来执行 <code>set.add(item)</code> 操作。</p><ul><li><strong><code>$push</code></strong> &amp; <strong><code>pushAll</code></strong><ul><li><strong><code>push</code></strong>：在 <code>Document</code> 记录中末尾添加一项，相当于<code>list.append(item)</code>。</li><li><strong><code>pushAll</code></strong>：在 <code>Document</code> 记录中末尾添加多项，相当于<code>list=list + another_list</code>。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'attribute1'</span>: value1&#125;, &#123;<span class="string">'$push'</span>: &#123;<span class="string">'attribute2'</span>: value2&#125;&#125;, multi=<span class="keyword">True</span>)</span><br><span class="line">db.collection.update(&#123;<span class="string">'attribute1'</span>: value1&#125;, &#123;<span class="string">'$pushAll'</span>: &#123;<span class="string">'attribute2'</span>: value2&#125;&#125;, multi=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><ul><li><strong><code>$pop</code></strong></li></ul><p><strong><code>$pop</code></strong>表示按照 <code>index</code> 位置下标移除元素。</p><p>例如，存在这样的数据 <code>{&#39;_id&#39;: 1, &#39;attribute1&#39;: [1, 2, 3, 4, 5, 6, 7, 2, 3]}</code>，对其进行<strong><code>$pop</code></strong> 更新操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">1</span>&#125;, &#123;<span class="string">'$pop'</span>: &#123;<span class="string">'attribute1'</span>: <span class="number">1</span>&#125;&#125;) <span class="comment"># 移除最后一个元素</span></span><br><span class="line"><span class="comment"># 此刻字段显示：&#123;'_id': 1, 'attribute1': [1, 2, 3, 4, 5, 6, 7, 2]&#125;</span></span><br><span class="line"></span><br><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">1</span>&#125;, &#123;<span class="string">'$pop'</span>:&#123;<span class="string">'attribute1'</span>: <span class="number">-1</span>&#125;&#125;) <span class="comment"># 移除第一个元素</span></span><br><span class="line"><span class="comment"># 此刻字段显示：&#123;'_id': 1, 'attribute1': [2, 3, 4, 5, 6, 7, 2]&#125;</span></span><br></pre></td></tr></table></figure><ul><li><strong><code>$pull</code></strong> &amp; <strong><code>$pullAll</code></strong></li></ul><p><strong><code>$pull</code></strong>表示按值移除元素，<strong><code>$pullAll</code></strong>表示移除所有符合条件的元素。</p><p>例如，存在这样的数据 <code>{&#39;_id&#39;: 1, &#39;attribute1&#39;: [2, 3, 4, 5, 6, 7, 2]}</code>，对其进行<strong><code>$pull</code></strong> 以及 <strong><code>$pullAll</code></strong> 更新操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">db.collection..update(&#123;<span class="string">'_id'</span>: <span class="number">1</span>&#125;, &#123;<span class="string">'$pull'</span>:&#123;<span class="string">'attribute1'</span>: <span class="number">2</span>&#125;&#125;) <span class="comment"># 移除全部 2</span></span><br><span class="line"><span class="comment"># 此刻字段显示：&#123;'_id': 1, 'attribute1': [3, 4, 5, 6, 7]&#125;</span></span><br><span class="line"></span><br><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">1</span>&#125;, &#123;<span class="string">'$pullAll'</span>:&#123;<span class="string">'attribute1'</span>: [<span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>]&#125;&#125;) <span class="comment"># 移除 3, 5, 6</span></span><br><span class="line"><span class="comment"># 此刻字段显示：&#123;'_id': 1, 'attribute1': [4, 7]&#125;</span></span><br></pre></td></tr></table></figure><ul><li><strong><code>$each</code></strong></li></ul><p><strong><code>$each</code></strong>表示添加多个元素，相当于加强版的 <strong><code>$addToSet</code></strong> 或者 <strong><code>$push</code></strong> 操作（添加多个记录）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use $each with $addToSet Operator</span></span><br><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">1</span>&#125;, &#123;<span class="string">'$addToSet'</span>: &#123;<span class="string">'attribute1'</span>: &#123;<span class="string">'$each'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]&#125;&#125;&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use each with $push Operator</span></span><br><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">1</span>&#125;, &#123;<span class="string">'$push'</span>: &#123;<span class="string">'attribute1'</span>: &#123;<span class="string">'$each'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]&#125;&#125;&#125;)</span><br></pre></td></tr></table></figure><ul><li><strong><code>$position</code></strong></li></ul><p><strong><code>$position</code></strong>可以看作是指定插入位置的 <strong><code>$push</code></strong> 操作。</p><p>例如，存在这样的条数据 <code>{&#39;_id&#39;: 1, &#39;scores&#39;: [100]}</code>，对其进行<strong><code>$position</code></strong> 更新操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">1</span>&#125;, &#123;<span class="string">'$push'</span>: &#123;<span class="string">'attribute1'</span>: &#123;<span class="string">'$each'</span>: [<span class="number">50</span>, <span class="number">60</span>, <span class="number">70</span>]&#125;, <span class="string">'$position'</span>: <span class="number">0</span>&#125;&#125;)</span><br><span class="line"><span class="comment"># 此刻字段显示：&#123;'_id': 1, 'attribute1': [50, 60, 70, 100]&#125;</span></span><br><span class="line"></span><br><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">1</span>&#125;, &#123;<span class="string">'$push'</span>: &#123;<span class="string">'attribute1'</span>: &#123;<span class="string">'$each'</span>: [<span class="number">20</span>, <span class="number">30</span>]&#125;, <span class="string">'$position'</span>: <span class="number">2</span>&#125;&#125;)</span><br><span class="line"><span class="comment"># 此刻字段显示：&#123;'_id': 1, 'attribute1': [50, 60, 20, 30, 70, 100]&#125;</span></span><br></pre></td></tr></table></figure><ul><li><strong><code>$slice</code></strong></li></ul><p><strong><code>$slice</code></strong>可以看作是进行 <strong><code>$push</code></strong> 操作之后再进行一次切片操作。</p><p>例如，存在这样的几条数据：</p><ol><li><code>{&#39;_id&#39;: 2, &#39;scores&#39;: [89, 90]}</code></li><li><code>{&#39;_id&#39;: 1, &#39;scores&#39;: [40, 50, 60]}</code></li><li><code>{&#39;_id&#39;: 3, &#39;scores&#39;: [89, 70, 100, 20]}</code></li></ol><p>◎ Slice from the Front of the Array</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">1</span>&#125;, &#123;<span class="string">'$push'</span>: &#123;<span class="string">'scores'</span>: &#123;<span class="string">'$each'</span>: [<span class="number">100</span>, <span class="number">20</span>]&#125;, <span class="string">'$slice'</span>: <span class="number">3</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><p>更新后的数据 1 为<code>{&#39;_id&#39;: 1, &#39;scores&#39;: [89, 90, 100]}</code>。</p><p>◎ Slice from the End of the Array</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">2</span>&#125;, &#123;<span class="string">'$push'</span>: &#123;<span class="string">'scores'</span>: &#123;<span class="string">'$each'</span>: [<span class="number">80</span>, <span class="number">78</span>, <span class="number">86</span>]&#125;, <span class="string">'$slice'</span>: <span class="number">-5</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><p>更新后的数据 2 为<code>{&#39;_id&#39;: 2, &#39;scores&#39;: [50, 60, 80, 78, 86]}</code>。</p><p>◎ Update Array Using Slice Only</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">3</span>&#125;, &#123;<span class="string">'$push'</span>: &#123;<span class="string">'scores'</span>: &#123;<span class="string">'$each'</span>: []&#125;, <span class="string">'$slice'</span>: <span class="number">-3</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><p>更新后的数据 3 为<code>{&#39;_id&#39;: 3, &#39;scores&#39;: [70, 100, 20]}</code>。</p><ul><li><strong><code>$sort</code></strong></li></ul><p><strong><code>$sort</code></strong>可以看作是 <strong><code>$push</code></strong> 操作之后再进行一次排序操作。</p><p>例如，存在这样的几条数据：</p><ol><li><code>{&#39;_id&#39;: 1, &#39;quizzes&#39;: [{&#39;id&#39;: 1, &#39;score&#39;: 6}, {&#39;id&#39;: 2, &#39;score&#39;: 9}]}</code></li><li><code>{&#39;_id&#39;: 2, &#39;tests&#39;: [89, 70, 89, 50] }</code></li></ol><p>◎ Sort Array of Documents by a Field in the Documents</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">1</span>&#125;, &#123;<span class="string">'$push'</span>: &#123;<span class="string">'quizzes'</span>: &#123;<span class="string">'$each'</span>: [&#123;id<span class="string">': 3,'</span>score<span class="string">': 8&#125;,\</span></span><br><span class="line"><span class="string">                    &#123;'</span>id<span class="string">': 4,'</span>score<span class="string">': 7&#125;, &#123;'</span>id<span class="string">': 5,'</span>score<span class="string">': 6&#125;],'</span>$sort<span class="string">': &#123;'</span>score<span class="string">': 1&#125;&#125;&#125;)</span></span><br></pre></td></tr></table></figure><p>更新后的数据 1 为<code>{&#39;_id&#39;: 1, &#39;quizzes&#39;: [{&#39;id&#39;: 1, &#39;score&#39;: 6}, {&#39;id&#39;: 5, &#39;score&#39;: 6}, {&#39;id&#39;: 4, &#39;score&#39;: 7}, {&#39;id&#39;: 3, &#39;score&#39;: 8}, {&#39;id&#39;: 2, &#39;score&#39;: 9}]}</code>。</p><p>◎ Sort Array Elements That Are Not Documents</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">2</span>&#125;, &#123;<span class="string">'$push'</span>: &#123;<span class="string">'tests'</span>: &#123;<span class="string">'$each'</span>: [<span class="number">40</span>, <span class="number">60</span>], <span class="string">'$sort'</span>: <span class="number">1</span>&#125;&#125;&#125;)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>Use <strong><code>$push</code></strong> Operator with Multiple Modifiers</li></ul><p>例如，存在这样一条数据 <code>{&#39;_id&#39;: 1, &#39;quizzes&#39;: [{&#39;wk&#39;: 1, &#39;score&#39;: 10}, {&#39;wk&#39;: 2, &#39;score&#39;: 8}, {&#39;wk&#39;: 3, &#39;score&#39;: 5}, {&#39;wk&#39;: 4, &#39;score&#39;: 6}]}</code>，对其进行<strong><code>$push</code></strong> 以及各种 Modifiers （<strong><code>$each</code></strong>、<strong><code>$slice</code></strong>、<strong><code>$sort</code></strong>以及<strong><code>$position</code></strong>）更新操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">db.collection.update(&#123;<span class="string">'_id'</span>: <span class="number">4</span>&#125;, &#123;<span class="string">'$push'</span>: &#123;<span class="string">'quizzes'</span>: &#123;<span class="string">'$each'</span>: [&#123;<span class="string">'wk'</span>: <span class="number">5</span>, <span class="string">'score'</span>: <span class="number">8</span>&#125;,\</span><br><span class="line">                    &#123;<span class="string">'wk'</span>: <span class="number">6</span>, <span class="string">'score'</span>: <span class="number">7</span>&#125;, &#123;<span class="string">'wk'</span>: <span class="number">7</span>, <span class="string">'score'</span>: <span class="number">6</span>&#125;],\</span><br><span class="line">                    <span class="string">'$sort'</span>: &#123;<span class="string">'score'</span>: <span class="number">-1</span>&#125;, <span class="string">'$slice'</span>: <span class="number">3</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><p>更新后的数据为<code>{&#39;_id&#39;: 1, &#39;quizzes&#39;:[{&#39;wk&#39;: 1, &#39;score&#39;: 10}, {&#39;wk&#39;: 2, &#39;score&#39;: 8}, {&#39;wk&#39;: 5, &#39;score&#39;: 8}}</code>。</p><p>最终目的就是取得分数前三的数据信息，关于 <strong><code>$each</code></strong>、<strong><code>$sort</code></strong> 以及 <strong><code>$slice</code></strong> 执行的前后顺序与代码中的顺序无关，其实依照下表优先级：</p><ol><li>Update array to add elements in the correct position.</li><li>Apply sort, if specified.</li><li>Slice the array, if specified.</li><li>Store the array.</li></ol><h3 id="Find"><a href="#Find" class="headerlink" title="Find()"></a>Find()</h3><p><strong><code>find(filter, projection,skip=0, limit=0, sort=None)</code></strong></p><ol><li><strong><code>filter</code></strong>：过滤器，用于定位需要更新的目标文档。</li><li><strong><code>projection</code></strong>： 指定返回记录的属性信息。</li><li><strong><code>skip</code></strong>： 跳过指定数量的记录。</li><li><strong><code>limit</code></strong>：指定读取的记录数量。</li><li><strong><code>sort</code></strong>：排序参数。</li></ol><p>同样地，和 <code>insert()</code> 插入操作一样，除了 <code>find()</code> 查询操作之外，还有的 <code>find_one()</code>、<code>find_and_modify()</code> 等等。</p><ul><li><strong><code>projection</code></strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果不指定，是默认显示所有字段（包括 _id ）</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.colleciton.find(&#123;&#125;, &#123;<span class="string">'_id'</span>: <span class="number">0</span>, <span class="string">'attribute1'</span>: <span class="number">1</span>, <span class="string">'attribute2'</span>: <span class="number">1</span>&#125;): <span class="keyword">print</span> item</span><br></pre></td></tr></table></figure><p>这里的 <code>projection</code> 就是 <code>{&#39;_id&#39;: 0, &#39;attribute1&#39;: 1, &#39;attribute2&#39;: 1}</code>，表示只显示集合中的所有记录的<code>attribute1</code>、<code>attribute2</code> 属性值，<code>_id: 0</code>表示一般忽略不显示 <code>_id</code> 的值，<code>&#39;attribute&#39;: 1</code>表示显示该字段。</p><h4 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h4><p>数据库的查询基本是通过 <code>find()</code> 函数进行查询，其中大于、大于等于、小于、小于等于这些关系运算符经常要用到，分别用 <code>$gt</code>、<code>$gte</code>、<code>$lt</code>、<code>$lte</code> 表示。</p><div class="table-container"><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td>$ne</td><td>Matches all values that are not equal to a specified value.</td></tr><tr><td>$gt</td><td>Matches values that are greater than a specified value.</td></tr><tr><td>$gte</td><td>Matches values that are greater than or equal to a specified value.</td></tr><tr><td>$lt</td><td>Matches values that are less than a specified value.</td></tr><tr><td>$lte</td><td>Matches values that are less than or equal to a specified value.</td></tr><tr><td>$in</td><td>Matches any of the values specified in an array.</td></tr><tr><td>$nin</td><td>Matches none of the values specified in an array.</td></tr></tbody></table></div><ul><li><strong><code>$ne</code></strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">db.collection.find(&#123;<span class="string">'attribute'</span>: value&#125;) <span class="comment"># 查找符合某属性 attribute 等于 value 的所有记录，查不到时返回 None</span></span><br><span class="line">db.collection.find(&#123;<span class="string">'attribute'</span>: &#123;<span class="string">"$ne"</span>: <span class="number">73</span>&#125;&#125;) <span class="comment"># 查找符合某属性 attribute 的值不等于 73 的所有记录，查不到时返回 None</span></span><br><span class="line"><span class="comment"># 显示集合中所有 attribute 等于 21 的记录的 attribute1、attribute2 属性值</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(&#123;<span class="string">'attribute2'</span>: <span class="number">21</span>&#125;, &#123;<span class="string">'_id'</span>: <span class="number">0</span>, <span class="string">'attribute1'</span>: <span class="number">1</span>, <span class="string">'attribute2'</span>: <span class="number">1</span>&#125;): <span class="keyword">print</span> item</span><br></pre></td></tr></table></figure><ul><li><strong><code>$gt</code> &amp; <code>$gte</code> &amp; <code>$lt</code> &amp; <code>$lte</code></strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">db.collection.find(&#123;<span class="string">'attribute'</span>: &#123;<span class="string">"$lt"</span>: <span class="number">15</span>&#125;&#125;) <span class="comment"># 查找符合某属性 attribute 的值小于 15 的所有记录，查不到时返回 None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找符合属性 attribute1 大于等于 12 小于等于 15，并且 attribute2 等于 value 的所有记录，查不到时返回 None</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.colleciton.find(&#123;<span class="string">'attribute1'</span>: &#123;<span class="string">'$gte'</span>: <span class="number">12</span>, <span class="string">'$lte'</span>: <span class="number">15</span>&#125;, <span class="string">'attribute2'</span>: value&#125;): <span class="keyword">print</span> item</span><br></pre></td></tr></table></figure><ul><li><strong><code>$in</code> &amp; <code>$nin</code></strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查找符合属性 attribute 等于 (23, 26, 32) 的所有记录，查不到时返回 None</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(&#123;<span class="string">'attribute'</span>: &#123;<span class="string">'$in'</span>: (<span class="number">23</span>, <span class="number">26</span>, <span class="number">32</span>)&#125;&#125;): <span class="keyword">print</span> item </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找符合属性 attribute 不等于 (23, 26, 32) 的所有记录，查不到时返回 None</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(&#123;<span class="string">'attribute'</span>: &#123;<span class="string">'$nin'</span>: (<span class="number">23</span>, <span class="number">26</span>, <span class="number">32</span>)&#125;&#125;): <span class="keyword">print</span> item </span><br><span class="line">    </span><br><span class="line"><span class="comment"># IN 与 查询制定字段结合</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(&#123;<span class="string">'attribute'</span>: &#123;<span class="string">'$in'</span>: (value1, value2)&#125;&#125;, &#123;<span class="string">'_id'</span>: <span class="number">0</span>, <span class="string">'attribute'</span>: <span class="number">1</span>&#125;)</span><br></pre></td></tr></table></figure><h4 id="Logical"><a href="#Logical" class="headerlink" title="Logical"></a>Logical</h4><div class="table-container"><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td>$and</td><td>Joins query clauses with a logical AND returns all documents that match the conditions of both clause.</td></tr><tr><td>$not</td><td>Inverts the effect of a query expression and returns documents that do not match the query expression.</td></tr><tr><td>$nor</td><td>Joins query clauses with a logical NOR returns all documents that fail to match both clauses.</td></tr><tr><td>$or</td><td>Joins query clauses with a logical OR returns all documents that match the conditions of either clause.</td></tr></tbody></table></div><ul><li><strong><code>$or</code></strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(&#123;<span class="string">"$or"</span>:[&#123;<span class="string">"age"</span>:<span class="number">25</span>&#125;, &#123;<span class="string">"age"</span>:<span class="number">28</span>&#125;]&#125;): <span class="keyword">print</span> item</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(&#123;<span class="string">"$or"</span>:[&#123;<span class="string">"age"</span>:&#123;<span class="string">"$lte"</span>:<span class="number">23</span>&#125;&#125;, &#123;<span class="string">"age"</span>:&#123;<span class="string">"$gte"</span>:<span class="number">33</span>&#125;&#125;]&#125;): <span class="keyword">print</span> item</span><br><span class="line"></span><br><span class="line"><span class="comment"># OR 与 查询制定字段结合</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(&#123;<span class="string">'$or'</span>: [&#123;<span class="string">'attribute'</span>: value1&#125;, &#123;<span class="string">'attribute'</span>: value2&#125;]&#125;, &#123;<span class="string">'_id'</span>: <span class="number">0</span>, <span class="string">'attribute'</span>: <span class="number">1</span>&#125;)</span><br></pre></td></tr></table></figure><h4 id="Element"><a href="#Element" class="headerlink" title="Element"></a>Element</h4><div class="table-container"><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td>$exists</td><td>Matches documents that have the specified field.</td></tr><tr><td>$type</td><td>Selects documents if a field is of the specified type.</td></tr></tbody></table></div><ul><li><strong><code>$exists</code></strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查找存在属性 attribute 的所有记录</span></span><br><span class="line">db.collection.find(&#123;<span class="string">'attribute'</span>: &#123;<span class="string">'$exists'</span>:<span class="keyword">True</span>&#125;&#125;)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 查找不存在属性 attribute 的所有记录 </span></span><br><span class="line">db.collection.find(&#123;<span class="string">'attribute'</span>: &#123;<span class="string">'$exists'</span>:<span class="keyword">False</span>&#125;&#125;)</span><br></pre></td></tr></table></figure><ul><li><strong><code>$type</code></strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询属性 attribute 其数据类型为指定 type 的所有记录</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(&#123;<span class="string">'attribute'</span>: &#123;<span class="string">'$type'</span>:<span class="number">1</span>&#125;&#125;): <span class="keyword">print</span> item <span class="comment"># 查询数字类型的</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(&#123;<span class="string">'attribute'</span>: &#123;<span class="string">'$type'</span>:<span class="number">2</span>&#125;&#125;): <span class="keyword">print</span> item <span class="comment"># 查询字符串类型的</span></span><br></pre></td></tr></table></figure><p>各种类型值的代表值:</p><div class="table-container"><table><thead><tr><th style="text-align:center">Condition A</th><th style="text-align:center">Condition B</th><th style="text-align:center">Condition C</th><th style="text-align:center">Condition D</th><th style="text-align:center">Condition E</th></tr></thead><tbody><tr><td style="text-align:center">Double = 1</td><td style="text-align:center">String = 2</td><td style="text-align:center">Object = 3</td><td style="text-align:center">Array = 4</td><td style="text-align:center">Binary data = 5</td></tr><tr><td style="text-align:center">Undefined = 6(Abandon)</td><td style="text-align:center">Object Id = 7</td><td style="text-align:center">Boolean = 8</td><td style="text-align:center">Date = 9</td><td style="text-align:center">Null = 10</td></tr></tbody></table></div><h4 id="Array-1"><a href="#Array-1" class="headerlink" title="Array"></a>Array</h4><div class="table-container"><table><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td>$all</td><td>Matches arrays that contain all elements specified in the query.</td></tr><tr><td>$elemMatch</td><td>Selects documents if element in the array field matches all the specified conditions.</td><td></td></tr><tr><td>$size</td><td>Selects documents if the array field is a specified size.</td></tr></tbody></table></div><ul><li><strong><code>$all</code></strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查找存在属性 attribute 包含全部条件的所有记录 </span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(&#123;<span class="string">'attribute'</span>: &#123;<span class="string">'$all'</span>: (<span class="number">23</span>, <span class="number">26</span>, <span class="number">32</span>)&#125;&#125;): <span class="keyword">print</span> item</span><br></pre></td></tr></table></figure><p><strong>注意和 <code>$in</code> 的区别。<code>$in</code>是检查目标属性值是条件表达式中的一员，而 <code>$all</code> 则要求属性值包含全部条件元素。</strong></p><ul><li><strong><code>$elemMatch</code></strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询属性 attribute 其数组元素数量为指定 size 的所有记录</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(&#123;<span class="string">'attribute'</span>: &#123;<span class="string">'$size'</span>: <span class="number">3</span>&#125;&#125;, &#123;<span class="string">'_id'</span>: <span class="number">0</span>&#125;): <span class="keyword">print</span> item</span><br></pre></td></tr></table></figure><ul><li><strong><code>$size</code></strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询属性 attribute 其数组元素数量为指定 size 的所有记录</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(&#123;<span class="string">'attribute'</span>: &#123;<span class="string">'$size'</span>: <span class="number">3</span>&#125;&#125;, &#123;<span class="string">'_id'</span>: <span class="number">0</span>&#125;): <span class="keyword">print</span> item</span><br></pre></td></tr></table></figure><ul><li><strong><code>$regex</code></strong></li></ul><p>正则表达式查询：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询属性 attribute 满足指定正则表达式，即其值为 'value1', 'value3', 'value5' 的所有记录</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(&#123;<span class="string">'attribute'</span>: &#123;<span class="string">'$regex'</span> : <span class="string">r'(?i)value[135]'</span>&#125;&#125;, &#123;<span class="string">'_id'</span>: <span class="number">0</span>, <span class="string">'attribute'</span>: <span class="number">1</span>&#125;): </span><br><span class="line">    <span class="keyword">print</span> item</span><br></pre></td></tr></table></figure><ul><li><strong><code>count()</code></strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计数</span></span><br><span class="line">print(db.collection.find().count()) </span><br><span class="line">print(db.collection.find(&#123;<span class="string">'attribute'</span>: &#123;<span class="string">'$gt'</span>:<span class="number">30</span>&#125;&#125;).count())</span><br></pre></td></tr></table></figure><ul><li><strong><code>sort()</code></strong></li></ul><p>对记录进行排序，用 <code>sort()</code> 函数，形如 <code>find().sort([(&#39;attribute&#39;,1/-1)])</code> 表示按某属性 <code>attribute</code> 的<strong>升序 / 降序 </strong> 排列：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">pymongo.ASCENDING <span class="comment"># 表按升序排列，也可以用 1 来代替</span></span><br><span class="line">pymongo.DESCENDING <span class="comment"># 表按降序排列， 也可以用 -1 来代替</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find().sort([(<span class="string">'attribute'</span>, pymongo.ASCENDING)]): <span class="keyword">print</span> item</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find().sort([(<span class="string">'attribute'</span>, pymongo.DESCENDING)]): <span class="keyword">print</span> item</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find().sort([(<span class="string">'attribute1'</span>, pymongo.ASCENDING), (<span class="string">'attribute2'</span>, pymongo.DESCENDING)]): </span><br><span class="line">    <span class="keyword">print</span> item </span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(sort = [(<span class="string">'attribute1'</span>, pymongo.ASCENDING), (<span class="string">'attribute2'</span>, pymongo.DESCENDING)]): </span><br><span class="line">    <span class="keyword">print</span> item</span><br></pre></td></tr></table></figure><ul><li><strong>指定字段条件查找 + 排序 + 指定字段显示</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(&#123;<span class="string">'attribute1'</span>: value&#125;, &#123;<span class="string">'_id'</span>: <span class="number">0</span>, <span class="string">'attribute1'</span>: <span class="number">1</span>, <span class="string">'attribute2'</span>: <span class="number">1</span>&#125;) \</span><br><span class="line">                            .sort([(<span class="string">'attribute1'</span>, <span class="number">1</span>), (<span class="string">'attribute2'</span>, <span class="number">-1</span>)]):  <span class="keyword">print</span> item</span><br></pre></td></tr></table></figure><p><strong>可能会出现 <code>OperationFailed: Sort operation used more than the maximum bytes of RAM. Add an index, or specify a smaller limit.</code> 的错误，需要在后面添加一个 <code>limit()</code> 函数：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(&#123;<span class="string">'attribute1'</span>: value&#125;, &#123;<span class="string">'_id'</span>: <span class="number">0</span>, <span class="string">'attribute1'</span>: <span class="number">1</span>, <span class="string">'attribute2'</span>: <span class="number">1</span>&#125;) \</span><br><span class="line">                            .sort([(<span class="string">'attribute1'</span>, <span class="number">1</span>), (<span class="string">'attribute2'</span>, <span class="number">-1</span>)]).limit(<span class="number">100</span>):  <span class="keyword">print</span> item</span><br></pre></td></tr></table></figure><p><strong>比如，我们要做一个排行榜功能，需要在某 <code>collection</code> 中查找分数最多的 100 名玩家。</strong></p><ul><li><strong><code>skip()</code> &amp; <code>limit()</code></strong></li></ul><p>从第几行开始读取<code>skip()</code>，读取多少行<code>limit()</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从第 2 行开始读取，读取 3 行记录</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find().skip(<span class="number">2</span>).limit(<span class="number">3</span>): <span class="keyword">print</span> item</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(skip=<span class="number">2</span>, limit=<span class="number">3</span>): <span class="keyword">print</span> item</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> db.collection.find(&#123;&#125;, &#123;<span class="string">'_id'</span>: <span class="number">0</span>, <span class="string">'attribute1'</span>: <span class="number">1</span>&#125;, skip=<span class="number">2</span>, limit=<span class="number">3</span>): <span class="keyword">print</span> item</span><br></pre></td></tr></table></figure><p><strong><code>skip()</code>和 <code>limit()</code> 结合就能实现分页，当查询时同时使用 <code>sort()</code>、<code>skip()</code> 以及 <code>limit()</code>，无论位置先后，最先执行顺序先<code>sort()</code> 再<code>skip()</code>最后<code>limit()</code>。</strong></p><h1 id="Chapter-3-Common-MongoDB-and-Python-Patterns"><a href="#Chapter-3-Common-MongoDB-and-Python-Patterns" class="headerlink" title="Chapter 3: Common MongoDB and Python Patterns"></a>Chapter 3: Common MongoDB and Python Patterns</h1>]]></content>
    
    <summary type="html">
    
      本文介绍了 MongoDB 的相关知识，以及 PyMongo （Python 上连接 MongoDB 的第三方库）的 API 操作。
    
    </summary>
    
      <category term="Tools" scheme="http://randolph.pro/categories/Tools/"/>
    
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
      <category term="Tools" scheme="http://randolph.pro/tags/Tools/"/>
    
      <category term="MongoDB" scheme="http://randolph.pro/tags/MongoDB/"/>
    
  </entry>
  
  <entry>
    <title>♣︎「TensorFlow」 Use WeChat to Monitor Your Network</title>
    <link href="http://randolph.pro/2017/03/17/%E2%99%A3%EF%B8%8E%E3%80%8CTensorFlow%E3%80%8DUse%20WeChat%20to%20Monitor%20Your%20Network/"/>
    <id>http://randolph.pro/2017/03/17/♣︎「TensorFlow」Use WeChat to Monitor Your Network/</id>
    <published>2017-03-16T16:00:00.000Z</published>
    <updated>2019-03-18T07:55:23.340Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm5.staticflickr.com/4399/36206153111_6662041dd1_o.png" alt></p><p> 有关「TensorFlow」的其他学习笔记系列：<a href="http://randolph.pro/categories/TensorFlow/">「TensorFlow」</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p> 平时，大家自己的机器模型在训练期间（特别是深度网络），训练时间通常几小时到十几小时不等，甚至可能会花上好几天，那么在这段时间，你们又会干些什么事情呢？作为程序员，这里提供一个「有趣的」方式，用你的微信来监控你的模型在训练期间的一举一动。</p><p> 大概的效果是：</p><p><img src="https://farm4.staticflickr.com/3767/32574547714_59711d3f0b_o.jpg" alt></p><p> 程序用到的主角是 Python 中的微信个人号接口 <strong>itchat</strong>。<a href="https://itchat.readthedocs.io/zh/latest/" target="_blank" rel="noopener">What’s itchat?</a> （itchat 的介绍及安装过程）</p><p> 这次，我们要监控的模型是先前提到过的 <a href="http://randolph.pro/2017/03/13/♣%EF%B8%8E「TensorFlow」%20Tensorboard/"> 基于 MNIST 手写体数据集的 CNN 模型 </a>。</p><p> 注意：</p><ol><li> 文章要求读者事先下载安装好 itchat。</li><li> 文章不会详细介绍 TensorFlow 以及 Tensorboard 的知识。</li></ol><h2 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h2><p><strong>OS: macOS Sierra 10.12.x</strong></p><p><strong>Python Version: 3.6.x</strong></p><p><strong>TensorFlow: 1.x</strong></p><p><strong>itchat: 1.2.x</strong></p><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><ul><li>Use WeChat to Monitor Your Network（tensorboard 绘图）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于 MNIST 数据集 的 「CNN」（tensorboard 绘图）</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Import itchat &amp; threading</span></span><br><span class="line"><span class="keyword">import</span> itchat</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a running status flag</span></span><br><span class="line">lock = threading.Lock()</span><br><span class="line">running = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">training_iters = <span class="number">200000</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">display_step = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">initial = tf.truncated_normal(shape, stddev = <span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">initial = tf.constant(<span class="number">0.1</span>, shape = shape)</span><br><span class="line"><span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W, strides=<span class="number">1</span>)</span>:</span></span><br><span class="line"><span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, strides, strides, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x, k=<span class="number">2</span>)</span>:</span></span><br><span class="line"><span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, k, k, <span class="number">1</span>], strides=[<span class="number">1</span>, k, k, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_summaries</span><span class="params">(var)</span>:</span></span><br><span class="line"><span class="string">"""Attach a lot of summaries to a Tensor (for TensorBoard visualization)."""</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'summaries'</span>):</span><br><span class="line">mean = tf.reduce_mean(var)</span><br><span class="line">tf.summary.scalar(<span class="string">'mean'</span>, mean)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'stddev'</span>):</span><br><span class="line">stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))</span><br><span class="line">tf.summary.scalar(<span class="string">'stddev'</span>, stddev)</span><br><span class="line">tf.summary.scalar(<span class="string">'max'</span>, tf.reduce_max(var))</span><br><span class="line">tf.summary.scalar(<span class="string">'min'</span>, tf.reduce_min(var))</span><br><span class="line">tf.summary.histogram(<span class="string">'histogram'</span>, var)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(input_tensor, weights_shape, biases_shape, layer_name, act = tf.nn.relu, flag = <span class="number">1</span>)</span>:</span></span><br><span class="line"><span class="string">"""Reusable code for making a simple neural net layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">It does a matrix multiply, bias add, and then uses relu to nonlinearize.</span></span><br><span class="line"><span class="string">It also sets up name scoping so that the resultant graph is easy to read,</span></span><br><span class="line"><span class="string">and adds a number of summary ops."""</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> tf.name_scope(layer_name):</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</span><br><span class="line">weights = weight_variable(weights_shape)</span><br><span class="line">variable_summaries(weights)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</span><br><span class="line">biases = bias_variable(biases_shape)</span><br><span class="line">variable_summaries(biases)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</span><br><span class="line"><span class="keyword">if</span> flag == <span class="number">1</span>:</span><br><span class="line">preactivate = tf.add(conv2d(input_tensor, weights), biases)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">preactivate = tf.add(tf.matmul(input_tensor, weights), biases)</span><br><span class="line">tf.summary.histogram(<span class="string">'pre_activations'</span>, preactivate)</span><br><span class="line"><span class="keyword">if</span> act == <span class="keyword">None</span>:</span><br><span class="line">outputs = preactivate</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">outputs = act(preactivate, name = <span class="string">'activation'</span>)</span><br><span class="line">tf.summary.histogram(<span class="string">'activation'</span>, outputs)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_train</span><span class="params">(wechat_name, param)</span>:</span></span><br><span class="line"><span class="keyword">global</span> lock, running</span><br><span class="line"><span class="comment"># Lock</span></span><br><span class="line"><span class="keyword">with</span> lock:</span><br><span class="line">running = <span class="keyword">True</span></span><br><span class="line"><span class="comment"># 参数 </span></span><br><span class="line">learning_rate, training_iters, batch_size, display_step = param</span><br><span class="line"></span><br><span class="line"><span class="comment"># Import data</span></span><br><span class="line">mnist_data_path = <span class="string">'MNIST_data/'</span></span><br><span class="line">mnist = input_data.read_data_sets(mnist_data_path, one_hot = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Network Parameters</span></span><br><span class="line">n_input = <span class="number">28</span>*<span class="number">28</span> <span class="comment"># MNIST data input (img shape: 28*28)</span></span><br><span class="line">n_classes = <span class="number">10</span> <span class="comment"># MNIST total classes (0-9 digits)</span></span><br><span class="line">dropout = <span class="number">0.75</span> <span class="comment"># Dropout, probability to keep units</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Input'</span>):</span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_input], name = <span class="string">'input_x'</span>)</span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_classes], name = <span class="string">'target_y'</span>)</span><br><span class="line">keep_prob = tf.placeholder(tf.float32, name = <span class="string">'keep_prob'</span>) <span class="comment">#dropout (keep probability)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cnn_net</span><span class="params">(x, weights, biases, dropout)</span>:</span></span><br><span class="line"><span class="comment"># Reshape input picture</span></span><br><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span> ,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># First Convolutional Layer</span></span><br><span class="line">conv_1 = add_layer(x_image, weights[<span class="string">'conv1_w'</span>], biases[<span class="string">'conv1_b'</span>], <span class="string">'First_Convolutional_Layer'</span>, flag = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># First Pooling Layer</span></span><br><span class="line">pool_1 = max_pool_2x2(conv_1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Second Convolutional Layer </span></span><br><span class="line">conv_2 = add_layer(pool_1, weights[<span class="string">'conv2_w'</span>], biases[<span class="string">'conv2_b'</span>], <span class="string">'Second_Convolutional_Layer'</span>, flag = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Second Pooling Layer </span></span><br><span class="line">pool_2 = max_pool_2x2(conv_2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Densely Connected Layer</span></span><br><span class="line">pool_2_flat = tf.reshape(pool_2, [<span class="number">-1</span>, weight_variable(weights[<span class="string">'dc1_w'</span>]).get_shape().as_list()[<span class="number">0</span>]])</span><br><span class="line">dc_1 = add_layer(pool_2_flat, weights[<span class="string">'dc1_w'</span>], biases[<span class="string">'dc1_b'</span>], <span class="string">'Densely_Connected_Layer'</span>, flag = <span class="number">0</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Dropout</span></span><br><span class="line">dc_1_drop = tf.nn.dropout(dc_1, keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Readout Layer</span></span><br><span class="line">y = add_layer(dc_1_drop, weights[<span class="string">'out_w'</span>], biases[<span class="string">'out_b'</span>], <span class="string">'Readout_Layer'</span>, flag = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment"># Store layers weight &amp; bias</span></span><br><span class="line">weights = &#123;</span><br><span class="line"><span class="comment"># 5x5 conv, 1 input, 32 outputs</span></span><br><span class="line"><span class="string">'conv1_w'</span>: [<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>],</span><br><span class="line"><span class="comment"># 5x5 conv, 32 inputs, 64 outputs</span></span><br><span class="line"><span class="string">'conv2_w'</span>: [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>],</span><br><span class="line"><span class="comment"># fully connected, 7*7*64 inputs, 1024 outputs</span></span><br><span class="line"><span class="string">'dc1_w'</span>: [<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>, <span class="number">1024</span>],</span><br><span class="line"><span class="comment"># 1024 inputs, 10 outputs (class prediction)</span></span><br><span class="line"><span class="string">'out_w'</span>: [<span class="number">1024</span>, n_classes]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">biases = &#123;</span><br><span class="line"><span class="string">'conv1_b'</span>: [<span class="number">32</span>],</span><br><span class="line"><span class="string">'conv2_b'</span>: [<span class="number">64</span>],</span><br><span class="line"><span class="string">'dc1_b'</span>: [<span class="number">1024</span>],</span><br><span class="line"><span class="string">'out_b'</span>: [n_classes]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">y = cnn_net(x, weights, biases, dropout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'cost'</span>):</span><br><span class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_,</span><br><span class="line">logits = y))</span><br><span class="line">tf.summary.scalar(<span class="string">'cost'</span>, cost)</span><br><span class="line">tf.summary.histogram(<span class="string">'cost'</span>, cost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'correct_prediction'</span>):</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">tf.summary.scalar(<span class="string">'accuracy'</span>, accuracy)</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line">train_writer = tf.summary.FileWriter(<span class="string">'train/'</span>, sess.graph)</span><br><span class="line">test_writer = tf.summary.FileWriter(<span class="string">'test/'</span>)</span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model, and also write summaries.</span></span><br><span class="line"><span class="comment"># Every 10th step, measure test-set accuracy, and write test summaries</span></span><br><span class="line"><span class="comment"># All other steps, run train_step on training data, &amp; add training summaries</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Keep training until reach max iterations</span></span><br><span class="line">print(<span class="string">'Wait for lock'</span>)</span><br><span class="line"><span class="keyword">with</span> lock:</span><br><span class="line">run_state = running</span><br><span class="line">print(<span class="string">'Start'</span>)</span><br><span class="line"></span><br><span class="line">step = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> step * batch_size &lt; training_iters <span class="keyword">and</span> run_state:</span><br><span class="line">batch_x, batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line"><span class="comment"># Run optimization op (backprop)</span></span><br><span class="line">sess.run(optimizer, feed_dict = &#123;x: batch_x, y_: batch_y, keep_prob: dropout&#125;)</span><br><span class="line"><span class="keyword">if</span> step % display_step == <span class="number">0</span>:<span class="comment"># Record execution stats</span></span><br><span class="line">run_options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE)</span><br><span class="line">run_metadata = tf.RunMetadata()</span><br><span class="line">summary, _ = sess.run([merged, optimizer], feed_dict = </span><br><span class="line">&#123;x: batch_x, y_: batch_y, keep_prob: <span class="number">1.</span>&#125;, </span><br><span class="line">options = run_options, run_metadata = run_metadata)</span><br><span class="line">train_writer.add_run_metadata(run_metadata, <span class="string">'step %d'</span> % step)</span><br><span class="line">train_writer.add_summary(summary, step)</span><br><span class="line">print(<span class="string">'Adding run metadata for'</span>, step)</span><br><span class="line"></span><br><span class="line">summary, loss, acc = sess.run([merged, cost, accuracy], feed_dict = </span><br><span class="line">&#123;x: batch_x, y_: batch_y, keep_prob: <span class="number">1.</span>&#125;)</span><br><span class="line">print(<span class="string">"Iter"</span> + str(step*batch_size) + <span class="string">", Minibatch Loss="</span> + \</span><br><span class="line"><span class="string">"&#123;:.6f&#125;"</span>.format(loss) + <span class="string">", Training Accuracy="</span> + \</span><br><span class="line"><span class="string">"&#123;:.5f&#125;"</span>.format(acc))</span><br><span class="line">itchat.send(<span class="string">"Iter"</span> + str(step*batch_size) + <span class="string">", Minibatch Loss="</span> + \</span><br><span class="line"><span class="string">"&#123;:.6f&#125;"</span>.format(loss) + <span class="string">", Training Accuracy="</span> + \</span><br><span class="line"><span class="string">"&#123;:.5f&#125;"</span>.format(acc), <span class="string">'filehelper'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">summary, _ = sess.run([merged, optimizer], feed_dict = &#123;x: batch_x, y_: batch_y, keep_prob: <span class="number">1.</span>&#125;)</span><br><span class="line">train_writer.add_summary(summary, step)</span><br><span class="line">step += <span class="number">1</span></span><br><span class="line"><span class="keyword">with</span> lock:</span><br><span class="line">run_state = running</span><br><span class="line">print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line">itchat.send(<span class="string">"Optimization Finished!"</span>, <span class="string">'filehelper'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate accuracy for 256 mnist test images</span></span><br><span class="line">summary, acc = sess.run([merged, accuracy], feed_dict = </span><br><span class="line">&#123;x: mnist.test.images[:<span class="number">256</span>], y_: mnist.test.labels[:<span class="number">256</span>], </span><br><span class="line">keep_prob: <span class="number">1.</span>&#125; )</span><br><span class="line">text_writer.add_summary(summary)</span><br><span class="line">print(<span class="string">"Testing Accuracy:"</span>, acc)</span><br><span class="line">itchat.send(<span class="string">"Testing Accuracy: %s"</span> % acc, wechat_name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@itchat.msg_register([itchat.content.TEXT])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chat_trigger</span><span class="params">(msg)</span>:</span></span><br><span class="line"><span class="keyword">global</span> lock, running, learning_rate, training_iters, batch_size, display_step</span><br><span class="line"><span class="keyword">if</span> msg[<span class="string">'Text'</span>] == <span class="string">u'开始'</span>:</span><br><span class="line">print(<span class="string">'Starting'</span>)</span><br><span class="line"><span class="keyword">with</span> lock:</span><br><span class="line">run_state = running</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> run_state:</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">threading.Thread(target=nn_train, args=(msg[<span class="string">'FromUserName'</span>], (learning_rate, training_iters, batch_size, display_step))).start()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">msg.reply(<span class="string">'Running'</span>)</span><br><span class="line"><span class="keyword">elif</span> msg[<span class="string">'Text'</span>] == <span class="string">u'停止'</span>:</span><br><span class="line">print(<span class="string">'Stopping'</span>)</span><br><span class="line"><span class="keyword">with</span> lock:</span><br><span class="line">running = <span class="keyword">False</span></span><br><span class="line"><span class="keyword">elif</span> msg[<span class="string">'Text'</span>] == <span class="string">u'参数'</span>:</span><br><span class="line">itchat.send(<span class="string">'lr=%f, ti=%d, bs=%d, ds=%d'</span>%(learning_rate, training_iters, batch_size, display_step),msg[<span class="string">'FromUserName'</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">param = msg[<span class="string">'Text'</span>].split()</span><br><span class="line">key, value = param</span><br><span class="line">print(key, value)</span><br><span class="line"><span class="keyword">if</span> key == <span class="string">'lr'</span>:</span><br><span class="line">learning_rate = float(value)</span><br><span class="line"><span class="keyword">elif</span> key == <span class="string">'ti'</span>:</span><br><span class="line">training_iters = int(value)</span><br><span class="line"><span class="keyword">elif</span> key == <span class="string">'bs'</span>:</span><br><span class="line">batch_size = int(value)</span><br><span class="line"><span class="keyword">elif</span> key == <span class="string">'ds'</span>:</span><br><span class="line">display_step = int(value)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line"><span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">itchat.auto_login(hotReload=<span class="keyword">True</span>)</span><br><span class="line">itchat.run()</span><br></pre></td></tr></table></figure><p> 大家可以看到，我对先前的代码进行了一些修改。</p><p> 下面我会对代码中用到 itchat 的部分进行一些简短的说明。</p><ul><li> 代码部分截图：</li></ul><p><img src="https://farm4.staticflickr.com/3687/33417461155_a448845f98_o.png" alt></p><p> 说明：</p><ol><li> 首先我导入了 itchat 和 threading。</li><li> 在原先所有 <strong><code>print</code></strong> 消息的地方，都添加了 <strong><code>itchat.send()</code></strong> 来输出我们的模型训练日志。</li><li> 加了一个带锁的状态量 <strong><code>running</code></strong> 用来做为发送微信消息的运行开关。</li><li> 写了一个 itchat 的 handler（就是上图）。其作用就是当程序运行，我们需要在微信中，对自己的微信号发送「开始」，模型才会开始训练，为了防止信息阻塞，所以要用到 <strong><code>threading</code></strong> 将其放在另一个线程当中。在训练的过程中，如果我们觉得结果已到达我们自己的预期，可以微信发送「停止」来停止模型的训练过程。</li></ol><p><strong> 另外，脚本刚开始运行时，程序会弹出一个包含二维码的图片，我们需要通过微信来扫描该二维码，来登陆微信并启动 itchat 的服务。</strong></p><p> 程序是包含了 Tensorboard 绘图的，所以等模型训练好，我们依然是可以通过 Tensorboard 来更加详细地查看我们模型的训练过程。 </p><p> 至此，我们就可以一边通过微信来监控我们的模型训练过程，一边与身边的朋友们谈笑风生了。</p><p> 如果看过 itchat 那个连接的读者，可以了解到 itchat 同样是可以发送图片信息的，所以我们可以写额外的脚本在训练的过程中每隔 100 次迭代， plot 到目前为止 loss，acc 等指标的趋势图。在此，我就不再进行拓展了。</p><p> 关于各个模块的作用，以及各个变量的意义，我在此就不再赘述了。</p><p> 如果有读者对于 CNN 卷积神经网络有些陌生或者是遗忘，可以参考我的另外一篇文章 <a href="http://randolph.pro/2017/03/07/♛「Machine%20Learning」%20CNN%20Introduction/">♛「Machine Learning」CNN Introduction</a>。</p><p> 如果读者对 Tensorboard 有所遗忘，可以参考我的另一篇文章 <a href="http://randolph.pro/2017/03/13/♣%EF%B8%8E「TensorFlow」%20Tensorboard/">♣︎「TensorFlow」 Tensorboard</a>。</p>]]></content>
    
    <summary type="html">
    
      平时，大家自己的机器模型在训练期间（特别是深度网络），训练时间通常几小时到十几小时不等，甚至可能会花上好几天，那么在这段时间，你们又会干些什么事情呢？作为程序员，这里提供一个「有趣的」方式，用你的微信来监控你的模型在训练期间的一举一动。
    
    </summary>
    
      <category term="TensorFlow" scheme="http://randolph.pro/categories/TensorFlow/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
      <category term="TensorFlow" scheme="http://randolph.pro/tags/TensorFlow/"/>
    
      <category term="WeChat" scheme="http://randolph.pro/tags/WeChat/"/>
    
  </entry>
  
  <entry>
    <title>♣︎「TensorFlow」 Tensorboard</title>
    <link href="http://randolph.pro/2017/03/13/%E2%99%A3%EF%B8%8E%E3%80%8CTensorFlow%E3%80%8D%20Tensorboard/"/>
    <id>http://randolph.pro/2017/03/13/♣︎「TensorFlow」 Tensorboard/</id>
    <published>2017-03-12T16:00:00.000Z</published>
    <updated>2019-03-18T08:15:46.364Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm5.staticflickr.com/4399/36206153111_6662041dd1_o.png" alt></p><p>有关「TensorFlow」的其他学习笔记系列：<a href="http://randolph.pro/categories/TensorFlow/">「TensorFlow」</a></p><h1 id="TensorFlow-Tensorboard"><a href="#TensorFlow-Tensorboard" class="headerlink" title="TensorFlow Tensorboard"></a>TensorFlow Tensorboard</h1><p>Tensorboard 是 TensorFlow 中帮助我们对所构建的静态图 Graph 进行可视化的工具，对于我们初学者理解网络架构、每层网络的细节都是很有帮助的。由于前几天刚接触 TensorFlow，所以在尝试学习 Tensorboard 的过程中，遇到了一些问题。在此基础上，参考了 TensorFlow 官方的 Tensorboard Tutorials 以及网上的一些文章。由于前不久 TensorFlow 1.0 刚发布，网上的一些学习资源或者是 tensorboard 代码在新的版本中并不适用，所以自己改写并实现了官方网站上提及的三个实例的 Tensorboard 版本：</p><ol><li>最基础简单的线性回归模型</li><li>基于 MNIST 手写体数据集的浅层 MLP 模型</li><li>基于 MNIST 手写体数据集的 CNN 模型</li></ol><p>文章不会详细介绍 TensorFlow 以及 Tensorboard 的知识，主要是模型的代码以及部分模型实验截图。</p><p>注意：文章前提默认读者们知晓 TensorFlow，知晓 Tensorboard，以及 TensorFlow 的一些主要概念<strong><code>Variables</code></strong>、<strong><code>Placeholder</code></strong>。还有，默认你已经将需要用到的 MNIST 数据集下载到了你代码当前所在文件夹。</p><h2 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h2><p><strong>OS: macOS Sierra 10.12.x</strong></p><p><strong>Python Version: 3.6.x</strong></p><p><strong>TensorFlow: 1.x</strong></p><h2 id="Tensorboard"><a href="#Tensorboard" class="headerlink" title="Tensorboard"></a>Tensorboard</h2><p>Tensorboard 有几大模块：</p><ul><li><strong>SCALARS</strong>：记录单一变量的，使用 <strong><code>tf.summary.scalar()</code></strong> 收集构建。</li><li><strong>IMAGES</strong>：收集的图片数据，当我们使用的数据为图片时（选用）。</li><li><strong>AUDIO</strong>：收集的音频数据，当我们使用数据为音频时（选用）。</li><li><strong>GRAPHS</strong>：构件图，效果图类似流程图一样，我们可以看到数据的流向，使用 <strong><code>tf.name_scope()</code></strong> 收集构建。</li><li><strong>DISTRIBUTIONS</strong>：用于查看变量的分布值，比如 W（Weights）变化的过程中，主要是在 0.5 附近徘徊。</li><li><strong>HISTOGRAMS</strong>：用于记录变量的历史值（比如 weights 值，平均值等），并使用折线图的方式展现，使用 <strong><code>tf.summary.histogram()</code></strong> 进行收集构建。</li></ul><h2 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h2><h3 id="最简单的线性回归模型"><a href="# 最简单的线性回归模型" class="headerlink" title="最简单的线性回归模型"></a>最简单的线性回归模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(layoutname, inputs, in_size, out_size, act = None)</span>:</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(layoutname):</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</span><br><span class="line">weights = tf.Variable(tf.random_normal([in_size, out_size]), name = <span class="string">'weights'</span>)</span><br><span class="line">w_hist = tf.summary.histogram(<span class="string">'weights'</span>, weights)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</span><br><span class="line">biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>, name = <span class="string">'biases'</span>)</span><br><span class="line">b_hist = tf.summary.histogram(<span class="string">'biases'</span>, biases)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</span><br><span class="line">Wx_plus_b = tf.add(tf.matmul(inputs, weights), biases)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> act <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">outputs = Wx_plus_b</span><br><span class="line"><span class="keyword">else</span> :</span><br><span class="line">outputs = act(Wx_plus_b)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">300</span>)[:,np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>,<span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Input'</span>):</span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>], name = <span class="string">"input_x"</span>)</span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>], name = <span class="string">"target_y"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">l1 = add_layer(<span class="string">"first_layer"</span>, xs, <span class="number">1</span>, <span class="number">10</span>, act = tf.nn.relu)</span><br><span class="line">l1_hist = tf.summary.histogram(<span class="string">'l1'</span>, l1)</span><br><span class="line"></span><br><span class="line">y = add_layer(<span class="string">"second_layout"</span>, l1, <span class="number">10</span>, <span class="number">1</span>, act = <span class="keyword">None</span>)</span><br><span class="line">y_hist = tf.summary.histogram(<span class="string">'y'</span>, y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'loss'</span>): </span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - y), </span><br><span class="line">reduction_indices = [<span class="number">1</span>]))</span><br><span class="line">tf.summary.histogram(<span class="string">'loss'</span>, loss)</span><br><span class="line">tf.summary.scalar(<span class="string">'loss'</span>, loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">ax.scatter(x_data, y_data)</span><br><span class="line">plt.ion()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">'logs/'</span>, sess.graph)</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> train <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">sess.run(train_step, feed_dict = &#123;xs: x_data, ys: y_data&#125;)</span><br><span class="line"><span class="keyword">if</span> train % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">ax.lines.remove(lines[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line"><span class="keyword">pass</span></span><br><span class="line">summary_str = sess.run(merged, feed_dict = &#123;xs: x_data, ys: y_data&#125;)</span><br><span class="line">writer.add_summary(summary_str, train)</span><br><span class="line"></span><br><span class="line">print(train, sess.run(loss, feed_dict = &#123;xs: x_data, ys: y_data&#125;))</span><br><span class="line"></span><br><span class="line">prediction_value = sess.run(y, feed_dict = &#123;xs: x_data&#125;)</span><br><span class="line">lines = ax.plot(x_data, prediction_value, <span class="string">'r-'</span>, lw = <span class="number">5</span>)</span><br><span class="line">plt.pause(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="基于 -MNIST- 手写体数据集的浅层 -MLP- 模型"><a href="# 基于 -MNIST- 手写体数据集的浅层 -MLP- 模型" class="headerlink" title="基于 MNIST 手写体数据集的浅层 MLP 模型"></a>基于 MNIST 手写体数据集的浅层 MLP 模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于 MNIST 手写体数据集的浅层 MLP 模型</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(layoutname, inputs, in_size, out_size, act = None)</span>:</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(layoutname):</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</span><br><span class="line">weights = tf.Variable(tf.zeros([in_size, out_size]), name = <span class="string">'weights'</span>)</span><br><span class="line">w_hist = tf.summary.histogram(<span class="string">"weights"</span>, weights)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</span><br><span class="line">biases = tf.Variable(tf.zeros(out_size), name = <span class="string">'biases'</span>)</span><br><span class="line">b_hist = tf.summary.histogram(<span class="string">"biases"</span>, biases)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</span><br><span class="line">Wx_plus_b = tf.add(tf.matmul(inputs, weights), biases)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> act <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">outputs = Wx_plus_b</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">outputs = act(Wx_plus_b)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"><span class="comment"># Import data</span></span><br><span class="line">mnist_data_path = <span class="string">'MNIST_data/'</span></span><br><span class="line">mnist = input_data.read_data_sets(mnist_data_path, one_hot = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Input'</span>):</span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">28</span> * <span class="number">28</span>], name = <span class="string">'input_x'</span>)</span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>], name = <span class="string">'target_y'</span>)</span><br><span class="line"></span><br><span class="line">y = add_layer(<span class="string">"hidden_layout"</span>, x, <span class="number">28</span>*<span class="number">28</span>, <span class="number">10</span>, act = tf.nn.softmax)</span><br><span class="line">y_hist = tf.summary.histogram(<span class="string">'y'</span>, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># labels 真实值 logits 预测值</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'loss'</span>):</span><br><span class="line">cross_entroy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_,</span><br><span class="line">logits = y))</span><br><span class="line">tf.summary.histogram(<span class="string">'cross entropy'</span>, cross_entroy)</span><br><span class="line">tf.summary.scalar(<span class="string">'cross entropy'</span>, cross_entroy)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entroy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test trained model</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'test'</span>):</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">tf.summary.scalar(<span class="string">'accuracy'</span>, accuracy)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">'logs/'</span>, sess.graph)</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line"><span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">feed = &#123;x: mnist.test.images, y_: mnist.test.labels&#125;</span><br><span class="line">result = sess.run([merged, accuracy], feed_dict = feed)</span><br><span class="line">summary_str = result[<span class="number">0</span>]</span><br><span class="line">acc = result[<span class="number">1</span>]</span><br><span class="line">writer.add_summary(summary_str, i)</span><br><span class="line">print(i, acc)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">feed = &#123;x: batch_xs, y_: batch_ys&#125;</span><br><span class="line">sess.run(train_step, feed_dict = feed)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'final result:'</span>, sess.run(accuracy, feed_dict = &#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure><h3 id="基于 -MNIST- 手写体数据集的 -CNN- 模型"><a href="# 基于 -MNIST- 手写体数据集的 -CNN- 模型" class="headerlink" title="基于 MNIST 手写体数据集的 CNN 模型"></a>基于 MNIST 手写体数据集的 CNN 模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于 MNIST 数据集的 CNN 模型</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">initial = tf.truncated_normal(shape, stddev = <span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">initial = tf.constant(<span class="number">0.1</span>, shape = shape)</span><br><span class="line"><span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line"><span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line"><span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_summaries</span><span class="params">(var)</span>:</span></span><br><span class="line"><span class="string">"""Attach a lot of summaries to a Tensor (for TensorBoard visualization)."""</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'summaries'</span>):</span><br><span class="line">mean = tf.reduce_mean(var)</span><br><span class="line">tf.summary.scalar(<span class="string">'mean'</span>, mean)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'stddev'</span>):</span><br><span class="line">stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))</span><br><span class="line">tf.summary.scalar(<span class="string">'stddev'</span>, stddev)</span><br><span class="line">tf.summary.scalar(<span class="string">'max'</span>, tf.reduce_max(var))</span><br><span class="line">tf.summary.scalar(<span class="string">'min'</span>, tf.reduce_min(var))</span><br><span class="line">tf.summary.histogram(<span class="string">'histogram'</span>, var)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(input_tensor, weights_shape, biases_shape, layer_name, act = tf.nn.relu, flag = <span class="number">1</span>)</span>:</span></span><br><span class="line"><span class="string">"""Reusable code for making a simple neural net layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">It does a matrix multiply, bias add, and then uses relu to nonlinearize.</span></span><br><span class="line"><span class="string">It also sets up name scoping so that the resultant graph is easy to read,</span></span><br><span class="line"><span class="string">and adds a number of summary ops."""</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> tf.name_scope(layer_name):</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</span><br><span class="line">weights = weight_variable(weights_shape)</span><br><span class="line">variable_summaries(weights)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</span><br><span class="line">biases = bias_variable(biases_shape)</span><br><span class="line">variable_summaries(biases)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</span><br><span class="line"><span class="keyword">if</span> flag == <span class="number">1</span>:</span><br><span class="line">preactivate = tf.add(conv2d(input_tensor, weights), biases)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">preactivate = tf.add(tf.matmul(input_tensor, weights), biases)</span><br><span class="line">tf.summary.histogram(<span class="string">'pre_activations'</span>, preactivate)</span><br><span class="line"><span class="keyword">if</span> act == <span class="keyword">None</span>:</span><br><span class="line">outputs = preactivate</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">outputs = act(preactivate, name = <span class="string">'activation'</span>)</span><br><span class="line">tf.summary.histogram(<span class="string">'activation'</span>, outputs)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment"># Import data</span></span><br><span class="line">mnist_data_path = <span class="string">'MNIST_data/'</span></span><br><span class="line">mnist = input_data.read_data_sets(mnist_data_path, one_hot = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Input'</span>):</span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">28</span>*<span class="number">28</span>], name = <span class="string">'input_x'</span>)</span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>], name = <span class="string">'target_y'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># First Convolutional Layer</span></span><br><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span> ,<span class="number">1</span>])</span><br><span class="line">conv_1 = add_layer(x_image, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>], [<span class="number">32</span>], <span class="string">'First_Convolutional_Layer'</span>, flag = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># First Pooling Layer</span></span><br><span class="line">pool_1 = max_pool_2x2(conv_1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Second Convolutional Layer </span></span><br><span class="line">conv_2 = add_layer(pool_1, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>], [<span class="number">64</span>], <span class="string">'Second_Convolutional_Layer'</span>, flag = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Second Pooling Layer </span></span><br><span class="line">pool_2 = max_pool_2x2(conv_2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Densely Connected Layer</span></span><br><span class="line">pool_2_flat = tf.reshape(pool_2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">dc_1 = add_layer(pool_2_flat, [<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>, <span class="number">1024</span>], [<span class="number">1024</span>], <span class="string">'Densely_Connected_Layer'</span>, flag = <span class="number">0</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Dropout</span></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">dc_1_drop = tf.nn.dropout(dc_1, keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Readout Layer</span></span><br><span class="line">y = add_layer(dc_1_drop, [<span class="number">1024</span>, <span class="number">10</span>], [<span class="number">10</span>], <span class="string">'Readout_Layer'</span>, flag = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'cross_entroy'</span>):</span><br><span class="line">cross_entroy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_,</span><br><span class="line">logits = y))</span><br><span class="line">tf.summary.scalar(<span class="string">'cross_entropy'</span>, cross_entroy)</span><br><span class="line">tf.summary.histogram(<span class="string">'cross_entropy'</span>, cross_entroy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entroy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'correct_prediction'</span>):</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">tf.summary.scalar(<span class="string">'accuracy'</span>, accuracy)</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line">train_writer = tf.summary.FileWriter(<span class="string">'train/'</span>, sess.graph)</span><br><span class="line">test_writer = tf.summary.FileWriter(<span class="string">'test/'</span>)</span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_dict</span><span class="params">(train)</span>:</span></span><br><span class="line"><span class="keyword">if</span> train:</span><br><span class="line">batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">k = <span class="number">0.5</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">batch_xs, batch_ys = mnist.test.images, mnist.test.labels</span><br><span class="line">k = <span class="number">1.0</span></span><br><span class="line"><span class="keyword">return</span> &#123;x: batch_xs, y_: batch_ys, keep_prob: k&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model, and also write summaries.</span></span><br><span class="line"><span class="comment"># Every 10th step, measure test-set accuracy, and write test summaries</span></span><br><span class="line"><span class="comment"># All other steps, run train_step on training data, &amp; add training summaries</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line"><span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:<span class="comment"># Record summaries and test-set accuracy</span></span><br><span class="line">summary, acc = sess.run([merged, accuracy], feed_dict = feed_dict(<span class="keyword">False</span>))</span><br><span class="line">test_writer.add_summary(summary, i)</span><br><span class="line">print(<span class="string">"step %d, training accuracy %g"</span> %(i, acc))</span><br><span class="line"><span class="keyword">else</span>:<span class="comment"># Record train set summaries, and train</span></span><br><span class="line"><span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">99</span>:<span class="comment"># Record execution stats</span></span><br><span class="line">run_options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE)</span><br><span class="line">run_metadata = tf.RunMetadata()</span><br><span class="line">summary, _ = sess.run([merged, train_step], feed_dict = feed_dict(<span class="keyword">True</span>), </span><br><span class="line">options = run_options, run_metadata = run_metadata)</span><br><span class="line">train_writer.add_run_metadata(run_metadata, <span class="string">'step %d'</span> % i)</span><br><span class="line">train_writer.add_summary(summary, i)</span><br><span class="line">print(<span class="string">'Adding run metadata for'</span>, i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">summary, _ = sess.run([merged, train_step], feed_dict = feed_dict(<span class="keyword">True</span>))</span><br><span class="line">train_writer.add_summary(summary, i)</span><br><span class="line">main()</span><br></pre></td></tr></table></figure><p>可能对于最后一个模型 CNN 的代码，需要一些 CNN 卷积神经网络的一些知识。例如什么是卷积、池化，还需要了解 TensorFlow 中用到的相应函数，如 <strong><code>tf.nn.conv2d()</code></strong>，<strong><code>tf.nn.max_pool()</code></strong>，这里不再赘述，可以参考我的这篇文章：<a href="http://randolph.pro/2017/03/07/♛「Machine%20Learning」%20CNN%20Introduction/">♛「Machine Learning」CNN Introduction</a>。</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>贴上最后一个模型的部分代码截图：</p><p><img src="https://farm4.staticflickr.com/3681/32385203053_a1a401b062_o.png" alt></p><p>说明：<strong>上图右侧是 CNN 网络训练的步数以及对应的结果，程序需要运行挺久时间的，CPU 占用率也很高，建议挂在晚上跑，人去休息睡觉。可以根据自身机器条件修改参数 range(10000)。</strong>。</p><p>上述代码运行完成之后，命令行中跳转到代码生成的「train」文件夹中（其和代码文件存在于同一文件夹中），然后输入 <strong><code>tensorboard --logdir .</code></strong>，等待程序反应之后，浏览器访问<strong><code>localhost:6006</code></strong>（当然你也可以自己定义端口）。如果不出意外，你会得到以下内容：</p><h3 id="Tensorboard-Info"><a href="#Tensorboard-Info" class="headerlink" title="Tensorboard Info"></a>Tensorboard Info</h3><h4 id="Scalars"><a href="#Scalars" class="headerlink" title="Scalars:"></a>Scalars:</h4><p><img src="https://farm3.staticflickr.com/2671/33073147421_7e52b090a1_o.png" alt></p><h4 id="Graphs"><a href="#Graphs" class="headerlink" title="Graphs:"></a>Graphs:</h4><p><img src="https://farm1.staticflickr.com/740/32818486500_bdd7dacc7f_o.png" alt></p><h4 id="Distributions"><a href="#Distributions" class="headerlink" title="Distributions:"></a>Distributions:</h4><p><img src="https://farm4.staticflickr.com/3703/32818489270_ae5fc65e5a_o.png" alt></p><h4 id="Histograms"><a href="#Histograms" class="headerlink" title="Histograms:"></a>Histograms:</h4><p><img src="https://farm3.staticflickr.com/2741/33073150051_260717b598_o.png" alt></p><p>关于各个模块的作用，以及各个变量的意义，开篇已经提及，我在此就不再赘述了。</p><p>另外，在自己的机器模型在训练期间（特别是深度网络），训练时间通常几小时到十几小时不等，甚至可能会花上好几天，那么在这段时间，你们又会干些什么事情呢？作为程序员，这里提供一个「有趣的」方式，可以使用你的微信来监控你的模型在训练期间的一举一动，具体做法参考我的另一篇文章 <a href="http://randolph.pro/2017/03/17/♣%EF%B8%8E「TensorFlow」Use%20WeChat%20to%20Monitor%20Your%20Network/">♣「TensorFlow」Use WeChat to Monitor Your Network</a></p>]]></content>
    
    <summary type="html">
    
      本文主要介绍 TensorFlow 的 Tensorboard 功能。
    
    </summary>
    
      <category term="TensorFlow" scheme="http://randolph.pro/categories/TensorFlow/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
      <category term="TensorFlow" scheme="http://randolph.pro/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>♣︎「TensorFlow」 About Text Classification based on CNN</title>
    <link href="http://randolph.pro/2017/03/10/%E2%99%A3%EF%B8%8E%E3%80%8CTensorFlow%E3%80%8DAbout%20Text%20Classification%20based%20on%20CNN/"/>
    <id>http://randolph.pro/2017/03/10/♣︎「TensorFlow」About Text Classification based on CNN/</id>
    <published>2017-03-09T16:00:00.000Z</published>
    <updated>2019-03-18T08:23:38.741Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm5.staticflickr.com/4373/35507009504_3298ce3029_o.jpg" alt></p><p> 有关「Machine Learning」的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/">「Machine Learning」</a><br> 有关「TensorFlow」的其他学习笔记系列：<a href="http://randolph.pro/categories/TensorFlow/">「TensorFlow」</a></p><h1 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h1><ul><li><strong>Embedding Layer</strong></li><li><strong>Convolution Layer</strong><ul><li>Convolution</li><li>Max-Pooling</li></ul></li><li><strong>Dropout</strong></li><li><strong>Output Layer</strong></li><li><strong>Loss Function</strong></li><li><strong>Accuracy</strong></li></ul><h2 id="Embedding-Layer"><a href="#Embedding-Layer" class="headerlink" title="Embedding Layer"></a>Embedding Layer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>), tf.name_scope(<span class="string">"embedding"</span>):</span><br><span class="line">    W = tf.Variable(</span><br><span class="line">        tf.random_uniform([vocab_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>),</span><br><span class="line">        name=<span class="string">"W"</span>)</span><br><span class="line">    self.embedded_sentence = tf.nn.embedding_lookup(W, self.input_x)</span><br><span class="line">    self.embedded_sentence_expanded = tf.expand_dims(self.embedded_chars, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p> 存储全部 word vector 的矩阵 $W$，$W$ 初始化时是随机 random 出来的，训练过程中并不是每次都会使用全部的 vocabulary，而只是产生一个 batch（batch 中都是 sentence，每个 sentence 标记了出现哪些 word，每个 sentence 最大长度为 <code>max_seq_len</code> 个 word，因此 batch 相当于一个二维列表），这个 batch 就是 <strong>input_x</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.input_x = tf.placeholder(tf.int32, [<span class="keyword">None</span>, sequence_length], name=<span class="string">"input_x"</span>)</span><br></pre></td></tr></table></figure><p><code>tf.nn.embedding_lookup</code>：查找 input_x 中所有的 ids，获取它们的 word vector。batch 中的每个 sentence 的每个 word 都要查找。所以得到的 <code>embedded_sentence</code> 的 shape 应该是 <code>[None, max_seq_len, embedding_size]</code>。<br> 但是，输入的 word vectors 得到之后，下一步就是输入到卷积层，用到 <code>tf.nn.conv2d()</code> 函数，<code>conv2d()</code> 的参数列表：</p><ul><li><code>input: [batch, in_height, in_width, in_channels]</code></li><li><code>filter: [filter_height, filter_width, in_channels, out_channels]</code></li></ul><p> 对比可以发现，就差一个 <code>in_channels</code> 了，而最 simple 的版本也就只有 1 通道（像有的论文中的模型用到了 multichannel）。因此需要 <code>expand dim</code> 来适应 <code>conv2d</code> 的 input 要求，tensorflow 已经提供了这样的功能：</p><blockquote><p>This operation is useful if you want to add a batch dimension to a single element. For example, if you have a single image of shape [height, width, channels], you can make it a batch of 1 image with expand_dims(image, 0), which will make the shape [1, height, width, channels].</p></blockquote><p>Example:</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 't' is a tensor of shape [2]</span></span><br><span class="line">shape(expand_dims(t, -1)) ==&gt; [2, 1]</span><br></pre></td></tr></table></figure><p> 因此只需要 <code>tf.expand_dims(self.embedded_chars, -1)</code>，之后就能在 embedded_sentence 后面加一个 <code>in_channels=1</code> 了。</p><h2 id="Convolution-Layer"><a href="#Convolution-Layer" class="headerlink" title="Convolution Layer"></a>Convolution Layer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a convolution + maxpool layer for each filter size</span></span><br><span class="line">pooled_outputs = []</span><br><span class="line"><span class="keyword">for</span> i, filter_size <span class="keyword">in</span> enumerate(filter_sizes):</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"conv-maxpool-%s"</span> % filter_size):</span><br><span class="line">        <span class="comment"># Convolution Layer</span></span><br><span class="line">        filter_shape = [filter_size, embedding_size, <span class="number">1</span>, num_filters]</span><br><span class="line">        W = tf.Variable(tf.truncated_normal(filter_shape, stddev=<span class="number">0.1</span>), name=<span class="string">"W"</span>)</span><br><span class="line">        b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[num_filters]), name=<span class="string">"b"</span>)</span><br><span class="line">        conv = tf.nn.conv2d(</span><br><span class="line">            self.embedded_sentence_expanded,</span><br><span class="line">            W,</span><br><span class="line">            strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">            padding=<span class="string">"VALID"</span>,</span><br><span class="line">            name=<span class="string">"conv"</span>)</span><br><span class="line">        <span class="comment"># Apply nonlinearity</span></span><br><span class="line">        conv_out = tf.nn.relu(tf.nn.bias_add(conv, b), name=<span class="string">"relu"</span>)</span><br><span class="line">        <span class="comment"># Maxpooling over the outputs</span></span><br><span class="line">        pooled = tf.nn.max_pool(</span><br><span class="line">            conv_out,</span><br><span class="line">            ksize=[<span class="number">1</span>, sequence_length - filter_size + <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">            strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">            padding=<span class="string">'VALID'</span>,</span><br><span class="line">            name=<span class="string">"pool"</span>)</span><br><span class="line">        pooled_outputs.append(pooled)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine all the pooled features</span></span><br><span class="line">num_filters_total = num_filters * len(filter_sizes)</span><br><span class="line">self.pool = tf.concat(pooled_outputs, axis=<span class="number">3</span>)</span><br><span class="line">self.pool_flat = tf.reshape(self.pool, [<span class="number">-1</span>, num_filters_total])</span><br></pre></td></tr></table></figure><h3 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h3><p> 首先，对 <code>filter_sizes</code> 中的每一个 <code>filter_window_size</code>（常见的为 3, 4, 5） 都要进行卷积（每一种 <code>size</code> 都要产生 <code>num_filters</code> 个 <code>filter maps</code> 特征图），所以外层就是一个大的 <strong>for</strong> 循环。</p><p> 由于在 <strong>for</strong> 循环内部，<code>filter_size</code> 是固定了的，所以每个 filter 的形状为 <code>filter_shape = [filter_size, embedding_size, 1, num_filters]</code>。之所以要弄清楚 <code>filter shape</code>，是因为要对 filter 的权重矩阵 $w$ 进行初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(tf.truncated_normal(filter_shape, stddev=<span class="number">0.1</span>), name=<span class="string">"W"</span>)</span><br></pre></td></tr></table></figure><p> 这里为什么要使用 <code>tf.truncated_normal()</code> 函数？</p><p> 这是因为 tensorflow 中提供了两个 normal 函数：</p><ul><li><code>tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</code></li><li><code>tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</code></li></ul><p> 对比了一下，这两个函数的参数列表完全相同，不同之处我就直接引用文档中的说明，讲解的很清楚：</p><blockquote><p>Outputs random values from a truncated normal distribution.<br>The generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.</p></blockquote><p> 也就是说 random 出来的值的范围都在 <code>[mean - 2 standard_deviations, mean + 2 standard_deviations]</code> 内。<br> 下图可以告诉你这个范围在哪：</p><p><img style="width:50%" src="https://farm3.staticflickr.com/2320/32925087411_1151d71bc1_o.jpg"></p><script type="math/tex; mode=display">c_{i} = f(w⋅x_{i:i+h-1}+b)</script><p><code>conv2d()</code> 得到的其实是公式中的 $w⋅x $ 的部分，还要加上 <code>bias</code> 项：<code>tf.nn.bias_add(conv, b)</code>，并且通过激励函数 relu：<code>tf.nn.relu</code>。最终得到卷积层的输出 <code>conv_out</code>。</p><p> 那究竟卷积层的输出 <code>conv_out</code> 的 shape 是什么样呢？<br> 官方文档中有一段话解释了卷积后得到的输出结果：</p><ol><li>Flattens the filter to a 2-D matrix with shape <code>[filter_height * filter_width * in_channels, output_channels]</code> .</li><li>Extracts image patches from the input tensor to form a virtual tensor of shape <code>[batch, out_height, out_width, filter_height * filter_width * in_channels]</code> .</li><li>For each patch, right_multiplies the filter matrix and the image patch over.</li></ol><p> 第三步进行了 <strong>right-multiply</strong> 之后得到的结果就是 <code>[batch, out_height, out_width, output_channels]</code>，但是还是不清楚这里的 <code>out_height</code> 和 <code>out_width</code> 到底是什么。 </p><blockquote><p>“VALID” padding means that we slide the filter over our sentence without padding the edges, performing a narrow convolution that gives us an output of shape [1, sequence_length - filter_size + 1, 1, 1]. </p></blockquote><p> 这句话的意思是说 <code>out_height</code> 和 <code>out_width</code> 其实和 <code>padding</code> 的方式有关系，这里选择了 <code>&#39;VALID&#39;</code> 的方式，也就是不在边缘加 padding，得到：</p><ul><li><code>out_height = sequence_length - filter_size + 1</code></li><li><code>out_width = 1</code></li></ul><p> 因此，综合上面的两个解释，我们知道 <code>conv2d-(+bias)-relu</code> 之后得到的卷积输出 <code>conv_out</code>  的 shape 为：</p><p><code>[batch, sequence_length - filter_size + 1, 1, num_filters]</code></p><h3 id="Max-Pooling"><a href="#Max-Pooling" class="headerlink" title="Max-Pooling"></a>Max-Pooling</h3><p> 接下来的工作就是 max-pooling 了，来看一下 tensorflow 中给出的函数:</p><p><code>tf.nn.max_pool(value, ksize, strides, padding, data_format=&#39;NHWC&#39;, name=None)</code></p><p> 其中最重要的两个参数是 <code>value</code> 和 <code>ksize</code>。</p><ul><li><code>value</code> ：相当于是 max pooling 层的输入，在整个网络中就是刚才我们得到的 <code>conv_out</code>，检查了一下它俩的 shape 是一致的，说明可以直接传递到下一层。</li><li><code>ksize</code>：官方解释说是 input tensor 每一维度上的 window size。仔细想一下，其实就是想定义多大的范围来进行 max-pooling，比如在图像中常见的 2*2 的小正方形区域对整个 $h$ 得到 feature map 进行 pooling；但是在 NLP 中，每一个 feature map 的 shape 是 <code>[batch, sequence_length - filter_size + 1, 1, num_filters]</code>，我们想知道每个 feature map 中的最大值，也就是当前 feature map 中最重要的 feature 是哪一个，因此我们设置 <code>ksize=[1, sequence_length - filter_size + 1, 1, 1]</code>。</li></ul><p> 根据 <code>ksize</code> 的设置，和 <code>value</code> 的 shape，可以得到 <code>pooled</code> 的 shape：<code>[batch, 1, 1, num_filters]</code>。</p><p> 这是一个 <code>filter_size</code> 的结果（比如 <code>filter_size = 3</code>），<code>pooled</code> 存储的是当前 filter_size 下每个 sentence  <code>num_filters</code> 个特征图中最重要的 features，将结果 append 到 <code>pooled_outputs</code> 列表中存起来，再对下一个 <code>filter_size</code> （比如 <code>filter_size = 4</code>）进行相同的操作。</p><p> 等到 for 循环结束时，也就是所有的 <code>filter_size</code> 全部进行了 convolution 和 max-pooling 之后，首先需要把相同 <code>filter_size</code> 的所有 pooled 结果 concat 起来（组成 batch），再将不同的 filter_size 之间的结果 concat 起来，最后的到的应该类似于二维数组：<code>[batch, num_filters_total]</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Combine all the pooled features</span></span><br><span class="line">num_filters_total = num_filters * len(filter_sizes)</span><br><span class="line">self.pool = tf.concat(pooled_outputs, axis=<span class="number">3</span>)</span><br><span class="line">self.pool_flat = tf.reshape(self.pool, [<span class="number">-1</span>, num_filters_total])</span><br></pre></td></tr></table></figure><p><code>num_filters_total</code> 一共有 <code>num_filters * len(filter_sizes)</code> 个，比如  $100 * 3 = 300​$ 个，连接的过程需要使用 <code>tf.concat</code>，官方给出的例子很容易理解。</p><p> 最后得到的 <code>pool_flat</code> 就是 shape 为 <code>[batch, 300]</code> 的 tensor。</p><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add dropout</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"dropout"</span>):</span><br><span class="line">    self.h_drop = tf.nn.dropout(self.pool_flat, self.dropout_keep_prob)</span><br></pre></td></tr></table></figure><p>Dropout 仅对隐层的输出层进行 drop，使得有些结点的值不输出给 softmax 层。</p><h2 id="Output-Layer"><a href="#Output-Layer" class="headerlink" title="Output Layer"></a>Output Layer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Final (unnormalized) scores and predictions</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"output"</span>):</span><br><span class="line">    W = tf.get_variable(</span><br><span class="line">        <span class="string">"W"</span>,</span><br><span class="line">        shape=[num_filters_total, num_classes],</span><br><span class="line">        initializer=tf.contrib.layers.xavier_initializer())</span><br><span class="line">    b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[num_classes]), name=<span class="string">"b"</span>)</span><br><span class="line">    self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=<span class="string">"scores"</span>)</span><br><span class="line">    self.softmax_scores = tf.nn.softmax(self.logits, name=<span class="string">"softmax_scores"</span>)</span><br><span class="line">    self.predictions = tf.argmax(self.logits, axis=<span class="number">1</span>, name=<span class="string">"predictions"</span>)</span><br><span class="line">    self.topKPreds = tf.nn.top_k(self.softmax_scores, k=<span class="number">1</span>, sorted=<span class="keyword">True</span>, name=<span class="string">"topKPreds"</span>)</span><br></pre></td></tr></table></figure><p> 输出层其实是个 softmax 分类器，可以得到所有类别的 scores 预测概率值，其中 <code>self.logits</code> 与 <code>self.softmax_scores</code> 的区别就是是否进行了 normalization。</p><p> 随后，通过 <code>tf.argmax()</code> 以及 <code>tf.topKPreds()</code>  选出概率值为最大的那个类别以及所预测出来的概率值。<code>self.logits</code> 与 <code>self.softmax_scores</code> 的 shape 均为：<code>[batch, num_classes]</code>，进行 <code>argmax()</code>  的时候是选取每行的 max，所以 <code>axis=1</code>，而 <code>top_k()</code> 直接取每行概率值最大的数。</p><p> 因此，最后 <code>self.predictions</code> 的 shape 为：<code>[batch, 1]</code>。</p><h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p> 得到了整个网络的输出之后，也就是我们得到了 <code>y_prediction</code> ，但还需要和真实的 <code>y_label</code> 进行比较，以此来确定预测好坏。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CalculateMean cross-entropy loss</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"loss"</span>):</span><br><span class="line">    losses = tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)</span><br><span class="line">    losses = tf.reduce_mean(losses,  name=<span class="string">"softmax_losses"</span>)</span><br><span class="line">    l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) <span class="keyword">for</span> v <span class="keyword">in</span></span><br><span class="line">                          tf.trainable_variables()], name=<span class="string">"l2_losses"</span>) * l2_reg_lambda</span><br><span class="line">    self.loss = tf.add(losses, l2_losses, name=<span class="string">"loss"</span>)</span><br></pre></td></tr></table></figure><p> 还是使用常规的交叉熵 <a href="http://randolph.pro/2017/09/25/♣%EF%B8%8E「TensorFlow」Cross%20Entropy%20Function%20in%20TensorFlow%20/">cross_entropy</a> 作为 loss function。最后一层是全连接层，为了防止过拟合，最后还要在 loss function 中加入 <strong>L2 正则项 </strong>，即 <code>l2_loss</code>。<code>l2_reg_lambda</code> 来确定惩罚的力度。</p><h2 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Accuracy</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"accuracy"</span>):</span><br><span class="line">    correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, <span class="number">1</span>))</span><br><span class="line">    self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, <span class="string">"float"</span>), name=<span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure><p><code>tf.equal(x, y)</code> 返回的是一个 bool tensor，如果 x 与 y 对应位置的值相等就是 <code>true</code>，否则 <code>false</code>。得到的 tensor 是 <code>[batch, 1]</code> 的。</p><p><code>tf.cast(x, dtype)</code> 将 bool tensor 转化成 float 类型的 tensor，方便计算。</p><p><code>tf.reduce_mean()</code> 本身输入的就是一个 float 类型的 vector（元素要么是 <code>0.0</code>，要么是 <code>1.0</code>），直接对这样的 vector 计算 mean 得到的就是 accuracy，不需要指定 reduction_indices。</p>]]></content>
    
    <summary type="html">
    
      本文介绍了 CNN 用于文本分类任务的流程，并结合 TensorFlow 代码详细介绍实现细节。
    
    </summary>
    
      <category term="TensorFlow" scheme="http://randolph.pro/categories/TensorFlow/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
      <category term="TensorFlow" scheme="http://randolph.pro/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>♛「Machine Learning」 CNN Introduction</title>
    <link href="http://randolph.pro/2017/03/07/%E2%99%9B%E3%80%8CMachine%20Learning%E3%80%8D%20CNN%20Introduction/"/>
    <id>http://randolph.pro/2017/03/07/♛「Machine Learning」 CNN Introduction/</id>
    <published>2017-03-06T16:00:00.000Z</published>
    <updated>2019-03-17T15:14:12.538Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm5.staticflickr.com/4300/35460446383_aafc34ca3c_o.jpg" alt></p><p>有关「Machine Learning」的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/">「Machine Learning」</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>在 20 世纪 60 年代，Hubel 和 Wiesel 在研究猫脑皮层中用于局部敏感和方向选择的神经元时发现其独特的网络结构可以有效地降低反馈神经网络的复杂性，继而出了卷积神经网络（Convolutional Neural Networks- 简称 CNN）。</p><p>卷积神经网络（Convolutional Neural Network）虽然很早被出，但是却是近些年才得以发展起来并引起广泛重视的。它是深度学习技术中极具代表的网络结构之一，也是近些年语音分析和图像识别领域的研究热点，后来发现其在 NLP 自然语言处理上的效果同样不俗。它以其特有的局部连接、权值共享网络结构，有效地降低了深度神经网络模型的复杂度，极大地减少了权值的数量。</p><h2 id="Perceptron"><a href="#Perceptron" class="headerlink" title="Perceptron"></a>Perceptron</h2><p>在理解 CNN 卷积神经网络之前，有必要了解神经网络的机制以及缺点，而其中典型的神经元网络就是 MLP （Multi-Layer Perceptron）多层感知器。</p><p>而多层感知器中的 Multi-Layer 是针对单层感知器（Perceptron）而言，单层感知器是最简单的前馈人工神经网络，就是通常人们所说的“神经元”。 </p><p>而单层感知器的基本结构如图所示，其以一个实数向量作为输入，计算这些输入的线性组合，若结果大于某个阈值则输出 1，否则输出 0。</p><h1 id><a href="#" class="headerlink" title="#"></a>#</h1><h2 id="Build-a-Multilayer-Convolutional-Network"><a href="#Build-a-Multilayer-Convolutional-Network" class="headerlink" title="Build a Multilayer Convolutional Network"></a>Build a Multilayer Convolutional Network</h2><p>在 CNN 卷积神经网络中，主要有四个操作：</p><ol><li>卷积</li><li>非线性处理（ReLU）</li><li>池化或者亚采样</li><li>分类（全连接层）</li></ol><p>这些操作对于各个卷积神经网络来说都是基本组件，因此理解它们的工作原理有助于充分了解卷积神经网络。下面我们将会尝试理解各步操作背后的原理。</p><h2 id="What’s-CNN"><a href="#What’s-CNN" class="headerlink" title="What’s CNN"></a>What’s CNN</h2><h3 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h3><p>卷积的主要目的是为了从输入图像中提取特征。卷积可以通过从输入的一小块数据中学到图像的特征，并可以保留像素间的空间关系。让我们举个例子来尝试理解一下卷积是如何处理图像的：</p><p>正如我们上面所说，每张图像都可以看作是像素值的矩阵。考虑一下一个 5 x 5 的图像，它的像素值仅为 0 或者 1（注意对于灰度图像而言，像素值的范围是 0 到 255，下面像素值为 0 和 1 的绿色矩阵仅为特例）：</p><p><img src="https://farm3.staticflickr.com/2446/32873275652_9f6261728c_o.png" alt></p><p>同时，考虑下另一个 3 x 3 的矩阵，如下所示：</p><p><img src="https://farm3.staticflickr.com/2563/32873276412_6330affec6_o.png" alt></p><p>接下来，5 x 5 的图像和 3 x 3 的矩阵的卷积可以按下图所示的动画一样计算：</p><p><img src="https://farm3.staticflickr.com/2233/32873279482_bf439eb5c7_o.gif" alt></p><p>现在停下来好好理解下上面的计算是怎么完成的。我们用橙色的矩阵在原始图像（绿色）上滑动，每次滑动一个像素（也叫做「步长」），在每个位置上，我们计算对应元素的乘积（两个矩阵间），并把乘积的和作为最后的结果，得到输出矩阵（粉色）中的每一个元素的值。注意，3 x 3 的矩阵每次步长中仅可以看到输入图像的一部分。</p><p>在 CNN 的术语中，3x3 的矩阵叫做「滤波器」(filter) 或「核」(kernel) 或者 「特征检测器」(feature detector)，通过在图像上滑动滤波器并计算点乘得到矩阵叫做「卷积特征」(Convolved Feature) 或者 「激活图」(Activation Map) 或者 「特征图」(Feature Map)。记住，滤波器在原始输入图像上的作用是特征检测器。</p><p>从上面图中的动画可以看出，对于同样的输入图像，不同值的滤波器将会生成不同的特征图。比如，对于下面这张输入图像：</p><p><img src="https://farm1.staticflickr.com/711/32873351892_a02b2be853_o.png" alt></p><p>在下表中，我们可以看到不同滤波器对上图卷积的效果。正如表中所示，通过在卷积操作前修改滤波矩阵的数值，我们可以进行诸如边缘检测、锐化和模糊等操作 —— 这表明不同的滤波器可以从图中检测到不同的特征，比如边缘、曲线等。</p><p><img style="width:50%" src="https://farm3.staticflickr.com/2919/32213941213_beabd8f9e5_o.png"></p><p>另一个直观理解卷积操作的好方法是看下面这张图的动画：</p><p><img src="https://farm3.staticflickr.com/2235/32902525261_d8091fe768_o.gif" alt></p><p>滤波器（红色框）在输入图像滑过（卷积操作），生成一个特征图。另一个滤波器（绿色框）在同一张图像上卷积可以得到一个不同的特征图。注意卷积操作可以从原图上获取局部依赖信息。同时注意这两个不同的滤波器是如何从同一张图像上生成不同的特征图。记住上面的图像和两个滤波器仅仅是我们上面讨论的数值矩阵。</p><p>在实践中，CNN 会在训练过程中学习到这些滤波器的值（尽管我们依然需要在训练前指定诸如滤波器的个数、滤波器的大小、网络架构等参数）。我们使用的滤波器越多，提取到的图像特征就越多，网络所能在未知图像上识别的模式也就越好。</p><p>特征图的大小（卷积特征）由下面三个参数控制，我们需要在卷积前确定它们：</p><ul><li><p>深度（Depth）：<strong>深度对应的是卷积操作所需的滤波器个数</strong>。在下图的网络中，我们使用三个不同的滤波器对原始图像进行卷积操作，这样就可以生成三个不同的特征图。你可以把这三个特征图看作是堆叠的 2d 矩阵，那么，特征图的「深度」就是 3。</p></li><li><p>步长（Stride）：<strong>步长是我们在输入矩阵上滑动滤波矩阵的像素数</strong>。当步长为 1 时，我们每次移动滤波器一个像素的位置。当步长为 2 时，我们每次移动滤波器会跳过 2 个像素。步长越大，将会得到更小的特征图。</p></li><li><p>零填充（Zero-padding）：<strong>有时，在输入矩阵的边缘使用零值进行填充，这样我们就可以对输入图像矩阵的边缘进行滤波。</strong>零填充的一大好处是可以让我们控制特征图的大小。使用零填充的也叫做泛卷积，不适用零填充的叫做严格卷积。</p><p><img style="width:50%" src="https://farm3.staticflickr.com/2929/32183844484_8be44cdea0_o.png"></p></li></ul><h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>ReLU 表示修正线性单元（Rectified Linear Unit），是一个非线性操作。</p><p><img style="width:50%" src="https://farm3.staticflickr.com/2745/32986518056_d320066568_o.png"></p><script type="math/tex; mode=display">Output = Max(zero, Input)</script><ol><li><p>为什么要引入非线性激励函数？</p><p>如果不用激励函数（其实相当于激励函数是 $f(x) = x$ ），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐层效果相当，这种情况就是最原始的感知机（Perceptron）了。</p><p>正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。最早的想法是 sigmoid 函数或者 tanh 函数，输出有界，很容易充当下一层输入（以及一些人的生物解释 balabala）。</p></li><li><p>为什么要引入 ReLU 而不是其他的非线性函数（例如 Sigmoid 函数）？</p><ul><li>采用 sigmoid 等函数，<strong>算激活函数时（指数运算），计算量大</strong>，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用 Relu 激活函数，整个过程的计算量节省很多。</li><li>对于深层网络，<strong>sigmoid 函数反向传播时，很容易就会出现梯度消失的情况 </strong>（在 sigmoid 接近饱和区时，变换太缓慢，导数趋于 0，这种情况会造成<strong> 信息丢失</strong>），从而无法完成深层网络的训练。</li><li><strong>Relu 会使一部分神经元的输出为 0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生</strong>（以及一些人的生物解释 balabala）。</li></ul><p>当然现在也有一些对 relu 的改进，比如 prelu，random relu 等，在不同的数据集上会有一些训练速度上或者准确率上的改进，具体的可以找相关的 paper 看。</p><p>（多加一句，现在主流的做法，会多做一步 batch normalization，尽可能保证每一层网络的输入具有相同的分布。而最新的 paper，他们在加入 bypass connection 之后，发现改变 batch normalization 的位置会有更好的效果。）</p></li><li><p>ReLU 的优点与缺点？</p><p>优点：</p><ul><li>解决了 gradient vanishing 问题 (在正区间)</li><li>计算速度非常快，只需要判断输入是否大于 0</li><li>收敛速度远快于 sigmoid 和 tanh</li></ul><p>缺点：</p><ul><li>ReLU 的输出不是 zero-centered</li><li><strong>Dead ReLU Problem</strong>，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate 太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用 Xavier 初始化方法，以及避免将 learning rate 设置太大或使用 adagrad 等自动调节 learning rate 的算法。</li></ul></li></ol><blockquote><p>几十年的机器学习发展中，我们形成了这样一个概念：非线性激活函数要比线性激活函数更加先进。</p><p>尤其是在布满 Sigmoid 函数的 BP 神经网络，布满径向基函数的 SVM 神经网络中，往往有这样的幻觉，非线性函数对非线性网络贡献巨大。</p><p>该幻觉在 SVM 中更加严重。核函数的形式并非完全是 SVM 能够处理非线性数据的主力功臣（支持向量充当着隐层角色）。</p><p>那么在深度网络中，对非线性的依赖程度就可以缩一缩。另外，在上一部分提到，稀疏特征并不需要网络具有很强的处理线性不可分机制。</p><p>综合以上两点，在深度学习模型中，使用简单、速度快的线性激活函数可能更为合适。</p></blockquote><p>ReLU 操作可以从下面的图中理解。这里的输出特征图也可以看作是“修正”过的特征图。</p><p><img style="width:80%" src="https://farm3.staticflickr.com/2421/32214171823_0cbab38971_o.png"></p><p>所谓麻雀虽小，五脏俱全，ReLU 虽小，但也是可以改进的。</p><h4 id="ReLU 的种类"><a href="#ReLU 的种类" class="headerlink" title="ReLU 的种类"></a>ReLU 的种类</h4><p><strong>ReLU 的区分主要在负数端，根据负数端斜率的不同来进行区分</strong>，大致如下图所示。</p><p><img src="https://farm3.staticflickr.com/2929/33186208875_5ab850f2ab_o.png" alt></p><p>普通的 ReLU 负数端斜率是 0，Leaky ReLU 则是负数端有一个比较小的斜率，而 PReLU 则是在后向传播中学习到斜率。而 Randomized Leaky ReLU 则是使用一个均匀分布在训练的时候随机生成斜率，在测试的时候使用均值斜率来计算。</p><h4 id="效果"><a href="# 效果" class="headerlink" title="效果"></a>效果 </h4><p> 其中，NDSB 数据集是 Kaggle 的比赛，而 RReLU 正是在这次比赛中崭露头角的。</p><p><img src="https://farm4.staticflickr.com/3852/32803703170_7605fcdbc2_o.png" alt></p><p>通过上述结果，可以看到四点：</p><ul><li>对于 Leaky ReLU 来说，如果斜率很小，那么与 ReLU 并没有大的不同，当斜率大一些时，效果就好很多。</li><li>在训练集上，PReLU 往往能达到最小的错误率，说明 PReLU 容易过拟合。</li><li>在 NSDB 数据集上 RReLU 的提升比 cifar10 和 cifar100 上的提升更加明显，而 NSDB 数据集比较小，从而可以说明，RReLU 在与过拟合的对抗中更加有效。</li><li>对于 RReLU 来说，还需要研究一下随机化得斜率是怎样影响训练和测试过程的。</li></ul><h4 id="参考文献"><a href="# 参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><p>[1]. Xu B, Wang N, Chen T, et al. Empirical evaluation of rectified activations in convolutional network[J]. arXiv preprint arXiv:1505.00853, 2015.</p><h3 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h3><p>空间池化（Spatial Pooling）（也叫做亚采用或者下采样）<strong>降低了各个特征图的维度，但可以保持大部分重要的信息</strong>。空间池化有下面几种方式：最大化、平均化、加和等等。</p><p>对于最大池化（Max Pooling），我们定义一个空间邻域（比如，2x2 的窗口），并从窗口内的修正特征图中取出最大的元素。除了取最大元素，我们也可以取平均（Average Pooling）或者对窗口内的元素求和。<strong>在实际中，最大池化被证明效果更好一些。</strong></p><p>下面的图展示了使用 2x2 窗口在修正特征图（在卷积 + ReLU 操作后得到）使用最大池化的例子。</p><p><img style="width:50%" src="https://farm3.staticflickr.com/2036/32647435560_470bdc2c7b_o.png"></p><p>我们以 2 个元素（也叫做“步长”）滑动我们 2x2 的窗口，并在每个区域内取最大值。如上图所示，这样操作可以降低我们特征图的维度。</p><p>在下图展示的网络中，池化操作是分开应用到各个特征图的（注意，因为这样的操作，我们可以从三个输入图中得到三个输出图）。</p><p><img style="width:50%" src="https://farm3.staticflickr.com/2202/32873928362_3a7b8b86cf_o.png"></p><p>下图展示了我们在 ReLU 操作之后得到的修正特征图的池化操作的效果：</p><p><img style="width:80%" src="https://farm4.staticflickr.com/3881/32214568423_416fed1642_o.png"></p><p>池化函数可以逐渐降低输入表示的空间尺度。特别地，Pooling 的好处是:</p><ul><li><p>使输入表示（特征维度）变得更小，并且网络中的参数和计算的数量更加可控的减小，因此，可以控制过拟合。</p></li><li><p>使网络对于输入图像中更小的变化、冗余和变换变得不变性（输入的微小冗余将不会改变池化的输出——因为我们在局部邻域中使用了最大化 / 平均值的操作）。</p></li><li><p>帮助我们获取图像最大程度上的尺度不变性（准确的词是“不变性”）。它非常的强大，因为我们可以检测图像中的物体，无论它们位置在哪里。</p></li></ul><p>到目前为止我们了解了卷积、ReLU 和池化是如何操作的。理解这些层是构建任意 CNN 的基础是很重要的。正如下图所示，<strong>我们有两组卷积、ReLU &amp; 池化层 —— 第二组卷积层使用六个滤波器对第一组的池化层的输出继续卷积，得到一共六个特征图</strong>。接下来对所有六个特征图应用 ReLU。接着我们对六个修正特征图分别进行最大池化操作。</p><p>这些层一起就可以从图像中提取有用的特征，并在网络中引入非线性，减少特征维度，同时保持这些特征具有某种程度上的尺度变化不变性。</p><p><img src="https://farm3.staticflickr.com/2044/32988985296_3a7106f13d_o.png" alt></p><p>第二组池化层的输出作为全连接层的输入，接下来我们将介绍全连接层。</p><h3 id="Connect"><a href="#Connect" class="headerlink" title="Connect"></a>Connect</h3><p>全连接层是传统的多层感知器，在输出层使用的是 softmax 激活函数（也可以使用其他像 SVM 的分类器，但在本文中只使用 softmax）。「全连接」(Fully Connected) 这个词表明前面层的所有神经元都与下一层的所有神经元连接。</p><p>卷积和池化层的输出表示了输入图像的高级特征。全连接层的目的是为了使用这些特征把输入图像基于训练数据集进行分类。比如，在下面图中我们进行的图像分类有四个可能的输出结果（注意下图并没有显示全连接层的节点连接）。</p><p><img style="width:80%" src="https://farm1.staticflickr.com/350/32875720272_88dd409c3f_o.png"></p><p>除了分类，添加一个全连接层也（一般）是学习这些特征的非线性组合的简单方法。从卷积和池化层得到的大多数特征可能对分类任务有效，但这些特征的组合可能会更好。</p><p>从全连接层得到的输出概率和为 1。这个可以在输出层使用 softmax 作为激活函数进行保证。softmax 函数输入一个任意大于 0 值的矢量，并把它们转换为零一之间的数值矢量，其和为一。</p><h3 id="Use-Backpropagation-to-Train-whole-network"><a href="#Use-Backpropagation-to-Train-whole-network" class="headerlink" title="Use Backpropagation to Train whole network"></a>Use Backpropagation to Train whole network</h3><p>正如上面讨论的，卷积 + 池化层的作用是从输入图像中提取特征，而全连接层的作用是分类器。</p><p>注意在下面的图中，因为输入的图像是船，对于船这一类的目标概率是 1，而其他三类的目标概率是 0，即</p><ul><li><p>输入图像 = 船</p></li><li><p>目标矢量 = [0, 0, 1, 0]</p><p><img src="https://farm1.staticflickr.com/739/32216466493_38095200db_o.png" alt></p></li></ul><p>完整的卷积网络的训练过程可以总结如下：</p><ul><li>第一步：我们初始化所有的滤波器，使用随机值设置参数 / 权重</li><li>第二步：网络接收一张训练图像作为输入，通过前向传播过程（卷积、ReLU 和池化操作，以及全连接层的前向传播），找到各个类的输出概率</li><li><ul><li>我们假设船这张图像的输出概率是 [0.2, 0.4, 0.1, 0.3]</li><li>因为对于第一张训练样本的权重是随机分配的，输出的概率也是随机的</li></ul></li><li>第三步：在输出层计算总误差（计算 4 类的和）</li><li><ul><li>Total Error = ∑  ½ (target probability – output probability) ²</li></ul></li><li>第四步：使用反向传播算法，根据网络的权重计算误差的梯度，并使用梯度下降算法更新所有滤波器的值 / 权重以及参数的值，使输出误差最小化</li><li><ul><li>权重的更新与它们对总误差的占比有关</li><li>当同样的图像再次作为输入，这时的输出概率可能会是 [0.1, 0.1, 0.7, 0.1]，这就与目标矢量 [0, 0, 1, 0] 更接近了</li><li>这表明网络已经通过调节权重 / 滤波器，可以正确对这张特定图像的分类，这样输出的误差就减小了</li><li>像滤波器数量、滤波器大小、网络结构等这样的参数，在第一步前都是固定的，在训练过程中保持不变——仅仅是滤波器矩阵的值和连接权重在更新</li></ul></li><li>第五步：对训练数据中所有的图像重复步骤 1 ~ 4</li></ul><p>上面的这些步骤可以 <strong> 训练</strong> ConvNet —— 这本质上意味着对于训练数据集中的图像，ConvNet 在更新了所有权重和参数后，已经优化为可以对这些图像进行正确分类。</p><p>当一张新的（未见过的）图像作为 ConvNet 的输入，网络将会再次进行前向传播过程，并输出各个类别的概率（对于新的图像，输出概率是使用已经在前面训练样本上优化分类的参数进行计算的）。如果我们的训练数据集非常的大，网络将会（有希望）对新的图像有很好的泛化，并把它们分到正确的类别中去。</p><p><strong>注 1</strong>: 上面的步骤已经简化，也避免了数学详情，只为提供训练过程的直观内容。</p><p><strong>注 2</strong>: 在上面的例子中我们使用了两组卷积和池化层。然而请记住，这些操作可以在一个 ConvNet 中重复多次。实际上，现在有些表现最好的 ConvNet 拥有多达十几层的卷积和池化层！同时，每次卷积层后面不一定要有池化层。如下图所示，我们可以在池化操作前连续使用多个卷积 + ReLU 操作。还有，请注意 ConvNet 的各层在下图中是如何可视化的。</p><p><img style="width:80%" src="https://farm3.staticflickr.com/2113/32875828582_ce237c84d2_o.png"></p><h3 id="Visualization-on-CNN"><a href="#Visualization-on-CNN" class="headerlink" title="Visualization on CNN"></a>Visualization on CNN</h3><p>一般而言，越多的卷积步骤，网络可以学到的识别特征就越复杂。比如，ConvNet 的图像分类可能在第一层从原始像素中检测出边缘，然后在第二层使用边缘检测简单的形状，接着使用这些形状检测更高级的特征，比如更高层的人脸。下面的图中展示了这些内容——我们使用 <a href="http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf" target="_blank" rel="noopener"> 卷积深度置信网络 </a> 学习到的特征，这张图仅仅是用来证明上面的内容（这仅仅是一个例子：真正的卷积滤波器可能会检测到对我们毫无意义的物体）。</p><p><img src="https://farm4.staticflickr.com/3738/33031508435_aee3cd62ca_o.png" alt></p><p>Adam Harley 创建了一个卷积神经网络的可视化结果，使用的是 MNIST 手写数字的训练集。我强烈建议使用它来理解 CNN 的工作原理。</p><p>我们可以在下图中看到网络是如何识别输入 「8」 的。注意下图中的可视化并没有单独展示 ReLU 操作。</p><p><img src="https://farm4.staticflickr.com/3934/32186606394_d904c8de2e_o.png" alt></p><p>输入图像包含 1024 个像素（32 x 32 大小），第一个卷积层（卷积层 1）由六个独特的 5x5 （步长为 1）的滤波器组成。如图可见，使用六个不同的滤波器得到一个深度为六的特征图。</p><p>卷积层 1 后面是池化层 1，在卷积层 1 得到的六个特征图上分别进行 2x2 的最大池化（步长为 2）的操作。你可以在池化层上把鼠标移动到任意的像素上，观察在前面卷积层（如上图所示）得到的 4x4 的小格。你会发现 4x4 小格中的最大值（最亮）的像素将会进入到池化层。</p><p><img src="https://farm1.staticflickr.com/682/32186608014_f69038eb43_o.png" alt></p><p>池化层 1 后面的是六个 5x5 （步长为 1）的卷积滤波器，进行卷积操作。后面就是池化层 2，进行 2x2 的最大池化（步长为 2）的操作。这两层的概念和前面描述的一样。</p><p>接下来我们就到了三个全连接层。它们是：</p><ul><li>第一个全连接层有 120 个神经元</li><li>第二层全连接层有 100 个神经元</li><li>第三个全连接层有 10 个神经元，对应 10 个数字——也就做输出层</li></ul><p>注意在下图中，输出层中的 10 个节点的各个都与第二个全连接层的所有 100 个节点相连（因此叫做全连接）。</p><p>同时，注意在输出层那个唯一的亮的节点是如何对应于数字 “8” 的——这表明网络把我们的手写数字正确分类（越亮的节点表明从它得到的输出值越高，即，8 是所有数字中概率最高的）。</p><p><img src="https://farm3.staticflickr.com/2830/32216998463_c7897cf1d5_o.png" alt></p><p>同样的 3D 可视化可以在 <a href="http://scs.ryerson.ca/~aharley/vis/conv/" target="_blank" rel="noopener"> 这里 </a> 看到。</p><h3 id="Other-ConvNet"><a href="#Other-ConvNet" class="headerlink" title="Other ConvNet"></a>Other ConvNet</h3><p>卷积神经网络从上世纪 90 年代初期开始出现。我们上面提到的 LeNet 是早期卷积神经网络之一。其他有一定影响力的架构如下所示：</p><ul><li>LeNet (1990s)： 本文已介绍。</li><li>1990s to 2012：在上世纪 90 年代后期至 2010 年初期，卷积神经网络进入孵化期。随着数据量和计算能力的逐渐发展，卷积神经网络可以处理的问题变得越来越有趣。</li><li>AlexNet (2012) – 在 2012，Alex Krizhevsky （与其他人）发布了 <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">AlexNet</a>，它是比 LeNet 更深更宽的版本，并在 2012 年的 ImageNet 大规模视觉识别大赛（ImageNet Large Scale Visual Recognition Challenge，ILSVRC）中以巨大优势获胜。这对于以前的方法具有巨大的突破，当前 CNN 大范围的应用也是基于这个工作。</li><li>ZF Net (2013) – ILSVRC 2013 的获胜者是来自 Matthew Zeiler 和 Rob Fergus 的卷积神经网络。它以 <a href="http://arxiv.org/abs/1311.2901" target="_blank" rel="noopener">ZFNet</a> （Zeiler &amp; Fergus Net 的缩写）出名。它是在 AlexNet 架构超参数上进行调整得到的效果提升。</li><li>GoogLeNet (2014) – ILSVRC 2014 的获胜者是来自于 Google 的 <a href="http://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">Szegedy</a>等人的卷积神经网络。它的主要贡献在于使用了一个 Inception 模块，可以大量减少网络的参数个数（4M，AlexNet 有 60M 的参数）。</li><li>VGGNet (2014) – 在 ILSVRC 2014 的领先者中有一个 <a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/" target="_blank" rel="noopener">VGGNet</a> 的网络。它的主要贡献是展示了网络的深度（层数）对于性能具有很大的影响。</li><li>ResNets (2015) – <a href="http://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">残差网络 </a> 是何凯明（和其他人）开发的，并赢得 ILSVRC 2015 的冠军。ResNets 是当前卷积神经网络中最好的模型，也是实践中使用 ConvNet 的默认选择（截至到 2016 年五月）。</li><li>DenseNet (2016 八月) – 近来由 Gao Huang （和其他人）发表的，<a href="http://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">the Densely Connected Convolutional Network</a> 的各层都直接于其他层以前向的方式连接。DenseNet 在五种竞争积累的目标识别基准任务中，比以前最好的架构有显著的提升。可以在 <a href="https://github.com/liuzhuang13/DenseNet" target="_blank" rel="noopener"> 这里 </a> 看 Torch 实现。</li></ul>]]></content>
    
    <summary type="html">
    
      本文主要介绍 Deep Learning 中的 CNN 卷积神经网络。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>♛「Machine Learning」 Factorization Machines</title>
    <link href="http://randolph.pro/2017/03/03/%E2%99%9B%E3%80%8CMachine%20Learning%E3%80%8D%20Factorization%20Machines/"/>
    <id>http://randolph.pro/2017/03/03/♛「Machine Learning」 Factorization Machines/</id>
    <published>2017-03-02T16:00:00.000Z</published>
    <updated>2019-03-17T15:14:26.280Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm5.staticflickr.com/4300/35460446383_aafc34ca3c_o.jpg" alt></p><p> 有关「Machine Learning」的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/">「Machine Learning」</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Factorization Machines（FM），可译为“隐因子分解机”，由 Steffen Rendle 于 2010 年提出，并发布开源工具 <strong>libFM</strong> 。他凭借 FM 单个模型，他在 KDD Cup 2012 上，取得 Track1 的第 2 名和 Track2 的第 3 名。</p><h1 id="Compared-with-Other-Models"><a href="#Compared-with-Other-Models" class="headerlink" title="Compared with Other Models"></a>Compared with Other Models</h1><ul><li> 在数据非常稀疏时（如推荐系统），SVM 不能取得很好的效果。</li><li> 对带非线性核函数的 SVM，需要在对偶问题上进行求解。</li><li> 目前还有很多不同的 factorization models ，比如 matrix factorization 和一些特殊的模型 SVD++, PITF, FPMC。这些模型的一个缺点是它们只适用于某些特定的输入数据，优化算法也需要根据问题专门设计。</li><li>FM 适用于实数值的特征向量。并且经过一些变换，可以看出 FM 囊括了以上方法。</li></ul><h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><p> 首先考虑线性模型：</p><script type="math/tex; mode=display">\hat y (x) = w_0 + w_1x_1 + w_2x_2+ \dots + w_nx_n = w_0 + \sum_{i=1}^{n}w_ix_i</script><p> 各特征分量之间是相互孤立的。</p><p>$\hat y(x)$ 仅考虑单个的特征分量，而没有考虑特征分量之间的相互关系。</p><p> 考虑任意两个特征分量之间的关系：</p><script type="math/tex; mode=display">\hat y (x) := w_0 +  \sum_{i=1}^{n}w_ix_i + \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}W_{ij}x_ix_j</script><p> 在数据高度系数的场景中，上述模型由很大的缺陷：</p><p> 当 <script type="math/tex">x_i</script> 与 <script type="math/tex">x_j</script> 未出现过交互时，不能对相应的参数 <script type="math/tex">w_{ij}</script> 进行估计， <script type="math/tex">w_{ij}</script> 一定为 0 。</p><p> 因此，需要对每个维度的特征分量 $x_i$ ，引入：</p><script type="math/tex; mode=display">v_i = (v_{i1},v_{i2}, \dots ,v_{ik})^T \epsilon \ \mathbb{R} ^k, \ i = 1,2,\dots,n</script><p> 改写 $w_{ij}$ :</p><script type="math/tex; mode=display">\hat w_{ij} = v_i^Tv_j := \sum_{l=1}^{k}v_{il}v_{jl}</script><h2 id="2-way-FM"><a href="#2-way-FM" class="headerlink" title="2-way FM"></a>2-way FM</h2><script type="math/tex; mode=display">\hat y(x) := w_0 + \sum_{i=1}^{n}w_ix_i + \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\left \langle  v_i, v_j\right \rangle x_ix_j</script><script type="math/tex; mode=display">w_0 \ \epsilon\  \mathbb{R},w \ \epsilon\  \mathbb{R}^n, V \ \epsilon\  \mathbb{R}^{n\times k}</script><script type="math/tex; mode=display">\left \langle  v_i, v_j\right \rangle := \sum_{f=1}^{k}v_{i,f} \cdot v_{j,f}</script><ul><li>$v_i$ 用 $k$ 个因子描述第 $i$ 个变量。</li><li> 正整数 $k$ 是超参，决定了分解的维度。</li></ul><h2 id="Complexity-Analysis"><a href="#Complexity-Analysis" class="headerlink" title="Complexity Analysis"></a>Complexity Analysis</h2><p>FM 模型中需要估计的参数包括 <script type="math/tex">w_0 \ \epsilon\  \mathbb{R},w \ \epsilon\  \mathbb{R}^n, V \ \epsilon\  \mathbb{R}^{n\times k}</script>，共 <script type="math/tex">1+n+n*k</script> 个，<script type="math/tex">w_{0}</script> 为整体的偏置量，$w$ 对特征向量的各个分量的强度进行建模，$V$ 对特征向量中任意两个分量之间的关系进行建模。</p><p> 直观上看，上述模型的计算复杂度是 $O(kn^2)$ ，但是经过下面的改写后：</p><script type="math/tex; mode=display">\begin{align}& \sum_{i=1}^{n}\sum_{j=i+1}^{n}\left \langle  v_i, v_j\right \rangle x_ix_j \cr=& \frac{1}{2} \sum_{i=1}^{n}\sum_{j=1}^{n}\left \langle  v_i, v_j\right \rangle x_ix_j - \frac{1}{2} \sum_{i=1}^{n}\left \langle  v_i, v_i\right \rangle x_ix_i\cr =& \frac{1}{2} \left (\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{f=1}^{k} v_{i,f} \cdot v_{j,f} \ x_ix_j - \sum_{i=1}^{n}\sum_{f=1}^{k} v_{i,f} \cdot v_{i,f} \ x_ix_i \right) \cr=& \frac{1}{2} \sum_{f=1}^{k} \left(\left( \sum_{i=1}^{n}v_{i,f}x_{i}\right) \left(\sum_{j=1}^{n}v_{j,f}x_{j}\right) - \sum_{i=1}^{n}v_{i,f}^2 x_{i}^2\right) \cr=& \frac{1}{2}  \sum_{f=1}^{k} \left(\left( \sum_{i=1}^{n}v_{i,f}x_{i}\right)^2 - \sum_{i=1}^{n}v_{i,f}^2 x_{i}^2\right)\cr\end{align}</script><p> 计算复杂度经过改写后降低到线性的 $O(kn)$ 。</p><h2 id="d-way-FM"><a href="#d-way-FM" class="headerlink" title="d-way FM"></a>d-way FM</h2><p> 方程同时刻画 $l(1 \leq l \leq d)$ 个特征向量之间的相互关系：</p><script type="math/tex; mode=display">\hat y(x) := w_0 + \sum_{i=1}^{n}w_ix_i + \sum_{l=2}^{d}\sum_{i_{1}=1}^{n}\dots \sum_{i_{t} = i_{t-1}+1}^{n}\left (\prod_{j=1}^{l}x_{i_{j}} \right)\left (\prod_{f=1}^{k_{l}}\prod_{j=1}^{l}v_{i_{j},f}^{(l)} \right)</script><h2 id="Problem-Solving"><a href="#Problem-Solving" class="headerlink" title="Problem Solving"></a>Problem Solving</h2><p> 最小化优化目标函数：</p><script type="math/tex; mode=display">L = \sum_{i=1}^{N} loss(\hat y(x^{(i)}),y^{(i)})</script><p> 回归问题，损失函数可取为最小平方误差，即：</p><script type="math/tex; mode=display">l^{LS}(y_1,y_2) := (y_1-y_2)^2</script><p> 二分类问题，损失函数可为 hinge loss 函数或 logistic loss 函数：</p><script type="math/tex; mode=display">l^{C}(y_1,y_2) := -ln\sigma (y_1y_2)</script><script type="math/tex; mode=display">\sigma (x) = \frac{1}{1+e^{-x}}</script><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><ul><li><p><strong> 随机梯度下降法 (StochasticGradient Descent, SGD)</strong></p></li><li><p><strong> 交替最小二乘法 (AlternatingLeast-Squares, ALS)</strong></p></li><li><p><strong> 马尔可夫链蒙特卡洛法 (MarkovChain Monte Carlo, MCMC)</strong></p></li></ul><h2 id="Multilinearity"><a href="#Multilinearity" class="headerlink" title="Multilinearity"></a>Multilinearity</h2><p>FM 的一个重要性质—<strong>Multilinearity</strong>，对于 FM 的任意参数 $\theta$ ，存在两个与 $\theta $ 的取值无关的函数 $g(\theta)$ 和 $h(\theta)$ 使得：</p><script type="math/tex; mode=display">\hat y(x) = g_{\theta}(x) + \theta h_{\theta}(x) \qquad \forall\theta \in \Theta</script><p> 其中：</p><script type="math/tex; mode=display">h_{\theta}(x) = \frac{\partial \hat y(x)}{\partial \theta} = \begin{cases}1,  & \text{if $\theta$ is $w_{0}$} \\x_{l}, & \text{if $\theta$ is $w_{l}$}  \\x_{l}\sum_{j \neq l}v_{j,f}x_{f}, & \text{if $\theta$ is $v_{l,f}$}\end{cases}</script><p>$g(\theta)$ 相对复杂，计算时，使用 <script type="math/tex">g_{\theta}(x) = \hat y(x) - \theta h_{\theta}(x)</script> 代替。</p><h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><p> 最小化损失函数的和：</p><script type="math/tex; mode=display">OPT(S) := argmin \sum_{(x, y) \in S} l(\hat y(x \mid \Theta), y)</script><p> 加入 L2 正则：</p><script type="math/tex; mode=display">OPTREG(S,\lambda) := argmin\left(\sum_{(x, y)\in S}l(\hat y(x \mid \Theta), y) + \sum_{\theta \in \Theta}\lambda_{\theta}\theta^2\right)</script><h2 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h2><p>  ​</p>]]></content>
    
    <summary type="html">
    
      本文介绍了 Factorization Machines 隐因子分解机模型，包括模型数学知识介绍以及与其他模型的对比。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>♛「Machine Learning」 About Dropout</title>
    <link href="http://randolph.pro/2017/03/01/%E2%99%9B%E3%80%8CMachine%20Learning%E3%80%8DAbout%20Dropout/"/>
    <id>http://randolph.pro/2017/03/01/♛「Machine Learning」About Dropout/</id>
    <published>2017-02-28T16:00:00.000Z</published>
    <updated>2019-03-17T15:14:41.926Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm5.staticflickr.com/4373/35507009504_3298ce3029_o.jpg" alt></p><p>有关「Machine Learning」的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/">「Machine Learning」</a></p><h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>开篇明义，dropout 是指在深度学习网络的训练过程中，对于神经网络单元，按照 <strong> 一定的概率 </strong> 将其 <strong> 暂时 </strong> 从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个 mini-batch 都在训练不同的网络。</p><p>Dropout 是深度学习中防止过拟合提高效果的一个大杀器，但对于其为何有效，却众说纷纭。在下读到两篇代表性的论文，代表两种不同的观点，特此分享给大家。</p><h2 id="组合派"><a href="# 组合派" class="headerlink" title="组合派"></a>组合派 </h2><p> 参考文献中第一篇中的观点，Hinton 在 2014 年提出的。</p><h3 id="观点"><a href="# 观点" class="headerlink" title="观点"></a>观点 </h3><p> 该论文从神经网络的难题出发，一步一步引出 dropout 为何有效的解释。大规模的神经网络有两个缺点：</p><ul><li>费时</li><li>容易过拟合</li></ul><p>这两个缺点是深度学习上的两大瓶颈，过拟合是很多机器学习的通病，过拟合了，得到的模型基本就废了。而为了解决过拟合问题，一般会采用 ensemble 方法，即训练多个模型做组合，此时，费时就成为一个大问题，不仅训练起来费时，测试起来多个模型也很费时。总之，几乎形成了一个死锁。</p><p>Dropout 的出现很好的可以解决这个问题，每次做完 dropout，相当于从原始的网络中找到一个更瘦的网络，如下图所示：</p><p><img src="https://farm4.staticflickr.com/3878/33037649431_ab442383e2_o.png" alt></p><p>因而，对于一个有 $N$ 个节点的神经网络，有了 dropout 后，就可以看做是 $2^n$ 个模型的集合了，但此时要训练的参数数目却是不变的，这就解脱了费时的问题。</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>虽然直观上看 dropout 是 ensemble 在分类性能上的一个近似，然而实际中，dropout 毕竟还是在一个神经网络上进行的，只训练出了一套模型参数。那么他到底是因何而有效呢？这就要从动机上进行分析了。论文中作者对 dropout 的动机做了一个十分精彩的类比：</p><blockquote><p>A motivation for dropout comes from a theory of the role of sex in evolution (Livnat et al., 2010). Sexual reproduction involves taking half the genes of one parent and half of the other, adding a very small amount of random mutation, and combining them to produce an offspring. The asexual alternative is to create an offspring with a slightly mutated copy of the parent’s genes. It seems plausible that asexual reproduction should be a better way to optimize individual fitness because a good set of genes that have come to work well together can be passed on directly to the offspring. On the other hand, sexual reproduction is likely to break up these co-adapted sets of genes, especially if these sets are large and, intuitively, this should decrease the fitness of organisms that have already evolved complicated co- adaptations. However, sexual reproduction is the way most advanced organisms evolved.</p><p><strong>One possible explanation for the superiority of sexual reproduction is that, over the long term, the criterion for natural selection may not be individual fitness but rather mix-ability of genes.</strong> The ability of a set of genes to be able to work well with another random set of genes makes them more robust. Since a gene cannot rely on a large set of partners to be present at all times, it must learn to do something useful on its own or in collaboration with a small number of other genes. <strong>According to this theory, the role of sexual reproduction is not just to allow useful new genes to spread throughout the population, but also to facilitate this process by reducing complex co-adaptations that would reduce the chance of a new gene improving the fitness of an individual.</strong> Similarly, each hidden unit in a neural network trained with dropout must learn to work with a randomly chosen sample of other units. This should make each hidden unit more robust and drive it towards creating useful features on its own without relying on other hidden units to correct its mistakes. However, the hidden units within a layer will still learn to do different things from each other. One might imagine that the net would become robust against dropout by making many copies of each hidden unit, but this is a poor solution for exactly the same reason as replica codes are a poor way to deal with a noisy channel.</p><p>A closely related, but slightly different motivation for dropout comes from thinking about successful conspiracies. Ten conspiracies each involving five people is probably a better way to create havoc than one big conspiracy that requires fifty people to all play their parts correctly. If conditions do not change and there is plenty of time for rehearsal, a big conspiracy can work well, but with non-stationary conditions, the smaller the conspiracy the greater its chance of still working. Complex co-adaptations can be trained to work well on a training set, but on novel test data they are far more likely to fail than multiple simpler co-adaptations that achieve the same thing.</p></blockquote><p>大概的意思，讲的是：</p><p>在自然界中，在中大型动物中，一般是有性繁殖，有性繁殖是指后代的基因从父母两方各继承一半。但是从直观上看，似乎无性繁殖更加合理，因为无性繁殖可以保留大段大段的优秀基因。而有性繁殖则将基因随机拆了又拆，破坏了大段基因的联合适应性。</p><p>但是自然选择中毕竟没有选择无性繁殖，而选择了有性繁殖，须知物竞天择，适者生存。我们先做一个假设，那就是基因的力量在于混合的能力而非单个基因的能力。不管是有性繁殖还是无性繁殖都得遵循这个假设。为了证明有性繁殖的强大，我们先看一个概率学小知识。</p><p>比如要搞一次恐怖袭击，两种方式： </p><ul><li>集中 50 人，让这 50 个人准确分工，搞一次大爆破。 </li><li>将 50 人分成 10 组，每组 5 人，分头行事，去随便什么地方搞点动作，成功一次就算。</li></ul><p>哪一个成功的概率比较大？ 显然是后者。</p><p>那么，类比过来，有性繁殖的方式不仅仅可以将优秀的基因传下来，还可以降低基因之间的联合适应性，使得复杂的大段大段基因联合适应性变成比较小的一个一个小段基因的联合适应性。</p><p>Dropout 也能达到同样的效果，它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。</p><p>个人补充一点：那就是植物和微生物大多采用无性繁殖，因为他们的生存环境的变化很小，因而不需要太强的适应新环境的能力，所以保留大段大段优秀的基因适应当前环境就足够了。而高等动物却不一样，要准备随时适应新的环境，因而将基因之间的联合适应性变成一个一个小的，更能提高生存的概率。</p><h3 id="Dropout 带来的模型的变化"><a href="#Dropout 带来的模型的变化" class="headerlink" title="Dropout 带来的模型的变化"></a>Dropout 带来的模型的变化 </h3><p> 而为了达到 ensemble 的特性，有了 dropout 后，神经网络的训练和预测就会发生一些变化。</p><ul><li><p>训练层面</p><p>无可避免的，训练网络的每个单元要添加一道概率流程。 </p><p><img src="https://farm3.staticflickr.com/2588/32782576600_aefb2c6586_o.png" alt></p><p>对应的公式变化如下如下：</p><ul><li><p>没有 dropout 的神经网络 </p><script type="math/tex; mode=display">\begin{align}z_{i}^{(l+1)} & = w_{i}^{(l+1)}y^{l} + b_{i}^{(l+1)} \cry_{i}^{(l+1)} & = f(z_{i}^{(l+1)})\end{align}</script></li><li><p>有 dropout 的神经网络 </p><script type="math/tex; mode=display">\begin{align}r_{i}^{(l)} & \sim Bernoulli(p) \cr\tilde{y}^{(l)} & = r^{(l)}*y^{(l)}\crz_{i}^{(l+1)} & = w_{i}^{(l+1)} \tilde{y}^{l} + b_{i}^{(l+1)} \cry_{i}^{(l+1)} & = f(z_{i}^{(l+1)})\end{align}</script></li></ul></li><li><p>测试层面</p><p>预测的时候，每一个单元的参数要预乘以 $p$。</p><p><img src="https://farm4.staticflickr.com/3817/33123833046_48bf6f3dba_o.png" alt> </p></li></ul><h3 id="论文中的其他技术点"><a href="# 论文中的其他技术点" class="headerlink" title="论文中的其他技术点"></a>论文中的其他技术点</h3><ul><li><p>防止过拟合的方法：</p><ul><li>提前终止（当验证集上的效果变差的时候）</li><li>L1 和 L2 正则化加权</li><li>Soft Weight Sharing</li><li>Dropout</li></ul></li><li><p>Dropout 率的选择</p><ul><li>经过交叉验证，隐含节点 dropout 率等于 0.5 的时候效果最好，原因是 0.5 的时候 dropout 随机生成的网络结构最多。</li><li>Dropout 也可以被用作一种添加噪声的方法，直接对 input 进行操作。输入层设为更接近 1 的数。使得输入变化不会太大（0.8）。</li></ul></li><li><p>训练过程 </p><ul><li>对参数 $w$ 的训练进行球形限制（max-normalization），对 dropout 的训练非常有用。</li><li>球形半径 $c$ 是一个需要调整的参数。可以使用验证集进行参数调优。</li><li>Dropout 单独使用效果不错，但是 <strong>dropout</strong>、<strong>max-normalization</strong>、<strong>large decaying learning rates</strong> 以及 <strong>high momentum</strong> 组合起来效果更好，比如 max-norm regularization 就可以防止大的 learning rate 导致的参数 blow up。</li><li>使用 pretraining 方法也可以帮助 dropout 训练参数，在使用 dropout 时，要将所有参数都乘以 $\frac{1}{p}$。</li></ul></li><li><p>部分实验结论</p><p>该论文的实验部分很丰富，有大量的评测数据。</p><ul><li><p>maxout 神经网络中得另一种方法，<code>Cifar-10</code> 数据集上超越 dropout</p></li><li><p>文本分类上，dropout 效果提升有限，分析原因可能是 <code>Reuters-RCV1</code> 数据集中数据量足够大，过拟合并不是模型的主要问题</p></li><li><p>dropout 与其他 standerd regularizers 的对比 </p></li><li><ul><li>L2 weight decay</li><li>lasso</li><li>KL-sparsity</li><li>max-norm regularization</li><li>dropout</li></ul></li><li><p>特征学习 </p><ul><li>标准神经网络，节点之间的相关性使得他们可以合作去 fix 其他节点中得噪声，但这些合作并不能在 unseen data 上泛化，于是，过拟合，dropout 破坏了这种相关性。在 autoencoder 上，有 dropout 的算法更能学习有意义的特征（不过只能从直观上，不能量化）。</li><li>产生的向量具有稀疏性。</li><li>保持隐含节点数目不变，dropout 率变化；保持激活的隐节点数目不变，隐节点数目变化。</li></ul></li><li><p>数据量小的时候，dropout 效果不好，数据量大了，dropout 效果好</p></li><li><p>模型均值预测</p><ul><li>使用 weight-scaling 来做预测的均值化</li><li>使用 mente-carlo 方法来做预测。即对每个样本根据 dropout 率先 sample 出来 $k$ 个 net，然后做预测，$k$ 越大，效果越好。</li></ul></li><li><p>Multiplicative Gaussian Noise </p><p>使用高斯分布的 dropout 而不是伯努利模型 dropout</p></li><li><p>dropout 的缺点就在于训练时间是没有 dropout 网络的 2-3 倍。</p></li></ul></li></ul><p>&gt;</p><blockquote><p>进一步需要了解的知识点</p><ul><li>dropout RBM</li><li>Marginalizing Dropout </li><li>具体来说就是将随机化的 dropout 变为确定性的，比如对于 Logistic 回归，其 dropout 相当于加了一个正则化项</li><li>Bayesian neural network 对稀疏数据特别有用，比如 medical diagnosis, genetics, drug discovery and other computational biology applications</li></ul></blockquote><h2 id="噪声派"><a href="# 噪声派" class="headerlink" title="噪声派"></a>噪声派 </h2><p> 参考文献中第二篇论文中得观点，也很强有力。</p><h3 id="观点 -1"><a href="# 观点 -1" class="headerlink" title="观点"></a>观点 </h3><p> 观点十分明确，就是对于每一个 dropout 后的网络，进行训练时，相当于做了 Data Augmentation，因为，总可以找到一个样本，使得在原始的网络上也能达到 dropout 单元后的效果。 比如，对于某一层，dropout 一些单元后，形成的结果是 <code>(1.5,0,2.5,0,1,2,0)</code>，其中 0 是被 drop 的单元，那么总能找到一个样本，使得结果也是如此。这样，每一次 dropout 其实都相当于增加了样本。</p><h3 id="稀疏性"><a href="# 稀疏性" class="headerlink" title="稀疏性"></a>稀疏性 </h3><h4 id="知识点 -A"><a href="# 知识点 -A" class="headerlink" title="知识点 A"></a> 知识点 A</h4><p>首先，先了解一个知识点：</p><blockquote><p>When the data points belonging to a particular class are distributed along a linear manifold, or sub-space, of the input space, it is enough to learn a single set of features which can span the entire manifold. But when the data is distributed along a highly non-linear and discontinuous manifold, the best way to represent such a distribution is to learn features which can explicitly represent small local regions of the input space, effectively “tiling” the space to define non-linear decision boundaries.</p></blockquote><p>大致含义就是： </p><p>在线性空间中，学习一个整个空间的特征集合是足够的，但是当数据分布在非线性不连续的空间中得时候，则学习局部空间的特征集合会比较好。</p><h4 id="知识点 -B"><a href="# 知识点 -B" class="headerlink" title="知识点 B"></a>知识点 B</h4><p>假设有一堆数据，这些数据由 $M$ 个不同的非连续性簇表示，给定 $K$ 个数据。那么一个有效的特征表示是将输入的每个簇映射为特征以后，簇之间的重叠度最低。使用 $A$ 来表示每个簇的特征表示中激活的维度集合。重叠度是指两个不同的簇的 $A<em>{i}$ 和 $A</em>{j}$ 之间的 Jaccard 相似度最小，那么：</p><ul><li>当 $K$ 足够大时，即便 $A$ 也很大，也可以学习到最小的重叠度</li><li>当 $K$ 小，$M$ 大时，学习到最小的重叠度的方法就是减小 $A$ 的大小，也就是稀疏性</li></ul><p>上述的解释可能是有点太专业化，比较拗口。主旨意思是这样，我们要把不同的类别区分出来，就要是学习到的特征区分度比较大，在数据量足够的情况下不会发生过拟合的行为，不用担心。但当数据量小的时候，可以通过稀疏性，来增加特征的区分度。</p><blockquote><p>因而有意思的假设来了，使用了 dropout 后，相当于得到更多的局部簇，同等的数据下，簇变多了，因而为了使区分性变大，就使得稀疏性变大。</p></blockquote><p>为了验证这个数据，论文还做了一个实验，如下图：</p><p><img src="https://farm3.staticflickr.com/2517/33185584305_9d146f80be_o.png" alt></p><p>该实验使用了一个模拟数据，即在一个圆上，有 15000 个点，将这个圆分为若干个弧，在一个弧上的属于同一个类，一共 10 个类，即不同的弧也可能属于同一个类。改变弧的大小，就可以使属于同一类的弧变多。</p><p>实验结论就是当弧长变大时，簇数目变少，稀疏度变低。与假设相符合。</p><p>个人观点：该假设不仅仅解释了 dropout 何以导致稀疏性，还解释了 dropout 因为使局部簇的更加显露出来，而根据知识点 A 可得，使局部簇显露出来是 dropout 能防止过拟合的原因，而稀疏性只是其外在表现。</p><h3 id="论文中的其他技术知识点"><a href="# 论文中的其他技术知识点" class="headerlink" title="论文中的其他技术知识点"></a>论文中的其他技术知识点</h3><ul><li><p>将 dropout 映射回得样本训练一个完整的网络，可以达到 dropout 的效果。</p></li><li><p>Dropout 由固定值变为一个区间，可以提高效果。</p></li><li><p>将 dropout 后的表示映射回输入空间时，并不能找到一个样本 $x^{*}$ 使得所有层都能满足 dropout 的结果，但可以为每一层都找到一个样本，这样，对于每一个 dropout，都可以找到一组样本可以模拟结果。</p></li><li><p>Dropout 对应的还有一个 dropConnect，公式如下：</p><ul><li><p>dropout</p><script type="math/tex; mode=display">h_{n} = \overrightarrow{w_{n}}^{T}(\overrightarrow{r} \odot \overrightarrow{x}) + b_{n}</script></li><li><p>dropConnect</p><script type="math/tex; mode=display">h_{n} = (\overrightarrow{r_{n}} \odot \overrightarrow{w_{n}})^{T}\overrightarrow{x} + b_{n}</script></li></ul></li></ul><ul><li>试验中，纯二值化的特征的效果也非常好，说明了稀疏表示在进行空间分区的假设是成立的，一个特征是否被激活表示该样本是否在一个子空间中。</li></ul><h1 id="参考文献"><a href="# 参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]. Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: A simple way to prevent neural networks from overfitting[J]. The Journal of Machine Learning Research, 2014, 15(1): 1929-1958.</p><p>[2]. Dropout as data augmentation. <a href="http://arxiv.org/abs/1506.08700" target="_blank" rel="noopener">http://arxiv.org/abs/1506.08700</a></p>]]></content>
    
    <summary type="html">
    
      Dropout 是深度学习中防止过拟合提高效果的一个大杀器，但对于其为何有效，却众说纷纭。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>♞「Books」Machine Learning -  KNN &amp; kd Tree</title>
    <link href="http://randolph.pro/2017/01/14/%E2%99%9E%E3%80%8CBooks%E3%80%8DMachine%20Learning%20-%20KNN%20&amp;%20kd%20Tree/"/>
    <id>http://randolph.pro/2017/01/14/♞「Books」Machine Learning - KNN &amp; kd Tree/</id>
    <published>2017-01-13T16:00:00.000Z</published>
    <updated>2019-03-18T08:13:27.118Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm5.staticflickr.com/4307/36174772306_62cfbc8cd6_o.jpg" alt></p><p>有关该书的其他学习笔记系列：<a href="http://randolph.pro/categories/Books/Book-「Machine-Learning」/">Book:「Machine Learning」</a></p><h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><ul><li>kNN</li><li>线性扫描实现 kNN 算法</li><li>kd 树实现 kNN 算法 </li></ul><hr><h1 id="kNN"><a href="#kNN" class="headerlink" title="kNN"></a>kNN</h1><p>$k$- 邻近算法 (kNN)，是一种基本<strong> 分类与回归 </strong> 方法。</p><p>此篇主要讲 $k$- 邻近算法在分类问题中的应用。</p><p>它的工作原理是： 存在一个样本数据合集 $S$，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较（<strong>距离度量 </strong>），然后算法提取样本集中 $k$ 个特征最相似数据（最近邻）的分类标签（<strong>$k$ 值的选择</strong>）。最后，选择 $k$ 个最相似数据中出现次数最多的分类（<strong> 分类决策规则</strong>），作为新数据的分类。</p><p>$k$- 邻近模型由三个基本要素—- 距离度量、$k$ 值的选择和分类决策规定决定。</p><h1 id="距离度量"><a href="# 距离度量" class="headerlink" title="距离度量"></a>距离度量 </h1><p> 一般使用的是欧氏距离，我们也可以根据自己的需求选择其他距离，比如更一般的 $L_p$ 距离，也称为<strong>Minkowski 距离</strong>。</p><p>这里 $L_p$ 距离中，$p \geq 1$，当 $p=1$ 时，称为曼哈顿距离；当 $p=2$ 时，称为欧式距离；当 $p=\infty$ 时，它是各个坐标距离的最大值，即切比雪夫距离。</p><p>设特征空间 $X$ 是 $n$ 维实数向量空间 $R^n$，$x_i$，$x_j$ $\in$ $X$，$x_i = (x_i^{(1)}，x_i^{(2)}，…，x_i^{(n)})^T$，$x_j = (x_j^{(1)}，x_j^{(2)}，…，x_j^{(n)})^T$，$x_i$，$x_j$ 的 $L_p$ 距离（Minkowski 距离）定义为：</p><script type="math/tex; mode=display">L_p(x_i, x_j) = (\sum{l=1}^{n} \left | x{i}^{(l)}-x_{j}^{(l)} \right | ^p)^{1 \over p}</script><p><img src="https://farm1.staticflickr.com/417/31490650822_a861eca015_o.png" alt></p><p>但是，Minkowski 类型的距离函数存在的一个问题是，它们假定数据分布本质上应当具有对称性，即距离在所有方向上都是相同的。然而很多时候，数据并不符合球状分布，因此不宜采用像 Minkowski 距离这样的对称距离。例如：</p><p><img src="https://farm1.staticflickr.com/395/31490660172_23aaf2b39a_o.png" alt></p><p>对于这种情况，像图那样所示那样围绕数据画出一个标准圆是不可取的，我们应当对数据分布呈椭圆形这个特点予以考虑：</p><p><img src="https://farm1.staticflickr.com/289/31521619731_530a2ab0bd_o.png" alt></p><p>因此，我们需要选择一个能够更好地体现数据分布特点的距离度量方法—- 即 Mahalanobis 距离。Mahalanobis 距离函数会考虑数据在每个维度上的波动性，因此对于数据的每个维度，都存在一个 $s_i$，表示该数据集在此维度上的标准差的变量。Mahalanobis 距离的计算公式如下：</p><script type="math/tex; mode=display">d(x,y) =\sqrt{\sum_{i=1}^{n}\frac{(x_{i} - y_{i})^2}{s_{i}^2}}</script><p>可以看出，该公式与欧式距离非常类似，但是不同的是它考虑到各种特性之间的联系（例如，一条关于身高的信息会带来一条关于体重的信息，因为两者是有关联的，并且是尺度无关的），即独立于测量尺度。</p><p>欧式距离就好比一个参照值，它表征的是当所有 <strong> 类别等概率 </strong> 出现的情况下，类别之间的距离。此时决策面中心点的位置就是两个类别中心的连线的中点。如下所示：</p><p><img src="https://farm1.staticflickr.com/458/30795845494_a42900cc19_o.png" alt></p><p>而当 <strong> 类别先验概率并不相等 </strong> 时，显然，如果仍然用中垂线作为决策线是不合理的，将出现判别错误（绿色类的点被判别为红色类），假设上图中绿色类别的先验概率变大，那么决策线将左移，如下图黄线。左移的具体位置，就是通过马氏距离来获得的。马氏距离中引入的协方差参数，表征的是点的稀密程度。</p><p><img src="https://farm1.staticflickr.com/458/30795845494_a42900cc19_o.png" alt></p><p><strong>从哲学上来说，用马氏距离处理数据时，不再把数据单纯的看作是冷冰冰的数字——那个引入的协方差，承认了客观上的差异性，就好像是有了人类的感情倾向，使得模式识别更加“人性化”也更加“视觉直观”。</strong></p><p>Mahalanobis 距离是基于样本分布的一种距离。<strong>物理意义就是在规范化的主成分空间中的欧氏距离。</strong>所谓规范化的主成分空间就是利用主成分分析对一些数据进行主成分分解。再对所有主成分分解轴做归一化，形成新的坐标轴。由这些坐标轴张成的空间就是规范化的主成分空间。</p><p>换句话说，主成分分析就是把椭球分布的样本改变到另一个空间里，使其成为球状分布。而 Mahalanobis 距离就是在样本呈球状分布的空间里面所求得的欧式距离。</p><p>当然，上面的解释只是对椭球分布而言，<strong>对一般分布，只能消除分布的二阶相关性，而不能消除高阶相关性</strong>。</p><hr><h1 id="k- 值的选择"><a href="#k- 值的选择" class="headerlink" title="$k$ 值的选择"></a>$k$ 值的选择</h1><ul><li>如果选择 $k$ 比较小，Bias 会比较低，但是 Variance 会比较高，$k$ 的减小就意味着整体模型变得复杂，容易发生过拟合 overfitting。</li><li>如果选择 $k$ 比较大，Variance 会比较低，但是 Bias 会比较高，$k$ 的增大就意味着整体模型变得简单，容易发生欠拟合 underfitting。<ul><li>通常来说 $k$ 是一个不大于 20 的整数。</li><li>为了确定 $k$ 值，主要有三种方案可供选择：<ol><li><strong>猜测</strong></li><li><strong>使用启发式策略</strong><ul><li>当分类问题中只涉及两个类别时，不要将 $k$ 值取为偶数。<ul><li>使 $k$ 与类别总数互质。将 $k$ 取为与类别总数互质的数，可保证投票数并列的情况较少出现。</li></ul></li><li>$k$ 的值应不小于类别总数加一。（使得所有的类别均有被表示的机会）</li><li>为避免出现噪音，$k$ 的值应足够小。</li></ul></li><li><strong>通过算法优化</strong><ul><li>使用 <strong> 遗传算法 </strong> 或者 <strong> 暴力网络搜索算法</strong>。</li><li>基于一个任意的 $k$ 试图将误差最小化称为爬山问题（Hill Climbing Problem）。其主要思想是对一组可能的 $k$ 值轮流进行考察，直至找到一个可接受的误差。利用遗传算法或者暴力网络搜索这样的算法来寻求 $k$ 的最优值的难点在于，当 $k$ 增大时，分类的复杂性也相应增加，从而降低性能。换言之，当增加 $k$ 时，程序的速度会逐渐变慢。<blockquote><p>If you want to learn more about genetic algorithms applied to find‐ ing an optimal K, you can read more about it in <strong>Florian Nigsch et al.’s Journal of Chemical Information and Modeling article, “Melting Point Prediction Employing k-Nearest Neighbor Algorithms and Genetic Parameter Optimization”</strong>.</p></blockquote></li></ul></li></ol></li></ul></li></ul><hr><h1 id="分类决策规则"><a href="# 分类决策规则" class="headerlink" title="分类决策规则"></a>分类决策规则</h1><p>$k$- 邻近法中的分类决策规则往往是“投票法”（多数表决），即由输入实例的 $k$ 个邻近的训练实例中的多数类决定输入实例的类。</p><p>因为算法思想简单，我们可以用很多方法实现它，这时效率就是我们需要慎重考虑的事情，最简单的自然是求出测试样本和训练集所有点的距离然后排序选择前 $k$ 个，这个是 $O(n(\log n))$ 的，而其实从 $N$ 个数据找前 $k$ 个数据是一个很常见的算法题，可以用最大堆（最小堆）实现，其效率是 $O(n(\log k))$ 的，而最广泛的算法是使用 $kd$ 树来减少扫描的点。</p><p>目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。</p><p><strong>KNN 算法不仅可以用于分类，还可以用于回归。</strong>通过找出一个样本的 $k$ 个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值（weight），如权值与距离成正比。</p><p>可以通过数学证明 <strong> 当数据规模趋于无穷时，最近邻分类器（$k=1$）的泛化错误率不超过贝叶斯最优分类器的错误率的两倍。</strong></p><blockquote><p>For every point in our dataset:    </p><ol><li>calculate the distance between inX and the current point.</li><li>sort the distances in increasing order.</li><li>take k items with lowest distances to inX.</li><li>find the majority class among these items.</li><li>return the majority class as our prediction for the class of inX.</li></ol></blockquote><p>对于未知类别属性的数据集中的每一个点一次执行以下操作：</p><ol><li>计算已知类别数据集中的点与当前点之间的距离。</li><li>按照距离递增次序排序。</li><li>选取与当前距离最小的 $k$ 个点。</li><li>确定前 $k$ 个点所在类别的出现频率。</li><li>返回前 $k$ 个点出现频率最高的类别作为当前点的预测分类。</li></ol><hr><h1 id="kNN 算法（线性扫描实现方法）"><a href="#kNN 算法（线性扫描实现方法）" class="headerlink" title="kNN 算法（线性扫描实现方法）"></a>kNN 算法（线性扫描实现方法）</h1><p>核心代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX, dataSet, labels, k)</span>:</span></span><br><span class="line">    dataSetSize = dataSet.shape[<span class="number">0</span>]</span><br><span class="line">    diffMat = tile(inX, (dataSetSize, <span class="number">1</span>)) - dataSet</span><br><span class="line">    sqDiffMat = diffMat ** <span class="number">2</span></span><br><span class="line">    sqDistances = sqDiffMat.sum(axis = <span class="number">1</span>)</span><br><span class="line">    distances = sqDistances ** <span class="number">0.5</span></span><br><span class="line">    sortedDistIndicies = distances.argsort()</span><br><span class="line">    classCount = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        voteIlabel = labels[sortedDistIndicies[i]]</span><br><span class="line">        classCount[voteIlabel] = classCount.get(voteIlabel,<span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">    sortedClassCount = sorted(classCount.items(),key = operator.itemgetter(<span class="number">1</span>),reverse = <span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>分析一下代码：</p><p><strong><code>tile()</code></strong>是 Numpy 中的一个 module。它的用法是：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">&gt;&gt;&gt; np.tile(a, <span class="number">2</span>)</span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">&gt;&gt;&gt; np.tile(a, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line">&gt;&gt;&gt; np.tile(a, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">array([[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]],</span><br><span class="line">       [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]]])</span><br></pre></td></tr></table></figure><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; b = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">&gt;&gt;&gt; np.tile(b, <span class="number">2</span>)</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">&gt;&gt;&gt; np.tile(b, (<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; c = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">&gt;&gt;&gt; np.tile(c,(<span class="number">4</span>,<span class="number">1</span>))</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure><p>总结一下：<strong><code>tile(A, reps)</code></strong>就是将数组 A 重复 reps 次。</p><ul><li>A 的类型可以是 <strong><code>array, list, tuple, dict, matrix</code></strong> 以及<strong><code>int, string, float, bool</code></strong>。</li><li>reps 的类型可以是 <strong><code>tuple, list, dict, int, bool</code></strong> 但不可以是<strong><code>float, string, matrix</code></strong>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">diffMat = tile(inX, (dataSetSize, <span class="number">1</span>)) - dataSet</span><br></pre></td></tr></table></figure><p><strong><code>inX</code></strong>是我们的未知类别集中的一个点的输入向量（形式为 <script type="math/tex">[x_i,y_i]</script>），也就是我们需要进行分类的一个点。<br><strong><code>dataSetSize</code></strong> 是我们已知类别数据集的大小。</p><p>这句代码的目的便是生成一个这样的矩阵<strong><code>matrix</code></strong>:</p><script type="math/tex; mode=display">[[\Delta x_1, \Delta y_1],[\Delta x_2, \Delta y_2],[\Delta x_3, \Delta y_3],...,[\Delta x_{datasize}, \Delta y_{datasize}]]</script><p>很明显，矩阵中存储的就是，未知点与已知类别数据集中的所有点的 $x$ 与 $y$ 坐标差值。</p><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqDistances = sqDiffMat.sum(axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>这句代码就是进一步生成的这样的矩阵，用作下一步计算点与点之间距离用：</p><script type="math/tex; mode=display">[[\Delta x_1^2 + \Delta y_1^2],[\Delta x_2^2 + \Delta y_2^2],[\Delta x_3^2 + \Delta y_3^2],...,[\Delta x_{datasize}^2 + \Delta y_{datasize}^2]]</script><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sortedDistIndicies = distances.argsort()</span><br></pre></td></tr></table></figure><p>这句代码的意思便是返回一组索引值，这个索引值分别对应原数组中的点，但是是根据离未知点从最近到最远来排序，只用使用这个索引值就可以找到对应的已知类别数据中的一个点，从而得到该点标签。</p><p>举个例子：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = array([<span class="number">1</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>])</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">array([<span class="number">1</span>,  <span class="number">9</span>,  <span class="number">7</span>,  <span class="number">8</span>, <span class="number">10</span>,  <span class="number">2</span>,  <span class="number">0</span>,  <span class="number">3</span>])</span><br><span class="line">&gt;&gt;&gt; b = a.argsort()</span><br><span class="line">&gt;&gt;&gt; b</span><br><span class="line">array([<span class="number">6</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><p>此时我们已经使用 $k$- 邻近算法构造了一个分类器，这个分类器可以处理二分类别任务，也可以处理多类别任务（labels 添加多个标签）。对于分类任务而言，评估分类器好坏的标准，最常用的便是错误率与精度，当然还有其他的评判标准，这里就不展开赘述。接下来在现实具体任务中来使用我们的 $k$- 邻近算法分类器。</p><hr><h2 id="Improving-matches-from-a-dating-site-with-kNN"><a href="#Improving-matches-from-a-dating-site-with-kNN" class="headerlink" title="Improving matches from a dating site with kNN"></a>Improving matches from a dating site with kNN</h2><p>约会数据 datingTestSet.txt 中包括 3 种 <strong><code>feature</code></strong> 与 1 个<strong><code>label</code></strong>:</p><p>(每年获得的飞行常客里程数，玩视频游戏所耗时间百分比，每周消费的冰淇淋公升数，是否喜欢)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">409208.3269760.953952largeDoses</span><br><span class="line">144887.1534691.673904smallDoses</span><br><span class="line">260521.4418710.805124didntLike</span><br><span class="line">7513613.1473940.428964didntLike</span><br><span class="line">383441.6697880.134296didntLike</span><br><span class="line">7299310.1417401.032955didntLike</span><br><span class="line">359486.8307921.213192largeDoses</span><br><span class="line">4266613.2763690.543880largeDoses</span><br><span class="line">674978.6315770.749278didntLike</span><br><span class="line">3548312.2731691.508053largeDoses</span><br><span class="line">....</span><br></pre></td></tr></table></figure></p><p>为了使得这些约会数据能够变成参数，输入分类器中进行处理，我们需要将文件中的数据转换成分类器所需要的 <strong><code>matrix</code></strong> 矩阵样式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file2matrix</span><span class="params">(filename)</span>:</span></span><br><span class="line">    fr = open(filename)</span><br><span class="line">    arrayOLines = fr.readlines()</span><br><span class="line">    numberOfLines = len(arrayOLines)</span><br><span class="line">    returnMat = zeros((numberOfLines,<span class="number">3</span>))</span><br><span class="line">    classLabelVector = []</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> arrayOLines:</span><br><span class="line">        line = line.strip()</span><br><span class="line">        listFromLine = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        returnMat[index,:] = listFromLine[<span class="number">0</span>:<span class="number">3</span>]</span><br><span class="line">        classLabelVector.append(listFromLine[<span class="number">-1</span>])</span><br><span class="line">        index += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> returnMat,classLabelVector</span><br></pre></td></tr></table></figure><p>具体做法为：</p><ol><li>打开约会文件，得到文件的行数，即一共多少条数据。</li><li>创建一个大小合适的矩阵，并以零填充，这里简化设置矩阵的维度为 3，我们也可以按照自己的实际需求增加相应代码来适应变化的维度输入值。(这里维度为 3 的时候，<strong><code>returnMat=[[0,0,0],[0,0,0],...,[0,0,0]]</code></strong>)</li><li>循环处理每一条数据，<strong><code>line.strip()</code></strong>截取掉所有的回车字符，然后使用 tab 字符 \t 将上一步得到的整行数据分割成一个元素列表，存储到特征矩阵 <strong><code>returnMat</code></strong> 中。</li><li>原约会文件的每条数据的最后一项（是否喜欢），单独存储到标签向量<strong><code>classLabelVector</code></strong>，作为类别。</li><li>返回得到特征矩阵 <strong><code>returnMat</code></strong> 与标签向量<strong><code>classLabelVector</code></strong>。</li></ol><p>可以试着使用一下这个函数，检查一下数据内容：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; datingDataMat,datingLabels = kNN.file2matrix(<span class="string">'datingTestSet.txt'</span>)</span><br><span class="line">&gt;&gt;&gt; datingDataMat</span><br><span class="line">array([[<span class="number">7.29170000e+04</span>,   <span class="number">7.10627300e+00</span>,   <span class="number">2.23600000e-01</span>],</span><br><span class="line">       [<span class="number">1.42830000e+04</span>,   <span class="number">2.44186700e+00</span>,   <span class="number">1.90838000e-01</span>],</span><br><span class="line">       [<span class="number">7.34750000e+04</span>,   <span class="number">8.31018900e+00</span>,   <span class="number">8.52795000e-01</span>],</span><br><span class="line">       ...,</span><br><span class="line">       [<span class="number">1.24290000e+04</span>,   <span class="number">4.43233100e+00</span>,   <span class="number">9.24649000e-01</span>],</span><br><span class="line">       [<span class="number">2.52880000e+04</span>,   <span class="number">1.31899030e+01</span>,   <span class="number">1.05013800e+00</span>],</span><br><span class="line">       [<span class="number">4.91800000e+03</span>,   <span class="number">3.01112400e+00</span>,   <span class="number">1.90663000e-01</span>]])</span><br><span class="line">&gt;&gt;&gt; datingLabels[<span class="number">0</span>:<span class="number">20</span>]</span><br><span class="line">[<span class="string">'didntLike'</span>, <span class="string">'smallDoses'</span>, <span class="string">'didntLike'</span>, <span class="string">'largeDoses'</span>, <span class="string">'smallDoses'</span>,</span><br><span class="line"><span class="string">'smallDoses'</span>, <span class="string">'didntLike'</span>, <span class="string">'smallDoses'</span>, <span class="string">'didntLike'</span>, <span class="string">'didntLike'</span>, <span class="string">'largeDoses'</span>, <span class="string">'largeDose s'</span>, <span class="string">'largeDoses'</span>, <span class="string">'didntLike'</span>, <span class="string">'didntLike'</span>, <span class="string">'smallDoses'</span>, <span class="string">'smallDoses'</span>, <span class="string">'didntLike'</span>, <span class="string">'smallDoses'</span>, <span class="string">'didntLike'</span>]</span><br></pre></td></tr></table></figure><p><strong><code>datingDataMat</code></strong>特征矩阵中飞行常客里程数远远大于其他特征值，但是我们认为三种特征是同等重要的，这种不同取值范围的特征值时，我们通常采用的方法将数值归一化，如将取值范围处理为 0 到 1 或者 -1 到 1 之间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">autoNorm</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    minVals = dataSet.min(<span class="number">0</span>)</span><br><span class="line">    maxVals = dataSet.max(<span class="number">0</span>)</span><br><span class="line">    ranges = maxVals - minVals</span><br><span class="line">    normDataSet = zeros(shape(dataSet))</span><br><span class="line">    m = dataSet.shape[<span class="number">0</span>]</span><br><span class="line">    normDataSet = dataSet - tile(minVals, (m, <span class="number">1</span>))</span><br><span class="line">    normDataSet = normDataSet/tile(ranges, (m, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> normDataSet, ranges, minVals</span><br></pre></td></tr></table></figure><p>这里需要说明的一点是，我们将每列的最小值放在变量 <strong><code>minVals</code></strong> 中，将最大值放在 <strong><code>maxVals</code></strong> 中，其中 <strong><code>dataSet.min(0)</code></strong> 中的参数 0 使得函数可以从列中选取最小值，而不是选取当前行中的最小值。</p><ul><li><strong><code>numpy.chararray.min</code></strong></li></ul><blockquote><p><strong><code>chararray.min(axis=None, out=None, keepdims=False)</code></strong></p><p>（Return the minimum along a given axis.）</p><p> <strong>axis = 1 对行进行操作； axis = 0 对列进行操作；</strong></p></blockquote><p>虽然改变数值取值范围增加了分类器的复杂度，但可以得到更为准确的结果。</p><p>接着我们需要做的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">datingClassTest</span><span class="params">()</span>:</span></span><br><span class="line">    hoRatio = <span class="number">0.10</span></span><br><span class="line">    datingDataMat, datingLabels = file2matrix(<span class="string">'datingTestSet2.txt'</span>)</span><br><span class="line">    normMat, ranges, minVals = autoNorm(datingDataMat)</span><br><span class="line">    m = normMat.shape[<span class="number">0</span>]</span><br><span class="line">    numTestVecs = int(m*hoRatio)</span><br><span class="line">    errorCount = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTestVecs):</span><br><span class="line">        classifierResult = classify0(normMat[i, :], normMat[numTestVecs:m, :], datingLabels[numTestVecs:m], <span class="number">3</span>)</span><br><span class="line">        print(<span class="string">"the classifier came back with: %d,the real answer is :%d"</span>%(classifierResult, datingLabels[i]))</span><br><span class="line">        <span class="keyword">if</span>(classifierResult != datingLabels[i]):errorCount += <span class="number">1.0</span></span><br><span class="line">    print(<span class="string">"the total error rate is :%f"</span> %(errorCount/float(numTestVecs)))</span><br></pre></td></tr></table></figure><p>其中 <strong><code>numTestVecs</code></strong> 是用于测试的数据集向量，我们可以改变 <strong><code>datingClassTest</code></strong> 内变量 <strong><code>hoRatio</code></strong> 和变量 k 的值，检测错误率是否随着变量值的变化而增加，也就是调参。</p><hr><h2 id="A-handwriting-recognition-system"><a href="#A-handwriting-recognition-system" class="headerlink" title="A handwriting recognition system"></a>A handwriting recognition system</h2><p>手写识别系统</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span><span class="params">(filename)</span>:</span></span><br><span class="line">    returnVect = zeros((<span class="number">1</span>, <span class="number">1024</span>))</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">        lineStr = fr.readline()</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">            returnVect[<span class="number">0</span>, <span class="number">32</span>*i+j] = int(lineStr[j])</span><br><span class="line">    <span class="keyword">return</span> returnVect</span><br></pre></td></tr></table></figure><p>该函数创建 $1 \times 1024$ 的 NumPy 数组，然后打开给定的文件，循环独读出文件的前 32 行，并将每行的头 32 个字符值存储在 NumPy 数组中，最后返回数组。（$32 \times 32=1024$）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handwritingClassTest</span><span class="params">()</span>:</span></span><br><span class="line">    hwLabels = []</span><br><span class="line">    trainingFileList = listdir(<span class="string">'trainingDigits'</span>)</span><br><span class="line">    m = len(trainingFileList)</span><br><span class="line">    trainingMat = zeros((m, <span class="number">1024</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        fileNameStr = trainingFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">        classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">        hwLabels.append(classNumStr)</span><br><span class="line">        trainingMat[i, :] = img2vector(<span class="string">'trainingDigits/%s'</span> %fileNameStr)</span><br><span class="line"></span><br><span class="line">    testFileList = listdir(<span class="string">'testDigits'</span>)</span><br><span class="line">    errorCount = <span class="number">0.0</span></span><br><span class="line">    mTest = len(testFileList)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(mTest):</span><br><span class="line">        fileNameStr = testFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">        classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">        vectorUnderTest = img2vector(<span class="string">'testDigits/%s'</span> %fileNameStr)</span><br><span class="line">        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, <span class="number">3</span>)</span><br><span class="line">        print(<span class="string">"the classifier came back with: %d, the real answer is: %d"</span> %(classifierResult, classNumStr))</span><br><span class="line">        <span class="keyword">if</span>(classifierResult != classNumStr):errorCount += <span class="number">1.0</span></span><br><span class="line">    print(<span class="string">"\n the total number of errors is: %d"</span> %errorCount)</span><br><span class="line">    print(<span class="string">"\n the total error rate is: %f"</span> % (errorCount/float(mTest)))</span><br></pre></td></tr></table></figure><p>步骤是：</p><ol><li>将 trainingDigits 目录中的文件内容存储在列表中 <strong><code>trainingFileList</code></strong>，然后可以得到目录中有多少文件，并将其存储在变量<strong><code>m</code></strong> 中。 </li><li>接着，创建一个 m 行 $1024$（$32 \times 32$）列的训练矩阵，该矩阵的每行数据都存储一个图像信息 <strong><code>trainingMat</code></strong>（用到前面定义到的<strong><code>img2vector()</code></strong> 函数）。</li><li>从文件名中解析出分类数字，将所有图像类别信息存储在 <strong><code>hwLabels</code></strong> 向量中。（例如 0_0.txt 的类别就是‘0’）</li><li>对剩下的 testDigits 目录中的测试数据进行上述 1，2，3 操作，但是不同的是，我们并不将测试数据的图像信息载入到新的矩阵当中，而是使用前面定义的 <strong><code>classify0()</code></strong> 函数测试该目录下的每个数据文件。</li></ol><p>说明：由于简化了图像信息，图像信息均是以 0 和 1 进行表示，所以不需要使用 <strong><code>autoNorm()</code></strong> 函数对数据进行规范化处理了。</p><h1 id="kNN 算法（-kd- 树实现方法）"><a href="#kNN 算法（-kd- 树实现方法）" class="headerlink" title="kNN 算法（$kd$ 树实现方法）"></a>kNN 算法（$kd$ 树实现方法）</h1><p>（Unfinished.）</p><h1 id="维度灾难"><a href="# 维度灾难" class="headerlink" title="维度灾难"></a>维度灾难 </h1><p> 在高维情形下出现的 <strong> 数据样本稀疏 </strong>、<strong> 距离计算困难 </strong> 等问题，是所有机器学习方法共同面临的严重障碍，被称为“<strong>维度灾难</strong>”（curse of dimensionality）。</p><p>自然会想到的一个解决办法便是 <strong> 降维</strong>（dimension reduction）。这是基于这样的一个事实：在很多时候，人们观测或收集到的数据样本虽然是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间中的一个低维“嵌入”（embedding）。</p><ul><li>若要求原始空间中样本之间的距离在低维空间中得以保持，我们可以采取“多维缩放”（Multiple Dimensional Scaling, MDS）这样一种经典的降维方法。</li></ul><p>那么假如我现在要用 kNN 邻近算法来实现分类问题，我会考虑：</p><ol><li>在训练数据集 $D$ 中寻找 $k$ 个最相似的点的扫描方法的优化，相比线性，<strong>kd 树</strong>（利用最大 / 小堆）是一个更好的选择，这对模型的最终结果无关，但是可以优化程序整体的性能。（试想，我们还可以进一步优化么？）</li><li>$k$ 的取值，也是至关决定性的，但一般不考虑超过 20 的整数，可以进行调参，根据结果选定对于当前问题最优的 $k$ 值。</li><li>被测点 $x$ 与测试数据集 $D$ 中的点的距离度量，一般采用欧式距离，标准化欧氏距离会更好么？另外，其他的距离度量方法呢？</li><li>选出来的 $k$ 个邻居与被测点 $x$ 的距离不同，应当权重也不同，即在分类决策规则中加入“权重”，对可能出现的不同类别，进行权重乘以距离累加，计算各类别得分，比较得分选择最终所属类别。</li><li>训练集与测试集的划分，应该会采用 <strong> 交叉验证法</strong>。</li></ol>]]></content>
    
    <summary type="html">
    
      本文是关于周志华「Machine Learning」这本书的 Classification - KNN &amp; kd Tree 的学习笔记。
    
    </summary>
    
      <category term="Books" scheme="http://randolph.pro/categories/Books/"/>
    
      <category term="Book:「Machine Learning」" scheme="http://randolph.pro/categories/Books/Book-%E3%80%8CMachine-Learning%E3%80%8D/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Books" scheme="http://randolph.pro/tags/Books/"/>
    
  </entry>
  
  <entry>
    <title>♞「Machine Learning」 Clustering</title>
    <link href="http://randolph.pro/2017/01/03/%E2%99%9E%E3%80%8CBooks%E3%80%8DMachine%20Learning%20-%20Clustering/"/>
    <id>http://randolph.pro/2017/01/03/♞「Books」Machine Learning - Clustering/</id>
    <published>2017-01-02T16:00:00.000Z</published>
    <updated>2019-03-18T08:13:43.360Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://farm5.staticflickr.com/4307/36174772306_62cfbc8cd6_o.jpg" alt></p><p>有关该书的其他学习笔记系列：<a href="http://randolph.pro/categories/Books/Book-「Machine-Learning」/">Book:「Machine Learning」</a></p><h1 id="聚类介绍"><a href="# 聚类介绍" class="headerlink" title="聚类介绍"></a>聚类介绍</h1><ul><li>聚类是从数据集中挖掘相似观测值集合的方法。</li><li>聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”（cluster）。通过这样的划分，每个簇可能对应于一些潜在的概念（类别）。</li><li>聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者自己来把握。</li><li>聚类既能作为一个单独的过程用于寻找数据内在的分布结构，也可以作为分类等其他学习任务的前驱过程。</li></ul><hr><h2 id="聚类算法"><a href="# 聚类算法" class="headerlink" title="聚类算法"></a>聚类算法 </h2><p><strong> 角度 I：</strong></p><ul><li><strong>基于原型的聚类（Prototype-based Clustering）</strong><ul><li><strong>K 均值聚类（K-means）</strong></li><li><strong>学习向量量化聚类（Learning Vector Quantization）</strong></li><li><strong>高斯混合模型聚类 （Gaussian Mixture Model）</strong></li></ul></li><li><strong>基于密度的聚类 （Density-based Clustering）</strong><ul><li><strong>DBSCAN （Density-Based Spatial Clustering of Application with Noise）</strong></li><li><strong>OPTICS （Ordering Points To Identify the Clustering Structure）</strong></li></ul></li><li><strong>层次聚类 （Hierarchical Clustering）</strong></li><li><strong>基于模型的聚类 （Model-based Clustering）</strong><ul><li><strong>混合回归模型 （Mixture Regression Model）</strong></li></ul></li></ul><p><strong>角度 II：</strong></p><ul><li><strong>基于中心的聚类： kmeans 聚类</strong></li><li><strong>基于分布的聚类： GMM 聚类</strong></li><li><strong>基于密度的聚类： DBSCAN, OPTICS</strong></li><li><strong>基于连通性的聚类： 层次聚类</strong></li><li><strong>基于模型的聚类： Miture Regression Model</strong></li><li><strong>其他聚类方法： 谱聚类, Chameleon, Canopy…</strong></li></ul><hr><h2 id="聚类数据设置"><a href="# 聚类数据设置" class="headerlink" title="聚类数据设置"></a>聚类数据设置 </h2><p> 假定样本集 $ D $ 包含 $n$ 个无标记样本：</p><script type="math/tex; mode=display">D = \{x_1, x_2, \ldots, x_n \}</script><p>每个样本是一个 $p$ 维特征向量：</p><script type="math/tex; mode=display">x_i=(x_{i1}; x_{i2}; \ldots; x_{ip})</script><p>聚类算法将样本集 $D$ 划分为 $k$ 个不相交的簇：</p><script type="math/tex; mode=display">\{C_l|l=1, 2, \ldots, k\}</script><p>其中， <script type="math/tex">(C_{l^{'}} \cap_{l^{'} \neq l} C_{l} = \emptyset)</script> 且 <script type="math/tex">(D=\cup_{l=1}^{k}C_{l})</script>。</p><p>相应的，用：</p><script type="math/tex; mode=display">\lambda_{i} \in {1, 2, \ldots, k}</script><p>表示样本 $x_{i}$ 的“簇标记”（cluster label）, 即：</p><script type="math/tex; mode=display">x_{i} \in C_{\lambda_{i}}</script><p>于是，聚类的结果可用包含 $n$ 个元素的簇标记向量表示：</p><script type="math/tex; mode=display">\lambda=(\lambda_{1}; \lambda_{2}, \ldots, \lambda_{n})</script><hr><h2 id="聚类性能度量"><a href="# 聚类性能度量" class="headerlink" title="聚类性能度量"></a>聚类性能度量 </h2><p><strong> 聚类性能度量亦称聚类“有效性指标”（validity index）。</strong></p><p><strong>设置聚类性能度量的目的:</strong></p><ul><li>对聚类结果，通过某种性能度量来评估其好坏；</li><li>若明确了最终将要使用的性能度量，则可直接将其作为聚类过程的优化目标，从而更好地得到符合要求的聚类结果。</li></ul><p><strong>什么样的聚类结果比较好？</strong></p><ul><li>“簇内相似度”（intra-cluster similarity）高</li><li>“蔟间相似度”（inter-cluster similarity）低</li></ul><p><strong>聚类性能度量分类：</strong></p><ul><li>“外部指标”（external index）：将聚类结果与某个“参考模型”（reference model）进行比较。</li><li>“内部指标”（internal index）：直接考察聚类结果而不利用任何参考模型。</li></ul><h3 id="外部指标"><a href="# 外部指标" class="headerlink" title="外部指标"></a>外部指标 </h3><p> 对数据集  <script type="math/tex">D = \{x_1, x_2, \ldots, x_n\}</script>，假定通过聚类，给出的簇划分为 <script type="math/tex">V=\{v_{1}, v_{2}, \ldots, v_{C}\}</script>，参考模型给出的簇划分为 <script type="math/tex">U=\{u_{1}, u_{2}\, \ldots, u_{R}\}</script>。相应地，令<script type="math/tex">\lambda</script> 与 <script type="math/tex">\lambda^{*}</script> 分别表示与 $V$ 和 $U$ 对应的簇标记向量。我们将样本两两配对考虑，定义：</p><script type="math/tex; mode=display">\begin{align}a & = |SS|, SS =\left \{(x_i,x_j)|(\lambda_i = \lambda_j, \lambda_i^* =  \lambda_j^* , i < j)\right \} \crb & = |SD|, SD =\left \{(x_i,x_j)|(\lambda_i = \lambda_j, \lambda_i^* \neq  \lambda_j^* , i < j)\right \} \crc & = |DS|, DS =\left \{(x_i,x_j)|(\lambda_i \neq \lambda_j, \lambda_i^* =  \lambda_j^* , i < j)\right \} \crd & = |DD|, DD =\left \{(x_i,x_j)|(\lambda_i \neq \lambda_j, \lambda_i^* \neq  \lambda_j^* , i < j)\right \}\end{align}</script><p>其中：</p><ul><li>集合 $SS$ 包含了在 $V$ 中隶属于相同簇且在 $U$ 中也隶属于相同簇的样本对；</li><li>集合 $SD$ 包含了在 $V$ 中隶属于相同簇且在 $U$ 中也隶属于不同簇的样本对；</li><li>集合 $DS$ 包含了在 $V$ 中隶属于不同簇且在 $U$ 中也隶属于相同簇的样本对；</li><li>集合 $DD$ 包含了在 $V$ 中隶属于不同簇且在 $U$ 中也隶属于不同簇的样本对；</li></ul><p>这样，由于每个样本对 <script type="math/tex">((x_{i}, x_{j})(i<j))</script> 仅能出现在一个集合中，因此有：</p><script type="math/tex; mode=display">a+b+c+d=n(n-1)/2</script><h4 id="JC-Jaccard-Coefficient"><a href="#JC-Jaccard-Coefficient" class="headerlink" title="JC (Jaccard Coefficient)"></a>JC (Jaccard Coefficient)</h4><script type="math/tex; mode=display">JC = \frac{a}{a+b+c}</script><p>JC 系数的结果分布在 $[0,1]$ 区间，值越大越好。个人总结，JC 系数是一个比较“爱憎分明”的相似性度量指标，对于聚类效果越好的模型的 JC 计算结果会很好，对于聚类效果越差的模型的 JC 计算结果会很差。关于 JC 系数的数学解释及实际意义，可以参考 Wiki 对应词条，讲的非常清晰：<a href="https://en.wikipedia.org/wiki/Jaccard_index" target="_blank" rel="noopener">Jaccard index</a>。</p><p>在此我摘取其中的重点：</p><blockquote><p>When used for binary attributes, the Jaccard index is very similar to the Simple matching coefficient. The main difference is that the SMC has the term $M_{00}$ in its numerator and denominator, whereas the Jaccard index does not. Thus, the SMC compares the number of matches with the entire set of the possible attributes, whereas the Jaccard index only compares it to tshe attributes that have been chosen by at least A or B.</p><p><strong>In market basket analysis for example, the basket of two consumers who we wish to compare might only contain a small fraction of all the available products in the store, so the SMC would always return very small values compared to the Jaccard index. Using the SMC would then induce a bias by systematically considering, as more similar, two customers with large identical baskets compared to two customers with identical but smaller baskets; thus making the Jaccard index a better measure of similarity in that context.</strong></p><p>In other contexts, where 0 and 1 carry equivalent information (symmetry), the SMC is a better measure of similarity. For example, vectors of demographic variables stored in dummies variables, such as gender, would be better compared with the SMC than with the Jaccard index since the impact of gender on similarity should be equal, independent of whether male is defined as a 0 and female as a 1 or the other way around. However, when we have symmetric dummy variables, one could replicate the behaviour of the SMC by splitting the dummies into two binary attributes (in this case, male and female), thus transforming them into asymmetric attributes, allowing the use of the Jaccard index without introducing the bias. By using this trick, the Jaccard index can be considered as making the SMC a fully redundant metric. The SMC remains however more computationally efficient in the case of symmetric dummy variables since it doesn’t require adding extra dimensions.</p><p><strong>In general, the Jaccard index can be considered as an indicator of local “similarity” while SMC evaluates “similarity” relative to the whole “universe”.</strong>Similarity and dissimilarity must be understood in a relative sense. For example, if there are only 2 attributes (x,y), then A=(1,0) is intuitively very different from B=(0,1). However if there are 10 attributes in the “universe”, A=(1,0,0,0,0,0,0,0,0,0) and B=(0,1,0,0,0,0,0,0,0) are not intuitively so different anymore. If the focus comes back to be just on A and B, the remaining 8 attributes are often considered as redundant. As a result, A and B are very different in a “local” sense (which the Jaccard Index measures efficiently), but less different in a “global” sense (which the SMC measures efficiently). From this point of view, the choice of using SMC or the Jaccard index comes down to more than just symmetry and asymmetry of information in the attributes. The distribution of sets in the defined “universe” and the nature of the problem to be modeled should also be considered.</p><p>The Jaccard index is also more general than the SMC and can be used to compare other data types than just vectors of binary attributes, such as Probability measures.</p></blockquote><p>上段重点的大体意思是：JC 的一个适用场景，例如商场或者电商（亚马逊）的用户们，在买东西的时候，我们对其相似性进行判断的时候，SMC（Simple Matching Coefficient）简单匹配系数并不太适用，是因为 SMC 中添加了（两个顾客都不感兴趣的商品这一信息），而 JC 并没有考虑这一部分。对于琳琅满目的商品信息而言，两个顾客不感兴趣的商品应该远远多于他们感兴趣的商品，简而言之，如果将顾客对所有的商品向量进行标记，感兴趣的为 1，不感兴趣的为 0，那么得到的这个向量应当是一个非常稀疏的。在这种情况下，如果使用 SMC，势必会导致结果很小，但是实际上我们完全可以不考虑两个顾客都不感兴趣的内容，而是考虑两个顾客感兴趣的商品信息之和，对其进行计算，相对于 SMC，得到的结果会好解释许多。就像那句话说的，<strong>JC 就好比是一个局域性的“相似性”比较的衡量指标，而 SMC 就好比是要考虑整个宏观宇宙之下的“相似性”比较</strong>。</p><h4 id="FMI-Fowlkes-and-Mallows-Index"><a href="#FMI-Fowlkes-and-Mallows-Index" class="headerlink" title="FMI (Fowlkes and Mallows Index)"></a>FMI (Fowlkes and Mallows Index)</h4><script type="math/tex; mode=display">FMI = \sqrt{\frac{a}{a+b}\cdot \frac{a}{a+c}}</script><p>FMI 系数的结果分布在 $[0,1]$ 区间，值越大越好。个人总结，FMI 系数是一个比较“温文儒雅”的相似性度量指标，对于聚类效果特别好的模型的 FMI 计算结果不会好得特别夸张，对于聚类效果特别越差的模型的 FMI 计算结果也不会差得特别夸张。同样，FMI 系数的数学解释及实际意义，对应的 Wiki 词条：<a href="https://en.wikipedia.org/wiki/Fowlkes–Mallows_index" target="_blank" rel="noopener">Fowlkes–Mallows index</a>。</p><p>在此我摘取其中的重点：</p><blockquote><p>Since the index is directly proportional to the number of true positives, a higher index means greater similarity between the two clusterings used to determine the index. <strong>One of the most basic thing to test the validity of this index is to compare two clusterings that are unrelated to each other.</strong> Fowlkes and Mallows showed that on using two unrelated clusterings, the value of this index approaches zero as the number of total data points chosen for clustering increase; whereas the value for the Rand index for the same data quickly approaches making Fowlkes–Mallows index a much more accurate representation for unrelated data. <strong>This index also performs well if noise is added to an existing dataset and their similarity compared. Fowlkes and Mallows showed that the value of the index decreases as the component of the noise increases.</strong> The index also showed similarity even when the noisy dataset had a different number of clusters than the clusters of the original dataset. Thus making it a reliable tool for measuring similarity between two clusters.</p></blockquote><p>上段重点的大体意思是：“衡量一个相似性度量指标是否可靠，应当将对于两个完全不相关簇的研究也作为重要的判定基础之一”，这样讲可能太绕了，解释一下就是如果我们对一个相似性度量指标的可靠性进行判断，不仅仅需要：我们的模型与“参考模型”两个模型越相似（聚类效果越好），指标系数越大（或越小）；同样的，我们的模型与“参考模型”越不相似（聚类效果越差），指标系数也应该呈现一个单调变大（或变小）的趋势。</p><p>FMI 与其他相似性度量指标的区别是，特别是与 JC 系数对比，JC 系数 $[0,1]$ 的特点是假如我们的模型聚类效果越好，结果就越趋向于 1，反之，如果我们的模型聚类效果越不好，结果就越趋向于 0；而 FMI $[0,1]$ 则不会显得那么“爱憎分明”，因为在 $[0,1]$ 范围内根号运算的存在，使得模型越好的结果不会好得特别夸张，但是模型越差的结果也不会显得特别夸张，因为根号运算起到一个“缓冲 ”的作用。另外一点，就是 FMI 系数对于含有噪声的数据集的判定效果仍然不错。</p><h4 id="RI-Rand-Index"><a href="#RI-Rand-Index" class="headerlink" title="RI (Rand Index)"></a>RI (Rand Index)</h4><script type="math/tex; mode=display">RI = \frac{a+d}{a+b+c+d}</script><p>RI 系数的结果分布在 $[0,1]$ 区间，值越大越好。关于 RI 系数的数学解释及实际意义，可以参考 Wiki 对应词条，重点是要看它的改进指标 ARI：<a href="https://en.wikipedia.org/wiki/Rand_index" target="_blank" rel="noopener">Rand  index</a>。</p><h4 id="ARI-Adjusted-Rand-Index"><a href="#ARI-Adjusted-Rand-Index" class="headerlink" title="ARI (Adjusted Rand Index)"></a>ARI (Adjusted Rand Index)</h4><p>  在 <strong>RI（Rand Index）</strong> 的评判基础上，为了实现“在聚类结果随机产生的情况下，指标应该接近零”，<strong>ARI（Adjusted Rand Index）</strong>系数被提出，它具有更高的区分度。</p><blockquote><p>A problem with the Rand index is that the expected value of the Rand index of two random partitions does not take a constant value (say zero). The adjusted Rand index proposed by [Hubert and Arabie, 1985] assumes the generalized hypergeometric distribution as the model of randomness, i.e., the $U$ and $V$ partitions are picked at random such that the number of objects in the classes and clusters are fixed.</p><script type="math/tex; mode=display">ARI  = \frac{Rand \ Index - expected \ index}{maximum \ index - expected \ index} = \frac{RI - E(RI)}{1-E(RI)}</script></blockquote><p>进行进一步的展开推导后：</p><script type="math/tex; mode=display">API = \frac{\frac{a+d}{a+b+c+d} - \frac{(a+b)(a+c)+(c+d)(b+d)}{(a+b+c+d)^2}}{\frac{(a+b)(a+c)+(c+d)(b+d)}{(a+b+c+d)^2}}</script><p>ARI 系数结果分布在 $[-1,1]$ 区间，负数代表结果不好，越接近于 1 意味着聚类结果与真实情况越吻合。个人总结，ARI 系数对于任意数量的聚类中心和样本数，随机聚类的 ARI 都非常接近于 0。ARI 相比于 RI 好在，它是负数的时候就说明我们自己的模型很糟糕，有个更加相对明确的评判标准。（从广义的角度上来说，ARI 衡量的是两个数据分布的吻合程度）</p><h4 id="举个具体例子计算 -ARI"><a href="# 举个具体例子计算 -ARI" class="headerlink" title="举个具体例子计算 ARI"></a>举个具体例子计算 ARI</h4><p>Let <script type="math/tex">n_{ij}</script> be the number of objects that are in both class <script type="math/tex">u_{i}</script> and cluster <script type="math/tex">v_{j}</script>. Let <script type="math/tex">n_{i.}</script> and <script type="math/tex">n_{.j}</script> be the number of objects in class <script type="math/tex">u_{i}</script> and cluster <script type="math/tex">v_{j}</script> respectively.The notations are illustrated in Table below:</p><script type="math/tex; mode=display">\begin{array}{c|cccc|c}Class \ Cluster & v_{1}  & v_{2} & ... & v_{C} & Sums \cr\hlineu_{1} & n_{11} & n_{12} & ... & n_{1C} & n_{1.}  \cru_{2} & n_{21} & n_{22} & ... & n_{2C} & n_{2.}  \cr... & ... & ... & ... & ... & ...  \cru_{R} & n_{R1} & n_{R2} & ... & n_{RC} & n_{R.}  \cr\hlineSums & n_{.1} & n_{.2} & ... & n_{.C} & n.. = n  \cr\end{array}</script><p>Here is the example:</p><script type="math/tex; mode=display">\begin{array}{c|ccc|c}Class \ Cluster & v_{1}  & v_{2} &  v_{3} & Sums \cr\hlineu_{1} & 1 & 1 &  0 & 2  \cru_{2} & 1 & 2 &  1 & 4  \cru_{3} & 0 & 0 &  4 & 4  \cr\hlineSums & 2 & 3 & 5 & n = 10  \cr\end{array}</script><p>$a$ is defined as the number of pairs of objects in the same class $U$ and same cluster in $V$,hence $a$ can be written as <script type="math/tex">\sum_{i,j} \binom{n_{ij}}{2}</script>.In Example,<script type="math/tex">a = \binom{2}{2} + \binom{4}{2} = 7</script>(所有不超过 2 的都不需要考虑，因为<script type="math/tex">\binom{1}{2} = 0</script>).</p><p>$b$ is defined as the number of pairs of objects in the same class in $U$ but not in the same cluster in $V$.In terms of the notation in Table, $b$ can be writtern as <script type="math/tex">\sum_{i}\binom{n_{i.}}{2}-\sum_{i,j}\binom{n_{ij}}{2}</script>. In Example, <script type="math/tex">b = \binom{2}{2} + \binom{4}{2} + \binom{4}{2} - 7 = 6</script>.</p><p>Similarly, $c$ is defined as the number of pairs of objects in the same cluster in $V$ but not in the same class in $U$, so $c$ can be writtern as <script type="math/tex">\sum_{j}\binom{n_{.j}}{2}-\sum_{i,j}\binom{n_{ij}}{2} = \binom{2}{2} + \binom{3}{2} + \binom{5}{2} -7 = 7</script>.</p><p>$d$ is defined as the number of pairs of objects that are not in the same class in $U$ and not in the same cluster in $V$. Since <script type="math/tex">a + b + c + d = \binom{n}{2}</script>, <script type="math/tex">d = \binom{10}{2} -7 - 6 - 7 = 25</script>.</p><p>The Rand Index for comparing the two partitions in Example is $\frac{7+25}{45} = 0.711$, while the adjusted Rand Index is 0.311.</p><p><strong>The Rand index is much higher than the adjusted Rand index, which is typical. Since the Rand index lies between 0 and 1, the expected value of the Rand index (although not a constant value) must be greater than or equal to 0.On the other hand, the expected value of the adjusted Rand index has value zero and the maximum value of the adjusted Rand is  also 1. Hence, there is a wider range of values that the adjusted Rand index can take on, thus increasing the sensitivity of the index.</strong></p><h3 id="内部指标"><a href="# 内部指标" class="headerlink" title="内部指标"></a>内部指标 </h3><p> 根据聚类结果的簇划分 <script type="math/tex">(C={C_{1}, C_{2}, \ldots, C_{k}})</script> , 定义：</p><ul><li>簇 $C$ 内样本间的平均距离</li></ul><script type="math/tex; mode=display">avg(C)=\frac{2}{|C|(|C|-1)}\sum_{1<i<j<|C|}dist(x_{i}, x_{j})</script><ul><li><p>簇 $C$ 内样本间的最远距离</p><script type="math/tex; mode=display">diam(C)=max_{1<i<j<|C|}dist(x_{i}, x_{j})</script></li><li><p>簇 <script type="math/tex">C_{i}</script> 与簇 <script type="math/tex">C_{j}</script> 最近样本间的距离</p><script type="math/tex; mode=display">d{min}(C_{i}, C_{j})=min_{1<i<j<|C|}dist(x_{i}, x_{j})</script></li><li>簇 <script type="math/tex">C_{i}</script> 与簇 <script type="math/tex">C_{j}</script> 中心点间的距离<script type="math/tex; mode=display">d_{cen}(C_{i}, C_{j})=dist(\mu_{i}, \mu_{j})</script></li></ul><p>其中：</p><ul><li>$dist(,)$ 是两个样本之间的距离</li><li>$\mu$ 是簇 $C$ 的中心点 <script type="math/tex">\mu=\frac{1}{|C|}\sum_{1<i<|C|}x_{i}</script></li></ul><h4 id="CP-Compactness"><a href="#CP-Compactness" class="headerlink" title="CP (Compactness)"></a>CP (Compactness)</h4><script type="math/tex; mode=display">\begin{align}\overline{CP_{i}} & =\frac{1}{|C_{i}|}\sum_{x_{i} \in C_{i}} dist(x_{i}, \mu_{i}) \cr\overline{CP} & =\frac{1}{k}\sum_{k=1}^{k} \overline{CP_{k}}\end{align}</script><p>CP 紧密性，其计算的是每个簇中各个点到簇中心的平均距离，值越低意味着簇内聚类距离越近，缺点就是没有考虑到簇间效果。</p><h4 id="SP-Separation"><a href="#SP-Separation" class="headerlink" title="SP (Separation)"></a>SP (Separation)</h4><script type="math/tex; mode=display">SP=\frac{2}{k^2-k}\sum^{k}_{i=1}\sum^{k}_{j=i+1} d_{cen}(\mu_{i}, \mu_{j})</script><p>SP 间隔性，其计算的是各簇中心两两之间的平均距离，值越高意味着簇间距离越远，缺点是没有考虑簇内效果。</p><h4 id="DBI-Davies-Bouldin-Index"><a href="#DBI-Davies-Bouldin-Index" class="headerlink" title="DBI (Davies-Bouldin Index)"></a>DBI (Davies-Bouldin Index)</h4><script type="math/tex; mode=display">DBI=\frac{1}{k}\sum^{k}_{i=1}\underset{j \neq i}{max}\bigg(\frac{avg(C_{i})+avg(C_{j})}{d_{cen}(\mu_{i}, \mu_{j})}\bigg)</script><p>关于 DBI 系数的数学解释及实际意义，可以参考 Wiki 对应词条：<a href="https://en.wikipedia.org/wiki/Davies–Bouldin_index" target="_blank" rel="noopener">Davies-Bouldin Index</a>。</p><blockquote><p>These conditions constrain the index so defined to be symmetric and non-negative. Due to the way it is defined, as a function of the ratio of the within cluster scatter, to the between cluster separation, a lower value will mean that the clustering is better. <strong>It happens to be the average similarity between each cluster and its most similar one, averaged over all the clusters, where the similarity is defined as $S_{i}$ above.</strong> This affirms the idea that no cluster has to be similar to another, and hence the best clustering scheme essentially minimizes the Davies–Bouldin index. This index thus defined is an average over all the $i$ clusters, and hence a good measure of deciding how many clusters actually exists in the data is to plot it against the number of clusters it is calculated over. The number <em>i</em> for which this value is the lowest is a good measure of the number of clusters the data could be ideally classified into. This has applications in deciding the value of $k$ in the <a href="https://en.wikipedia.org/wiki/Kmeans" target="_blank" rel="noopener">kmeans</a> algorithm, where the value of k is not known apriori. </p></blockquote><p>DBI 系数结果为非负数，值越小意味着簇内距离越小，同时簇间距离越大。个人总结，DBI 其实就是将几个 <script type="math/tex">d_{max}</script> 的值叠加，当我们模型最后聚集了多少个 Cluster 簇，就会出现多少个 <script type="math/tex">d_{max}</script>，这些<script type="math/tex">d_{max}</script> 值叠加的结果越小，说明我们模型的聚类越合理。可以通过 DBI 系数来确定 k-means 中 $k$ 的最佳值，即不同的 $k$ 值对应不同的 DBI 结果，选取 DBI 结果最小时对应的 $k$ 值，意味着此时这样划分 $k$ 个簇是最合理，效果最佳的。其缺点是，因为使用的是欧式距离，所以对于环状分布，聚类评测比较糟糕。</p><h4 id="DI-Dunn-Index"><a href="#DI-Dunn-Index" class="headerlink" title="DI (Dunn Index)"></a>DI (Dunn Index)</h4><script type="math/tex; mode=display">DI=\underset{1 \leqslant i \leqslant k}{min}\bigg\{\underset{j \neq i}{min}\bigg(\frac{d_{min}(C_{i}, C_{j})}{max_{1 \leqslant l \leqslant k}diam(C_{l})}\bigg)\bigg\}</script><p>关于 DI 系数的数学解释及实际意义，可以参考 Wiki 对应词条：<a href="https://en.wikipedia.org/wiki/Dunn_index" target="_blank" rel="noopener">Dunn Index</a>。</p><blockquote><p>Being defined in this way, the <em>DI</em> depends on $k$, the number of clusters in the set. If the number of clusters is not known apriori, the $k$ for which the <em>DI</em> is the highest can be chosen as the number of clusters. There is also some flexibility when it comes to the definition of d(x,y) where any of the well known metrics can be used, like <a href="https://en.wikipedia.org/wiki/Manhattan_distance" target="_blank" rel="noopener">Manhattan distance</a> or <a href="https://en.wikipedia.org/wiki/Euclidean_distance" target="_blank" rel="noopener">Euclidean distance</a> based on the geometry of the clustering problem. <strong>This formulation has a peculiar problem, in that if one of the clusters is badly behaved, where the others are tightly packed, since the denominator contains a ‘max’ term instead of an average term, the Dunn Index for that set of clusters will be uncharacteristically low. This is thus some sort of a worst case indicator, and has to be kept in mind.</strong></p></blockquote><p>DI 系数结果为非负数，值越大意味着簇间距离越大，同时簇内距离越小。个人总结，DI 系数计算的是，任意两个簇的最短距离（簇间）除以任意簇中的最大距离（簇内），其优点是对于离散点的聚类评测效果不错，但缺点是对于环状分布评测效果比较差。还有上述重点提到的，因为分母采用的是取一个 max 距离而不是取平均值 average，所以会出现一个奇怪的问题，对于某个聚类效果特别差的一个簇，它可能”一个老鼠屎坏了一锅粥”这种，导致 DI 系数会特别低，这也是当我们需要使用 DI 系数时，对于某些聚类表现非常差的簇需要注意的原因。</p><h2 id="聚类距离计算"><a href="# 聚类距离计算" class="headerlink" title="聚类距离计算"></a>聚类距离计算 </h2><p><strong> 距离度量（distance measure）函数 $dist(,)$ 需满足的基本性质：</strong></p><ul><li><strong>非负性</strong>：<script type="math/tex">dist(x_{i}, x_{j}) \geqslant 0</script></li><li><strong>同一性</strong>：<script type="math/tex">dist(x_{i}, x_{j})=0</script> 当且仅当 <script type="math/tex">x_{i}=x_{j}</script></li><li><strong>对称性</strong>：<script type="math/tex">dist(x_{i}, x_{j})=dist(x_{j}, x_{i})</script></li><li><strong>直递性</strong>：<script type="math/tex">dist(x_{i}, x_{j}) \leqslant dist(x_{i}, x_{k}) + dist(x_{k}, x_{j})</script> (可不满足)</li></ul><p><strong>变量属性：</strong></p><ul><li>连续属性： 闵可夫斯基距离</li><li>离散属性<ul><li>有序属性： 闵可夫斯基距离</li><li>无序属性：VDM (Value Difference Metric)</li></ul></li><li>混合属性：闵可夫斯基距离 与 VDM 混合距离</li></ul><h3 id="闵可夫斯基距离（Minkowski-distance）"><a href="# 闵可夫斯基距离（Minkowski-distance）" class="headerlink" title="闵可夫斯基距离（Minkowski distance）"></a>闵可夫斯基距离（Minkowski distance）</h3><p>样本：<script type="math/tex">x_{i}=(x_{i1}, x_{i2}, \ldots, x_{in})</script> 与 <script type="math/tex">x_{j}=(x_{j1}, x_{j2}, \ldots, x_{jn})</script></p><script type="math/tex; mode=display">dist_{mk}(x_{i}, x_{j})=\bigg(\sum_{u=1}^{n}|x_{iu}-x_{ju}|^{p}\bigg)^{\frac{1}{p}}</script><h3 id="VDM-Value-Difference-Metric"><a href="#VDM-Value-Difference-Metric" class="headerlink" title="VDM(Value Difference Metric)"></a>VDM(Value Difference Metric)</h3><blockquote><p>我们常将属性划分为“连续属性”（continuous attribute）和“离散属性”（categorical attribute），前者在定义域上有无穷多个可能的取值，后者在定义域上是有限个取值。然而，在讨论距离计算时，属性上是否定义了“序”关系更为重要。例如定义域为 $\lbrace 1,2,3 \rbrace$ 的离散属性与连续属性的性质更接近一些， 能直接在属性值上计算距离：“1”与“2”比较接近、与“3”比较远，这样的属性称为“有序属性”（ordinal attribute）; 而定义域为｛飞机，火车，轮船｝这样的离散属性则不能直接在属性值上计算距离，称为“无序属性”（non-ordinal attribute）。显然， 闵可夫斯基距离可用于有序属性。</p><p>对无序属性可采用 VDM（Value Difference Metric） 「Stanfill and Waltz, 1986」。</p></blockquote><p>令 <script type="math/tex">m_{u,a}</script> 表示在属性 $u$ 上取值为 $a$ 的样本数，<script type="math/tex">m_{u, a, i}</script> 表示在第 $i$ 个样本簇中在属性 $u$ 上取值为 $a$ 的样本数，$k$ 为样本簇数，则属性 $u$ 上两个离散值 $a$ 与 $b$ 之间的 VDM 距离为：</p><script type="math/tex; mode=display">VDM_{q}(a, b)=\sum^{k}_{i=1}\bigg|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}\bigg|^{p}.</script><p>（这里的 $p$ 同闵可夫斯基距离中的 $p$ 一样）</p><h3 id="闵可夫斯基距离与 VDM 混合距离"><a href="# 闵可夫斯基距离与 VDM 混合距离" class="headerlink" title="闵可夫斯基距离与 VDM 混合距离"></a>闵可夫斯基距离与 VDM 混合距离 </h3><p> 假设有 <script type="math/tex">n_{c}</script> 个有序属性，<script type="math/tex">n-n_{c}</script> 个无序属性，有序属性排列在无序属性之前：</p><script type="math/tex; mode=display">MinkovDM_{p}(x_{i}, x_{j})=\bigg(\sum^{n_{c}}_{u=1}|x_{i,u}-x_{j,u}|^{p}+\sum^{n}_{u=n_{c}+1}VDM_{p}(x_{i,u},x_{j,u})\bigg)^{\frac{1}{p}}</script><h3 id="加权闵可夫斯基距离"><a href="# 加权闵可夫斯基距离" class="headerlink" title="加权闵可夫斯基距离"></a>加权闵可夫斯基距离 </h3><p> 当样本在空间中不同属性的重要性不同时：</p><script type="math/tex; mode=display">dist_{wmk}(x_{i}, x_{j})=(w_{1}\cdot|x_{i1}-x_{j1}|^{p}+w_{2}\cdot|x_{i2}-x_{j2}|^{p}+\ldots+w_{n}\cdot|x_{in}-x_{jn}|^{p})^{\frac{1}{p}}</script><p>其中： 权重<script type="math/tex">w_{i} \geqslant 0(i=1, 2, \ldots, p)</script> 表示不同属性的重要性，通常 <script type="math/tex">\sum_{i=1}^{n}w_{i}=1</script>。</p><hr><h1 id="聚类算法介绍及实现"><a href="# 聚类算法介绍及实现" class="headerlink" title="聚类算法介绍及实现"></a>聚类算法介绍及实现 </h1><p><strong> 聚类算法类型：</strong></p><ul><li><p><strong>基于原型的聚类（Prototype-based Clustering）</strong></p><ul><li><strong>K 均值聚类（K-means）</strong></li><li><strong>学习向量量化聚类（Learning vector Quantization）</strong></li><li><strong>高斯混合聚类（Mixture-of-Gaussian）</strong></li></ul></li><li><p><strong>基于密度的聚类（Density-based Clustering）</strong></p></li><li><p><strong>层次聚类（Hierarchical Clustering）</strong></p></li></ul><hr><h2 id="基于原型的聚类"><a href="# 基于原型的聚类" class="headerlink" title="基于原型的聚类"></a>基于原型的聚类 </h2><p><strong> 基于原型的聚类（Prototype-based Clustering），此类算法假设聚类结构能通过一组原形刻画。通常情况下，算法先对原型进行初始化，然后对原型进行迭代更新求解，采用不同的原型表示，不同的求解方式，将产生不同的算法。</strong></p><ul><li><strong>基于原型的聚类（Prototype-based Clustering）</strong><ul><li><strong>K 均值聚类（K-means）</strong></li><li><strong>学习向量量化聚类（Learning vector Quantization）</strong></li><li><strong>高斯混合聚类（Mixture-of-Gaussian）</strong></li></ul></li></ul><h3 id="K 均值聚类（K-means）"><a href="#K 均值聚类（K-means）" class="headerlink" title="K 均值聚类（K-means）"></a>K 均值聚类（K-means）</h3><h4 id="算法介绍"><a href="# 算法介绍" class="headerlink" title="算法介绍"></a>算法介绍 </h4><p> 给定样本集 <script type="math/tex">D= \lbrace x_{1}, x_{2}, \ldots, x_{n} \rbrace</script>， K-means 算法针对聚类所得簇划分 <script type="math/tex">C= \lbrace C_{1}, C_{2}, \ldots, C_{k} \rbrace</script>，最小化平方误差：</p><script type="math/tex; mode=display">E = \sum^{k}_{i=1}\sum_{x \in C_{i}}|x-\mu_{i}|_{2}^{2}</script><p>其中 <script type="math/tex">\mu_{i}</script> 是簇 <script type="math/tex">C_{i}</script> 的均值向量：</p><script type="math/tex; mode=display">\mu_{i}=\frac{1}{|C_{i}|}\sum_{x \in C_{i}}x</script><p>直观上看，$E$ 在一定程度上刻画了簇内样本围绕均值向量的紧密程度， $E$ 值越小簇内样本相似度越高。但最小化 $E$ 不容易，是一个 NP 难问题， K-means 算法采用了贪心策略，通过迭代优化来近似求解 $E$ 的最小值。具体算法如下：</p><p><img src="https://farm5.staticflickr.com/4303/36049857562_cf0480eb62_o.png" alt="$k$ 均值算法"></p><h4 id="算法实现 -（R 语言）"><a href="# 算法实现 -（R 语言）" class="headerlink" title="算法实现 （R 语言）"></a>算法实现 （R 语言）</h4><p>Python 的实现可以参考<a href="http://randolph.pro/2016/03/19/♞「Books」Programming%20Collective%20Intelligence%20-%20%20Chapter%203/">♞「Books」Programming Collective Intelligence -  Chapter 3</a> 这篇文章。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">head(iris)</span><br></pre></td></tr></table></figure><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  Sepal.Length Sepal.Width Petal.Length Petal.Width Species</span><br><span class="line"><span class="number">1</span>          <span class="number">5.1</span>         <span class="number">3.5</span>          <span class="number">1.4</span>         <span class="number">0.2</span>  setosa</span><br><span class="line"><span class="number">2</span>          <span class="number">4.9</span>         <span class="number">3.0</span>          <span class="number">1.4</span>         <span class="number">0.2</span>  setosa</span><br><span class="line"><span class="number">3</span>          <span class="number">4.7</span>         <span class="number">3.2</span>          <span class="number">1.3</span>         <span class="number">0.2</span>  setosa</span><br><span class="line"><span class="number">4</span>          <span class="number">4.6</span>         <span class="number">3.1</span>          <span class="number">1.5</span>         <span class="number">0.2</span>  setosa</span><br><span class="line"><span class="number">5</span>          <span class="number">5.0</span>         <span class="number">3.6</span>          <span class="number">1.4</span>         <span class="number">0.2</span>  setosa</span><br><span class="line"><span class="number">6</span>          <span class="number">5.4</span>         <span class="number">3.9</span>          <span class="number">1.7</span>         <span class="number">0.4</span>  setosa</span><br></pre></td></tr></table></figure><p>After a little bit of exploration, I found that Petal.Length and Petal. Width were similar among the same species but varied considerably between different species, as demonstrated below:</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(ggplot2)</span><br><span class="line">ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()</span><br></pre></td></tr></table></figure><p><img src="https://farm1.staticflickr.com/33/31680514905_7dba9b551a_o.jpg" alt></p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set.seed(<span class="number">20</span>)</span><br><span class="line">irisCluster &lt;- kmeans(iris[, <span class="number">3</span>:<span class="number">4</span>], centers = <span class="number">3</span>, nstart = <span class="number">20</span>)</span><br><span class="line">irisCluster</span><br></pre></td></tr></table></figure><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">K-means clustering with <span class="number">3</span> clusters of sizes <span class="number">50</span>, <span class="number">52</span>, <span class="number">48</span></span><br><span class="line"></span><br><span class="line">Cluster means:</span><br><span class="line">  Petal.Length Petal.Width</span><br><span class="line"><span class="number">1</span>     <span class="number">1.462000</span>    <span class="number">0.246000</span></span><br><span class="line"><span class="number">2</span>     <span class="number">4.269231</span>    <span class="number">1.342308</span></span><br><span class="line"><span class="number">3</span>     <span class="number">5.595833</span>    <span class="number">2.037500</span></span><br><span class="line"></span><br><span class="line">Clustering vector:</span><br><span class="line">  [<span class="number">1</span>] <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"> [<span class="number">33</span>] <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span></span><br><span class="line"> [<span class="number">65</span>] <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">3</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">3</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span></span><br><span class="line"> [<span class="number">97</span>] <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">2</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">2</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">2</span> <span class="number">3</span></span><br><span class="line">[<span class="number">129</span>] <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">2</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span></span><br><span class="line"></span><br><span class="line">Within cluster sum of squares by cluster:</span><br><span class="line">[<span class="number">1</span>]  <span class="number">2.02200</span> <span class="number">13.05769</span> <span class="number">16.29167</span></span><br><span class="line"> (between_SS / total_SS =  <span class="number">94.3</span> %)</span><br><span class="line"></span><br><span class="line">Available components:</span><br><span class="line"></span><br><span class="line">[<span class="number">1</span>] <span class="string">"cluster"</span>      <span class="string">"centers"</span>      <span class="string">"totss"</span>        <span class="string">"withinss"</span>    </span><br><span class="line">[<span class="number">5</span>] <span class="string">"tot.withinss"</span> <span class="string">"betweenss"</span>    <span class="string">"size"</span>         <span class="string">"iter"</span>        </span><br><span class="line">[<span class="number">9</span>] <span class="string">"ifault"</span></span><br></pre></td></tr></table></figure><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table(irisCluster$cluster, iris$Species)</span><br></pre></td></tr></table></figure><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line">  setosa versicolor virginica</span><br><span class="line"><span class="number">1</span>     <span class="number">50</span>          <span class="number">0</span>         <span class="number">0</span></span><br><span class="line"><span class="number">2</span>      <span class="number">0</span>         <span class="number">48</span>         <span class="number">4</span></span><br><span class="line"><span class="number">3</span>      <span class="number">0</span>          <span class="number">2</span>        <span class="number">46</span></span><br></pre></td></tr></table></figure><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; irisCluster$cluster &lt;- as.factor(irisCluster$cluster)</span><br><span class="line">&gt; ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point()</span><br></pre></td></tr></table></figure><p><img src="https://farm1.staticflickr.com/353/31565301411_fc7281b525_o.jpg" alt></p><hr><h3 id="学习向量量化聚类（Learning-vector-Quantization）"><a href="# 学习向量量化聚类（Learning-vector-Quantization）" class="headerlink" title="学习向量量化聚类（Learning vector Quantization）"></a>学习向量量化聚类（Learning vector Quantization）</h3><h4 id="算法介绍 -1"><a href="# 算法介绍 -1" class="headerlink" title="算法介绍"></a>算法介绍</h4><p><strong>LVQ 假设数据样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类。</strong></p><p>给定样本集 <script type="math/tex">D=\lbrace (x_{1}, y_{1}), (x_{2}, y_{2}), \ldots, (x_{m}, y_{m}) \rbrace</script>，每个样本 <script type="math/tex">x_{j}</script> 是由 $n$ 个属性描述的特征向量 <script type="math/tex">(x_{j1}; x_{j2}; \ldots; x_{jn})</script>,<script type="math/tex">y_{j} \in \mathcal{Y}</script> 是样本 <script type="math/tex">x_{j}</script> 的类别标记。LVQ 的目标是学得一组 $n$ 维原型向量<script type="math/tex">\lbrace p_{1}, p_{2}, \ldots, p_{q} \rbrace</script>, 每个原型向量代表一个聚类簇，簇标记为<script type="math/tex">t_{i}\in \mathcal{Y}</script>。</p><p>具体算法如下：</p><p><img src="https://farm5.staticflickr.com/4320/36049858912_c400743b7f_o.png" alt="学习向量量化方法"></p><p><strong>算法解释</strong></p><ul><li>算法第 1 行：对原型向量进行初始化。例如：对第 $i,i=(1,2,\ldots,q)$ 个簇，可以从类别标记为 $t_{i}$ 的样本中随机选取一个作为原型向量。</li><li><p>算法第 2-12 行：对原型向量进行迭代优化。在每一轮迭代中，算法随机选取一个有标记训练样本，找出与其距离最近的原型向量，并根据两者的类别标记是否一致来对原型向量进行相应的更新。</p><ul><li>算法第 5 行：这是竞争学习的“胜者为王“的策略。SOM 是基于无标记样本的聚类算法，而 LVQ 可以看作是 SOM 基于监督信息的扩展。</li><li><p>算法第 6-10 行：如何更新原型向量。对样本 $x_{j}$，</p><ul><li><p>若距离 <script type="math/tex">x_{j}</script> 最近的原型向量 <script type="math/tex">p_{i^{*}}</script> 与 <script type="math/tex">x_{j}</script> 的标记相同，则令 <script type="math/tex">p_{i^{*}}</script> 向 <script type="math/tex">x_{j}</script> 的方向靠拢，此时新的原型向量为 </p><script type="math/tex; mode=display">p' = p_{i^{*}} + \eta \cdot (x_{j}-p_{i^{*}})</script><p>​        $p^{‘}$ 与 $x_{j}$ 之间的距离为 </p><script type="math/tex; mode=display">||p'-x_{j}||_{2}=(1-\eta) \cdot ||p_{i^{*}}-x_{j}||_{2}</script><p>​       原型向量 <script type="math/tex">p_{i^{*}}</script> 更新为 <script type="math/tex">p'</script>之后将更接近 <script type="math/tex">x_{j}</script>。</p></li><li>若距离 <script type="math/tex">x_{j}</script> 最近的原型向量 <script type="math/tex">p_{i^{*}}</script> 与 <script type="math/tex">x_{j}</script> 的标记不同，则令 <script type="math/tex">p_{i^{*}}</script> 向 <script type="math/tex">x_{j}</script> 的方向远离，此时新的原型向量为 <script type="math/tex; mode=display">p'= p_{i^{*}} - \eta \cdot (x_{j}-p_{i^{*}})</script>​       $p^{‘}$ 与 $x_{j}$ 之间的距离为 <script type="math/tex; mode=display">||p'-x_{j}|_{2}=(1+\eta) \cdot |p_{i^{*}}-x_{j}||_{2}</script>​       原型向量 <script type="math/tex">p_{i^{*}}</script> 更新为 <script type="math/tex">p'</script>之后将更远离 <script type="math/tex">x_{j}</script>。</li></ul></li></ul></li><li>算法第 12 行：若算法的停止条件已满足(例如已达到最大迭代轮数，或原型向量更新很小甚至不再更新)，则将当前原型向量作为最终结果返回。</li><li>在学得一组原型向量 <script type="math/tex">\lbrace p_{1}, p_{2}, \ldots, p_{q} \rbrace</script> 后即可实现对样本空间 <script type="math/tex">\mathcal{X}</script> 的簇划分。<ul><li>对任意样本 $x$, 他将被划入与其距离最近的原型向量所代表的簇中，每个原型向量<script type="math/tex">p_{i}</script> 定义了与之相关的一个区域 <script type="math/tex">R_{i}</script>，该区域中每个样本与 <script type="math/tex">p_{i}</script> 的距离不大于他与其他原型向量 <script type="math/tex">p_{i'} (i \neq i')</script>，即 <script type="math/tex; mode=display">R_{i}= \lbrace x \in \mathcal{X} \ |\ ||x-p_{i}||_{2} \leqslant ||x-p_{i'}||_{2}, i \neq i'\rbrace</script>​       由此形成了对样本空间 $\mathcal{X}$ 的簇划分 <script type="math/tex">{R_{1}, R_{2}, \ldots, R_{q}}</script>，该划分通常称为“Voronoi 剖分”（Voronoi tessellation）。</li></ul></li></ul><h4 id="算法实现"><a href="# 算法实现" class="headerlink" title="算法实现"></a>算法实现</h4><p>（Unfinished.）</p><h4 id="补充"><a href="# 补充" class="headerlink" title="补充"></a>补充 </h4><h5 id="竞争型学习"><a href="# 竞争型学习" class="headerlink" title="竞争型学习"></a> 竞争型学习 </h5><p> 竞争型学习是神经网络中一种常见的无监督学习策略，在使用该策略时，网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制。这种机制亦称为”胜者通吃”(winner-take-all)原则。</p><h5 id="ART- 网络"><a href="#ART- 网络" class="headerlink" title="ART 网络"></a>ART 网络</h5><p>ART (Adaptive Resonance Theory, 自适应谐振理论）网络「Carpenter and Grossberg, 1987」是竞争型学习的重要代表。该网络由比较层、识别层、识别阈值和重置模块构成。其中，比较层负责接收输入样本，并将其传递给识别层神经元。识别层每个神经元对应一个模式类（模式类可以认为是某类别的“子类”），神经元数目可在训练过程中动态增长以增加新的模式类。</p><p>在接收到比较层的输入信号后，识别层神经元之间相互竞争以产生获胜神经元。竞争的最简单方式是，计算输入向量与每个识别层神经元所对应的模式类的代表向量之间的距离，距离最小者胜．获胜神经元将向其他识别层神经元发送信号，抑制其激活。若输入向量与获胜神经元所对应的代表向量之间的相似度大于识别阈值，则当前输入样本将被归为该代表向量所属类别，同时，网络连接权将会更新，使得以后在接收到相似输入样本时该模式类会计算出更大的相似度，从而使该获胜神经元有更大可能获胜；若相似度不大于识别阈值，则重置模块将在识别层增设一个新的神经元，其代表向量就设置为当前输入向量。</p><p>显然，识别阈值对 ART 网络的性能有重要影响。当识别阈值较高时，输入样本将会被分成比较多、比较精细的模式类，而如果识别阈值较低，则会产生比较少、比较粗略的模式类。</p><p>ART 比较好地缓解了竞争型学习中的“可塑性一稳定性窘境”（stability— plasticity dilemma），可塑性是指神经网络要有学习新知识的能力，而稳定性则是指神经网络在学习新知识时要保持对旧知识的记忆．这就使得 ART 网络具有一个很重要的优点：可进行 <strong> 增量学习（incremental learning）</strong>或 <strong> 在线学习（online learning)</strong>。</p><p>早期的 ART 网络只能处理布尔型输入数据，此后 ART 发展成了一个算法族，包括能处理实值输入的 ART2 网络、结合模糊处理的 FuzzyART 网络，以及可进行监督学习的 ARTMAP 网络等。 </p><h5 id="SOM- 网络"><a href="#SOM- 网络" class="headerlink" title="SOM 网络"></a>SOM 网络</h5><p>SOM (Self-Organizing Map，自组织映射）网络「Kohollen, 1982」是一种竞争学习型的无监督神经网络，它能将高维输入数据映射到低维空间（通常为二维），同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元。</p><p><img src="https://farm6.staticflickr.com/5569/31429001070_b16d937809_o.png" alt></p><p>如图所示，SOM 网络中的输出层神经元以矩阵方式排列在二维空间中，每个神经元都拥有一个权向量，网络在接收输入向量后，将会确定输出层获胜神经元，它决定了该输入向量在低维空间中的位置。SOM 的训练目标就是为每个输出层神经元找到合适的权向量，以达到保持拓扑结构的目的。 SOM 的训练过程很简单：在接收到一个训练样本后，每个输出层神经元会计算该样本与自身携带的权向量之间的距离，距离最近的神经元成为竞争获胜者，称为最佳匹配单元（best matching unit)。然后，最佳匹配单元及其邻近神经元的权向量将被调整，以使得这些权向量与当前输入样本的距离缩小。这个过程不断迭代，直至收敛。</p><hr><h3 id="高斯混合聚类（Mixture-of-Gaussian）"><a href="# 高斯混合聚类（Mixture-of-Gaussian）" class="headerlink" title="高斯混合聚类（Mixture-of-Gaussian）"></a>高斯混合聚类（Mixture-of-Gaussian）</h3><h4 id="算法介绍 -2"><a href="# 算法介绍 -2" class="headerlink" title="算法介绍"></a>算法介绍 </h4><p> 与 $k$ 均值、LVQ 用原型向量来刻画聚类结构不同，<strong>高斯混合聚类 (Mixture-of-Gaussian) 采用概率模型来表达聚类原型</strong>。</p><h5 id="（多元）高斯分布："><a href="#（多元）高斯分布：" class="headerlink" title="（多元）高斯分布："></a>（多元）高斯分布：</h5><p>对 $n$ 维样本空间 $\mathcal{X}$ 中的随机向量 $x$，若 $x$ 服从 (多元) 高斯分布，其概率密度函数为：</p><script type="math/tex; mode=display">p(x| \mu, \Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} e^{- \frac{1}{2} (x-\mu)^{T} \Sigma^{-1} (x-\mu)}</script><p>其中：$\mu​$ 是 $n​$ 维均值向量，$\Sigma​$ 是 $n \times n​$ 协方差矩阵。($\Sigma​$：对称正定矩阵；$|\Sigma|​$：$\Sigma​$ 的行列式；$\Sigma^{-1}​$：$\Sigma​$ 的逆矩阵)</p><h5 id="（多元）高斯混合分布："><a href="#（多元）高斯混合分布：" class="headerlink" title="（多元）高斯混合分布："></a>（多元）高斯混合分布：</h5><p>对 $n$ 维样本空间 $\mathcal{X}$ 中的随机向量 $x$，若 $x$ 服从（多元）高斯混合分布，其概率密度函数为：</p><script type="math/tex; mode=display">p_{\mathcal{M}}(x)=\sum_{i=1}^{k}\alpha_{i} \cdot p(x| \mu_{i}, \Sigma_{i})</script><p>该分布由 $k$ 个混合成分组成，每个成分对应一个（多元）高斯分布，其中：<script type="math/tex">\mu_{i}</script>, <script type="math/tex">\Sigma_{i}</script> 是第 $i$ 个高斯混合成分的参数， 而 <script type="math/tex">\alpha_{i}</script> 为相应的“混合系数” (mixture coeffcient)， <script type="math/tex">\sum_{i=1}^{k}\alpha_{i}=1</script>。</p><h5 id="样本集的生成模型："><a href="# 样本集的生成模型：" class="headerlink" title="样本集的生成模型："></a>样本集的生成模型：</h5><p>假设样本集 <script type="math/tex">D= \lbrace x_{1}, x_{2}, \ldots, x_{n} \rbrace</script> 的生成过程由高斯混合分布给出：</p><ul><li><p>首先：根据 <script type="math/tex">\alpha_{1}, \alpha_{2}, \ldots,\alpha_{k}</script> 定义的先验分布选择高斯混合成分，其中 <script type="math/tex">\alpha_{i}</script> 为选择第 $i$ 个混合成分的概率；</p></li><li><p>然后：根据被选择的混合成分的概率密度函数进行采样， 生成相应的样本。</p></li></ul><p>令随机变量 <script type="math/tex">z_{j} \in \lbrace 1, 2, \ldots,k \rbrace</script> 表示生成样本 <script type="math/tex">x_{j}</script> 的高斯混合成分， 其取值未知。</p><p>$z_{j}$ 的先验概率：</p><script type="math/tex; mode=display">P(z{j}=i)=\alpha{i} \quad (i=1, 2, \ldots,k).</script><p>由 Bayesian 定理得 $z_{j}$ 的后验分布为：</p><script type="math/tex; mode=display">\begin{align*}P_{\mathcal{M}}(z_{j}=i|x_{j}) &=\frac{P(z_{j}=i) \cdot p_{\mathcal{M}}(x_{j}|z_{j}=i)}{p_{\mathcal{M}}(x_{j})} \cr& =\frac{\alpha_{i}\cdot p(x_{j}|\mu_{i},\Sigma_{i})}{\sum^{k}_{l=1}\alpha_{l}\cdot p(x_{j}|\mu_{l},\Sigma_{l})}\end{align*}</script><p>可知，<script type="math/tex">P_{\mathcal{M}}(z_{j}=i|x_{j})</script> 给出了样本 <script type="math/tex">x_{j}</script> 由第 $i$ 个高斯混合成分生成的后验概率，记：</p><script type="math/tex; mode=display">\gamma_{ji}=P_{\mathcal{M}}(z_{j}=i|x_{j}) \quad (i=1, 2, \ldots,k)</script><h5 id="高斯混合聚类策略："><a href="# 高斯混合聚类策略：" class="headerlink" title="高斯混合聚类策略："></a>高斯混合聚类策略：</h5><ul><li>若（多元）高斯混合分布 $p_{\mathcal{M}}(x)$ 已知，高斯混合聚类将把样本集 $D$ 划分为 $k$ 个簇：<script type="math/tex; mode=display">C= \lbrace C_{1}, C_{2}, \ldots,C_{k} \rbrace</script></li></ul><p>每个样本 <script type="math/tex">x_{j}</script> 的簇标记 <script type="math/tex">\lambda_{j}</script> 为:</p><script type="math/tex; mode=display">\lambda_{j}=arg \underset{i \in {1, 2, \ldots,k}}{max}\gamma_{ji}</script><ul><li>（多元）高斯混合分布 <script type="math/tex">p_{\mathcal{M}}(x)</script> 参数 <script type="math/tex">\lbrace (\alpha_{i},\mu_{i}, \Sigma_{i})|1 \leqslant i \leqslant k \rbrace</script> 的求解采用极大似然估计(MLE):</li></ul><p>给定样本集 $D$， 最大化（对数）似然函数：</p><script type="math/tex; mode=display">\begin{align*}LL(D) & = ln\Big(\prod^{n}_{j=1}p_{\mathcal{M}}(x_{j})\Big) \cr&=\sum^{n}_{j=1}\Big(\sum^{k}_{i=1}\alpha_{i}\cdot p(x_{j}|\mu_{i}, \Sigma_{i})\Big)\end{align*}</script><p>MLE 解为：</p><script type="math/tex; mode=display">\begin{align}\mu_{i} & = \frac{\sum^{n}_{j=1}\gamma_{ji}x_{j}}{\sum^{n}_{j=1}\gamma{ji}} \cr\Sigma_{i} & = \frac{\sum^{n}_{j=1}\gamma_{ji}(x_{j}-\mu_{i})(x_{j}-\mu_{i})^{T}}{\sum^{n}_{j=1}\gamma_{ji}} \cr\alpha_{i} &= \frac{1}{n}\sum^{n}_{j=1}\gamma_{ji}\end{align}</script><p>具体算法如下：<br><img src="https://farm5.staticflickr.com/4295/35824129960_2e09d0a8b8_o.png" alt="高斯混合聚类算法"></p><h4 id="算法实现 -1"><a href="# 算法实现 -1" class="headerlink" title="算法实现"></a>算法实现</h4><p>（Unfinished）</p><hr><h1 id="各聚类算法过程"><a href="# 各聚类算法过程" class="headerlink" title="各聚类算法过程"></a><strong>各聚类算法过程 </strong></h1><p> 使用西瓜数据集合：</p><script type="math/tex; mode=display">\begin{array}{ccc|ccc|ccc}\hline编号 & 密度 & 含糖率 & 编号 & 密度 & 含糖率 & 编号 & 密度 & 含糖率 \cr\hline1 & 0.697 & 0.460 & 11 & 0.245 & 0.057 & 21 & 0.748 & 0.232 \cr2 & 0.774 & 0.376 & 12 & 0.343 & 0.099 & 22 & 0.714 & 0.346 \cr3 & 0.634 & 0.264 & 13 & 0.639 & 0.161 & 23 & 0.483 & 0.312 \cr4 & 0.608 & 0.318 & 14 & 0.657 & 0.198 & 24 & 0.478 & 0.437 \cr5 & 0.556 & 0.215 & 15 & 0.360 & 0.370 & 25 & 0.525 & 0.369 \cr6 & 0.403 & 0.237 & 16 & 0.593 & 0.042 & 26 & 0.751 & 0.489 \cr7 & 0.481 & 0.149 & 17 & 0.719 & 0.103 & 27 & 0.532 & 0.472 \cr8 & 0.437 & 0.211 & 18 & 0.359 & 0.188 & 28 & 0.473 & 0.376 \cr9 & 0.666 & 0.091 & 19 & 0.339 & 0.241 & 29 & 0.725 & 0.445 \cr10 & 0.243 & 0.267 & 20 & 0282 & 0.257 & 30 & 0.446 & 0.459 \cr\end{array}</script><p>（其中编号为 9~21 的类别是”坏瓜”，其他样本的类别是”好瓜”） </p><h2 id="使用 -k- 均值算法"><a href="# 使用 -k- 均值算法" class="headerlink" title="使用 $k$ 均值算法"></a>使用 $k$ 均值算法 </h2><p> 假定聚类簇数 $k=3$，算法开始时随机选取的三个样本 <script type="math/tex">x_{6},x_{12},x_{27}</script> 作为初始均值向量，即：</p><script type="math/tex; mode=display">\mu_{1} = (0.403;0.237), \mu_{2} = (0.343;0.099),\mu_{3} = (0.532;0.472).</script><p>考察样本 <script type="math/tex">x_{1}=(0.697;0.460)</script>，它与当前均值向量 <script type="math/tex">\mu_{1},\mu_{2},\mu_{3}</script>的距离分别是<script type="math/tex">0.369, 0.506, 0.166</script>，因此 <script type="math/tex">x_{1}</script> 将被划入簇 <script type="math/tex">C_{3}</script> 中。类似的，对数据集中的所有样本考察一遍后，可得当前簇划分为：</p><script type="math/tex; mode=display">\begin{align}C_{1} & = \lbrace x_{5},x_{6},x_{7},x_{8},x_{9},x_{10},x_{13},x_{14},x_{15},x_{17},x_{18},x_{19},x_{20},x_{23} \rbrace; \crC_{2} & = \lbrace x_{11},x_{12},x_{16} \rbrace; \crC_{3} & = \lbrace x_{1},x_{2},x_{3},x_{4},x_{21},x_{22},x_{24},x_{25},x_{26},x_{27},x_{28},x_{29},x_{30} \rbrace\end{align}</script><p>于是，可以从 <script type="math/tex">C_{1},C_{2},C_{3}</script> 分别求出新的均值向量：</p><script type="math/tex; mode=display">\mu_{1}^{'} = (0.473;0.214), \mu_{2}^{'} = (0.394;0.066),\mu_{3}^{'} = (0.623;0.388).</script><p>更新当前均值向量后，不断重复上述过程，如图所示，第五轮迭代产生的结果与第四轮迭代结果相同，于是算法停止，得到最终的簇划分，其中样本点与均值向量分别用”●”与”+”表示，红色虚线显示出簇划分。</p><p><img src="https://farm1.staticflickr.com/403/30990399723_0db34a3cde_o.png" alt></p><h2 id="使用学习向量量化"><a href="# 使用学习向量量化" class="headerlink" title="使用学习向量量化"></a>使用学习向量量化 </h2><p> 令数据集中编号为 9-21 的样本的类别标记为 <script type="math/tex">c_{2}</script> ，其他样本的类别标记为 <script type="math/tex">c_{1}</script>。假定 $q=5$，即学习目标是找到 5 个原型向量 <script type="math/tex">p_{1},p_{2},p_{3},p_{4},p_{5}</script>，并假定其对应的类别标记分别为<script type="math/tex">c_{1},c_{2},c_{2},c_{1},c_{1}</script> 。</p><p>算法开始时，根据样本的类别标记和簇的预设类别标记，对原型向量进行随机初始化，假定初始化为样本 <script type="math/tex">x_{5},x_{12},x_{18},x_{23},x_{29}</script> 。在第一轮迭代中，假定随机选取的样本为 <script type="math/tex">x_{1}</script> ，该样本与当前原型向量 <script type="math/tex">p_{1},p_{2},p_{3},p_{4},p_{5}</script> 的距离分别为 <script type="math/tex">0.283, 0.506, 0.434, 0.260, 0.032</script> 。由于 <script type="math/tex">p_{5}</script> 与 <script type="math/tex">x_{1}</script> 距离最近且两者具有相同的类别标记 <script type="math/tex">c_{2}</script> ，假定学习率 $\eta = 0.1$ ，则 LVQ 更新 <script type="math/tex">p_{5}</script> 得到新原型向量：</p><script type="math/tex; mode=display">\begin{align}p^{'} & = p_{5} + \eta \cdot (x_{1} - p_{5}) \cr      & = (0.725;0.445) + 0.1 \cdot ((0.697;0.460) - (0.725;0.445)) \cr      & = (0.722;0.442).\end{align}</script><p>将 <script type="math/tex">p_{5}</script> 更新为 <script type="math/tex">p^{'}</script> 后，不断重复上述过程，不同轮数之后的聚类结果如下图所示，其中 <script type="math/tex">c_{1}</script> ， <script type="math/tex">c_{2}</script> 类别样本点与原型向量分别用”●”,”○”与”+”表示，红色虚线显示出聚类形成的 Voronoi 剖分。</p><p><img src="https://farm1.staticflickr.com/715/31427609470_fe9357f0cc_o.png" alt></p><h2 id="使用高斯混合聚类"><a href="# 使用高斯混合聚类" class="headerlink" title="使用高斯混合聚类"></a>使用高斯混合聚类 </h2><p> 令高斯混合成分的个数 $k=3$。算法开始时，假定将高斯混合分布的模型参数初始化为：<script type="math/tex">\alpha_{1} = \alpha_{2} = \alpha_{3} = \frac{1}{3}</script>；<script type="math/tex">\mu_{1} = x_{6}</script>，<script type="math/tex">\mu_{2}=x_{22}</script>，<script type="math/tex">\mu_{3} = x_{27}</script>；<script type="math/tex">\Sigma_{1} = \Sigma_{2} = \Sigma_{3} = \binom{0.1 \ 0.0}{0.0 \ 0.1}</script>。</p><p>在第一轮迭代中，先计算样本由各混合成分生成的后验概率。</p>]]></content>
    
    <summary type="html">
    
      本文是关于周志华「Machine Learning」这本书的 Clustering 的学习笔记。
    
    </summary>
    
      <category term="Books" scheme="http://randolph.pro/categories/Books/"/>
    
      <category term="Book:「Machine Learning」" scheme="http://randolph.pro/categories/Books/Book-%E3%80%8CMachine-Learning%E3%80%8D/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Books" scheme="http://randolph.pro/tags/Books/"/>
    
  </entry>
  
</feed>
