<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>黃某人</title>
  <subtitle>痴</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://randolph.pro/"/>
  <updated>2017-08-08T13:33:21.000Z</updated>
  <id>http://randolph.pro/</id>
  
  <author>
    <name>Randolph</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>「TensorFlow」 Use WeChat to Monitor Your Network</title>
    <link href="http://randolph.pro/2017/03/17/%E3%80%8CTensorFlow%E3%80%8DUse%20WeChat%20to%20Monitor%20Your%20Network/"/>
    <id>http://randolph.pro/2017/03/17/「TensorFlow」Use WeChat to Monitor Your Network/</id>
    <published>2017-03-16T16:00:00.000Z</published>
    <updated>2017-08-08T13:33:21.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4399/36206153111_6662041dd1_o.png" alt=""></p>
<p> 有关「TensorFlow」的其他学习笔记系列：<a href="http://randolph.pro/categories/TensorFlow/">「TensorFlow」</a></p>
<p> 大概的效果是：</p>
<p><img src="https://farm4.staticflickr.com/3767/32574547714_59711d3f0b_o.jpg" alt=""></p>
<p> 程序用到的主角是 Python 中的微信个人号接口 <strong>itchat</strong>。<a href="https://itchat.readthedocs.io/zh/latest/" target="_blank" rel="external">What’s itchat?</a> （itchat 的介绍及安装过程）</p>
<p> 这次，我们要监控的模型是先前提到过的 <a href="http://www.jianshu.com/p/b5caf4d5ce3e" target="_blank" rel="external"> 基于 MNIST 手写体数据集的「CNN」模型 </a>。</p>
<p> 注意：</p>
<ol>
<li> 文章要求读者事先下载安装好 itchat。</li>
<li> 文章不会详细介绍 TensorFlow 以及 Tensorboard 的知识。</li>
</ol>
<h2 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h2><p><strong>OS: macOS Sierra 10.12.x</strong></p>
<p><strong>Python Version: 3.4.x</strong></p>
<p><strong>TensorFlow: 1.0</strong></p>
<p><strong>itchat: 1.2.3</strong></p>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><ul>
<li>Use WeChat to Monitor Your Network（tensorboard 绘图）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 基于 MNIST 数据集 的 「CNN」（tensorboard 绘图）</span></div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> scipy</div><div class="line"></div><div class="line"><span class="comment"># Import itchat &amp; threading</span></div><div class="line"><span class="keyword">import</span> itchat</div><div class="line"><span class="keyword">import</span> threading</div><div class="line"></div><div class="line"><span class="comment"># Create a running status flag</span></div><div class="line">lock = threading.Lock()</div><div class="line">running = <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="comment"># Parameters</span></div><div class="line">learning_rate = <span class="number">0.001</span></div><div class="line">training_iters = <span class="number">200000</span></div><div class="line">batch_size = <span class="number">128</span></div><div class="line">display_step = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></div><div class="line">	initial = tf.truncated_normal(shape, stddev = <span class="number">0.1</span>)</div><div class="line">	<span class="keyword">return</span> tf.Variable(initial)</div><div class="line">	</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></div><div class="line">	initial = tf.constant(<span class="number">0.1</span>, shape = shape)</div><div class="line">	<span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W, strides=<span class="number">1</span>)</span>:</span></div><div class="line">	<span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, strides, strides, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x, k=<span class="number">2</span>)</span>:</span></div><div class="line">	<span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, k, k, <span class="number">1</span>], strides=[<span class="number">1</span>, k, k, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">	</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_summaries</span><span class="params">(var)</span>:</span></div><div class="line">	<span class="string">"""Attach a lot of summaries to a Tensor (for TensorBoard visualization)."""</span></div><div class="line">	<span class="keyword">with</span> tf.name_scope(<span class="string">'summaries'</span>):</div><div class="line">		mean = tf.reduce_mean(var)</div><div class="line">		tf.summary.scalar(<span class="string">'mean'</span>, mean)</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'stddev'</span>):</div><div class="line">			stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))</div><div class="line">		tf.summary.scalar(<span class="string">'stddev'</span>, stddev)</div><div class="line">		tf.summary.scalar(<span class="string">'max'</span>, tf.reduce_max(var))</div><div class="line">		tf.summary.scalar(<span class="string">'min'</span>, tf.reduce_min(var))</div><div class="line">		tf.summary.histogram(<span class="string">'histogram'</span>, var)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(input_tensor, weights_shape, biases_shape, layer_name, act = tf.nn.relu, flag = <span class="number">1</span>)</span>:</span></div><div class="line">	<span class="string">"""Reusable code for making a simple neural net layer.</span></div><div class="line"></div><div class="line">	It does a matrix multiply, bias add, and then uses relu to nonlinearize.</div><div class="line">	It also sets up name scoping so that the resultant graph is easy to read,</div><div class="line">	and adds a number of summary ops.</div><div class="line">"""</div><div class="line">	<span class="keyword">with</span> tf.name_scope(layer_name):</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</div><div class="line">			weights = weight_variable(weights_shape)</div><div class="line">			variable_summaries(weights)</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</div><div class="line">			biases = bias_variable(biases_shape)</div><div class="line">			variable_summaries(biases)</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</div><div class="line">			<span class="keyword">if</span> flag == <span class="number">1</span>:</div><div class="line">				preactivate = tf.add(conv2d(input_tensor, weights), biases)</div><div class="line">			<span class="keyword">else</span>:</div><div class="line">				preactivate = tf.add(tf.matmul(input_tensor, weights), biases)</div><div class="line">			tf.summary.histogram(<span class="string">'pre_activations'</span>, preactivate)</div><div class="line">		<span class="keyword">if</span> act == <span class="keyword">None</span>:</div><div class="line">			outputs = preactivate</div><div class="line">		<span class="keyword">else</span>:</div><div class="line">			outputs = act(preactivate, name = <span class="string">'activation'</span>)</div><div class="line">			tf.summary.histogram(<span class="string">'activation'</span>, outputs)</div><div class="line">		<span class="keyword">return</span> outputs</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_train</span><span class="params">(wechat_name, param)</span>:</span></div><div class="line">	<span class="keyword">global</span> lock, running</div><div class="line">	<span class="comment"># Lock</span></div><div class="line">	<span class="keyword">with</span> lock:</div><div class="line">		running = <span class="keyword">True</span>	</div><div class="line">	<span class="comment"># 参数 </span></div><div class="line">	learning_rate, training_iters, batch_size, display_step = param</div><div class="line">	</div><div class="line">	<span class="comment"># Import data</span></div><div class="line">	mnist_data_path = <span class="string">'MNIST_data/'</span></div><div class="line">	mnist = input_data.read_data_sets(mnist_data_path, one_hot = <span class="keyword">True</span>)</div><div class="line">	</div><div class="line">	<span class="comment"># Network Parameters</span></div><div class="line">	n_input = <span class="number">28</span>*<span class="number">28</span> <span class="comment"># MNIST data input (img shape: 28*28)</span></div><div class="line">	n_classes = <span class="number">10</span> <span class="comment"># MNIST total classes (0-9 digits)</span></div><div class="line">	dropout = <span class="number">0.75</span> <span class="comment"># Dropout, probability to keep units</span></div><div class="line">	</div><div class="line">	<span class="keyword">with</span> tf.name_scope(<span class="string">'Input'</span>):</div><div class="line">		x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_input], name = <span class="string">'input_x'</span>)</div><div class="line">		y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_classes], name = <span class="string">'target_y'</span>)</div><div class="line">		keep_prob = tf.placeholder(tf.float32, name = <span class="string">'keep_prob'</span>) <span class="comment">#dropout (keep probability)</span></div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">cnn_net</span><span class="params">(x, weights, biases, dropout)</span>:</span></div><div class="line">		<span class="comment"># Reshape input picture</span></div><div class="line">		x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span> ,<span class="number">1</span>])</div><div class="line">		</div><div class="line">		<span class="comment"># First Convolutional Layer</span></div><div class="line">		conv_1 = add_layer(x_image, weights[<span class="string">'conv1_w'</span>], biases[<span class="string">'conv1_b'</span>], <span class="string">'First_Convolutional_Layer'</span>, flag = <span class="number">1</span>)</div><div class="line">		</div><div class="line">		<span class="comment"># First Pooling Layer</span></div><div class="line">		pool_1 = max_pool_2x2(conv_1)</div><div class="line">		</div><div class="line">		<span class="comment"># Second Convolutional Layer </span></div><div class="line">		conv_2 = add_layer(pool_1, weights[<span class="string">'conv2_w'</span>], biases[<span class="string">'conv2_b'</span>], <span class="string">'Second_Convolutional_Layer'</span>, flag = <span class="number">1</span>)</div><div class="line"></div><div class="line">		<span class="comment"># Second Pooling Layer </span></div><div class="line">		pool_2 = max_pool_2x2(conv_2)</div><div class="line"></div><div class="line">		<span class="comment"># Densely Connected Layer</span></div><div class="line">		pool_2_flat = tf.reshape(pool_2, [<span class="number">-1</span>, weight_variable(weights[<span class="string">'dc1_w'</span>]).get_shape().as_list()[<span class="number">0</span>]])</div><div class="line">		dc_1 = add_layer(pool_2_flat, weights[<span class="string">'dc1_w'</span>], biases[<span class="string">'dc1_b'</span>], <span class="string">'Densely_Connected_Layer'</span>, flag = <span class="number">0</span>) </div><div class="line">		</div><div class="line">		<span class="comment"># Dropout</span></div><div class="line">		dc_1_drop = tf.nn.dropout(dc_1, keep_prob)	</div><div class="line">		</div><div class="line">		<span class="comment"># Readout Layer</span></div><div class="line">		y = add_layer(dc_1_drop, weights[<span class="string">'out_w'</span>], biases[<span class="string">'out_b'</span>], <span class="string">'Readout_Layer'</span>, flag = <span class="number">0</span>)</div><div class="line">		</div><div class="line">		<span class="keyword">return</span> y</div><div class="line">	</div><div class="line">	<span class="comment"># Store layers weight &amp; bias</span></div><div class="line">	weights = &#123;</div><div class="line">		<span class="comment"># 5x5 conv, 1 input, 32 outputs</span></div><div class="line">		<span class="string">'conv1_w'</span>: [<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>],</div><div class="line">		<span class="comment"># 5x5 conv, 32 inputs, 64 outputs</span></div><div class="line">		<span class="string">'conv2_w'</span>: [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>],</div><div class="line">		<span class="comment"># fully connected, 7*7*64 inputs, 1024 outputs</span></div><div class="line">		<span class="string">'dc1_w'</span>: [<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>, <span class="number">1024</span>],</div><div class="line">		<span class="comment"># 1024 inputs, 10 outputs (class prediction)</span></div><div class="line">		<span class="string">'out_w'</span>: [<span class="number">1024</span>, n_classes]</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	biases = &#123;</div><div class="line">		<span class="string">'conv1_b'</span>: [<span class="number">32</span>],</div><div class="line">		<span class="string">'conv2_b'</span>: [<span class="number">64</span>],</div><div class="line">		<span class="string">'dc1_b'</span>: [<span class="number">1024</span>],</div><div class="line">		<span class="string">'out_b'</span>: [n_classes]</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	y = cnn_net(x, weights, biases, dropout)</div><div class="line">	</div><div class="line">	<span class="comment"># Optimizer</span></div><div class="line">	<span class="keyword">with</span> tf.name_scope(<span class="string">'cost'</span>):</div><div class="line">		cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_,</div><div class="line">						logits = y))</div><div class="line">		tf.summary.scalar(<span class="string">'cost'</span>, cost)</div><div class="line">		tf.summary.histogram(<span class="string">'cost'</span>, cost)</div><div class="line">	</div><div class="line">	<span class="comment"># Train</span></div><div class="line">	<span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</div><div class="line">		optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</div><div class="line">	</div><div class="line">	<span class="comment"># Test</span></div><div class="line">	<span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'correct_prediction'</span>):</div><div class="line">			correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</div><div class="line">			accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">		tf.summary.scalar(<span class="string">'accuracy'</span>, accuracy)</div><div class="line">		</div><div class="line">	sess = tf.InteractiveSession()</div><div class="line">	merged = tf.summary.merge_all()</div><div class="line">	train_writer = tf.summary.FileWriter(<span class="string">'train/'</span>, sess.graph)</div><div class="line">	test_writer = tf.summary.FileWriter(<span class="string">'test/'</span>)</div><div class="line">	tf.global_variables_initializer().run()</div><div class="line"></div><div class="line">	</div><div class="line">	<span class="comment"># Train the model, and also write summaries.</span></div><div class="line">	<span class="comment"># Every 10th step, measure test-set accuracy, and write test summaries</span></div><div class="line">	<span class="comment"># All other steps, run train_step on training data, &amp; add training summaries</span></div><div class="line">	</div><div class="line">	<span class="comment"># Keep training until reach max iterations</span></div><div class="line">	print(<span class="string">'Wait for lock'</span>)</div><div class="line">	<span class="keyword">with</span> lock:</div><div class="line">		run_state = running</div><div class="line">	print(<span class="string">'Start'</span>)</div><div class="line">	</div><div class="line">	step = <span class="number">1</span></div><div class="line">	<span class="keyword">while</span> step * batch_size &lt; training_iters <span class="keyword">and</span> run_state:</div><div class="line">		batch_x, batch_y = mnist.train.next_batch(batch_size)</div><div class="line">		<span class="comment"># Run optimization op (backprop)</span></div><div class="line">		sess.run(optimizer, feed_dict = &#123;x: batch_x, y_: batch_y, keep_prob: dropout&#125;)</div><div class="line">		<span class="keyword">if</span> step % display_step == <span class="number">0</span>:	<span class="comment"># Record execution stats</span></div><div class="line">			run_options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE)</div><div class="line">			run_metadata = tf.RunMetadata()</div><div class="line">			summary, _ = sess.run([merged, optimizer], feed_dict = </div><div class="line">									&#123;x: batch_x, y_: batch_y, keep_prob: <span class="number">1.</span>&#125;, </div><div class="line">									options = run_options, run_metadata = run_metadata)</div><div class="line">			train_writer.add_run_metadata(run_metadata, <span class="string">'step %d'</span> % step)</div><div class="line">			train_writer.add_summary(summary, step)</div><div class="line">			print(<span class="string">'Adding run metadata for'</span>, step)</div><div class="line"></div><div class="line">			summary, loss, acc = sess.run([merged, cost, accuracy], feed_dict = </div><div class="line">											&#123;x: batch_x, y_: batch_y, keep_prob: <span class="number">1.</span>&#125;)</div><div class="line">			print(<span class="string">"Iter"</span> + str(step*batch_size) + <span class="string">", Minibatch Loss="</span> + \</div><div class="line">				<span class="string">"&#123;:.6f&#125;"</span>.format(loss) + <span class="string">", Training Accuracy="</span> + \</div><div class="line">				<span class="string">"&#123;:.5f&#125;"</span>.format(acc))</div><div class="line">			itchat.send(<span class="string">"Iter"</span> + str(step*batch_size) + <span class="string">", Minibatch Loss="</span> + \</div><div class="line">				<span class="string">"&#123;:.6f&#125;"</span>.format(loss) + <span class="string">", Training Accuracy="</span> + \</div><div class="line">						<span class="string">"&#123;:.5f&#125;"</span>.format(acc), <span class="string">'filehelper'</span>)</div><div class="line">		<span class="keyword">else</span>:</div><div class="line">			summary, _ = sess.run([merged, optimizer], feed_dict = &#123;x: batch_x, y_: batch_y, keep_prob: <span class="number">1.</span>&#125;)</div><div class="line">			train_writer.add_summary(summary, step)</div><div class="line">		step += <span class="number">1</span></div><div class="line">		<span class="keyword">with</span> lock:</div><div class="line">			run_state = running</div><div class="line">	print(<span class="string">"Optimization Finished!"</span>)</div><div class="line">	itchat.send(<span class="string">"Optimization Finished!"</span>, <span class="string">'filehelper'</span>)</div><div class="line"></div><div class="line">	<span class="comment"># Calculate accuracy for 256 mnist test images</span></div><div class="line">	summary, acc = sess.run([merged, accuracy], feed_dict = </div><div class="line">							&#123;x: mnist.test.images[:<span class="number">256</span>], y_: mnist.test.labels[:<span class="number">256</span>], </div><div class="line">							keep_prob: <span class="number">1.</span>&#125; )</div><div class="line">	text_writer.add_summary(summary)</div><div class="line">	print(<span class="string">"Testing Accuracy:"</span>, acc)</div><div class="line">	itchat.send(<span class="string">"Testing Accuracy: %s"</span> % acc, wechat_name)</div><div class="line"></div><div class="line">				</div><div class="line"><span class="meta">@itchat.msg_register([itchat.content.TEXT])</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chat_trigger</span><span class="params">(msg)</span>:</span></div><div class="line">	<span class="keyword">global</span> lock, running, learning_rate, training_iters, batch_size, display_step</div><div class="line">	<span class="keyword">if</span> msg[<span class="string">'Text'</span>] == <span class="string">u'开始'</span>:</div><div class="line">		print(<span class="string">'Starting'</span>)</div><div class="line">		<span class="keyword">with</span> lock:</div><div class="line">			run_state = running</div><div class="line">		<span class="keyword">if</span> <span class="keyword">not</span> run_state:</div><div class="line">			<span class="keyword">try</span>:</div><div class="line">				threading.Thread(target=nn_train, args=(msg[<span class="string">'FromUserName'</span>], (learning_rate, training_iters, batch_size, display_step))).start()</div><div class="line">			<span class="keyword">except</span>:</div><div class="line">				msg.reply(<span class="string">'Running'</span>)</div><div class="line">	<span class="keyword">elif</span> msg[<span class="string">'Text'</span>] == <span class="string">u'停止'</span>:</div><div class="line">		print(<span class="string">'Stopping'</span>)</div><div class="line">		<span class="keyword">with</span> lock:</div><div class="line">			running = <span class="keyword">False</span></div><div class="line">	<span class="keyword">elif</span> msg[<span class="string">'Text'</span>] == <span class="string">u'参数'</span>:</div><div class="line">		itchat.send(<span class="string">'lr=%f, ti=%d, bs=%d, ds=%d'</span>%(learning_rate, training_iters, batch_size, display_step),msg[<span class="string">'FromUserName'</span>])</div><div class="line">	<span class="keyword">else</span>:</div><div class="line">		<span class="keyword">try</span>:</div><div class="line">			param = msg[<span class="string">'Text'</span>].split()</div><div class="line">			key, value = param</div><div class="line">			print(key, value)</div><div class="line">			<span class="keyword">if</span> key == <span class="string">'lr'</span>:</div><div class="line">				learning_rate = float(value)</div><div class="line">			<span class="keyword">elif</span> key == <span class="string">'ti'</span>:</div><div class="line">				training_iters = int(value)</div><div class="line">			<span class="keyword">elif</span> key == <span class="string">'bs'</span>:</div><div class="line">				batch_size = int(value)</div><div class="line">			<span class="keyword">elif</span> key == <span class="string">'ds'</span>:</div><div class="line">				display_step = int(value)</div><div class="line">		<span class="keyword">except</span>:</div><div class="line">			<span class="keyword">pass</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">	itchat.auto_login(hotReload=<span class="keyword">True</span>)</div><div class="line">	itchat.run()</div></pre></td></tr></table></figure>
<p> 大家可以看到，我对先前的代码进行了一些修改。</p>
<p> 下面我会对代码中用到 itchat 的部分进行一些简短的说明。</p>
<ul>
<li> 代码部分截图：</li>
</ul>
<p><img src="https://farm4.staticflickr.com/3687/33417461155_a448845f98_o.png" alt=""></p>
<p> 说明：</p>
<ol>
<li> 首先我导入了 itchat 和 threading。</li>
<li> 在原先所有 <strong><code>print</code></strong> 消息的地方，都添加了 <strong><code>itchat.send()</code></strong> 来输出我们的模型训练日志。</li>
<li> 加了一个带锁的状态量 <strong><code>running</code></strong> 用来做为发送微信消息的运行开关。</li>
<li> 写了一个 itchat 的 handler（就是上图）。其作用就是当程序运行，我们需要在微信中，对自己的微信号发送「开始」，模型才会开始训练，为了防止信息阻塞，所以要用到 <strong><code>threading</code></strong> 将其放在另一个线程当中。在训练的过程中，如果我们觉得结果已到达我们自己的预期，可以微信发送「停止」来停止模型的训练过程。</li>
</ol>
<p><strong> 另外，脚本刚开始运行时，程序会弹出一个包含二维码的图片，我们需要通过微信来扫描该二维码，来登陆微信并启动 itchat 的服务。</strong></p>
<p> 程序是包含了 Tensorboard 绘图的，所以等模型训练好，我们依然是可以通过 Tensorboard 来更加详细地查看我们模型的训练过程。 </p>
<p> 至此，我们就可以一边通过微信来监控我们的模型训练过程，一边与身边的朋友们谈笑风生了。</p>
<p> 如果看过 itchat 那个连接的读者，可以了解到 itchat 同样是可以发送图片信息的，所以我们可以写额外的脚本在训练的过程中每隔 100 次迭代， plot 到目前为止 loss，acc 等指标的趋势图。在此，我就不再进行拓展了。</p>
<p> 关于各个模块的作用，以及各个变量的意义，我在此就不再赘述了。</p>
<p> 如果有读者对于 CNN 卷积神经网络有些陌生或者是遗忘，可以参考我的另外一篇文章 <a href="http://www.jianshu.com/p/95c79381ab4f" target="_blank" rel="external">CNN on TensorFlow</a>。</p>
<p> 如果读者对 Tensorboard 有所遗忘，可以参考我的另一篇文章 <a href="http://randolph.pro/2017/03/07/「TensorFlow」%20Tensorboard/">「TensorFlow」 Tensorboard</a>。</p>
]]></content>
    
    <summary type="html">
    
      平时，大家自己的机器模型在训练期间（特别是深度网络），训练时间通常几小时到十几小时不等，甚至可能会花上好几天，那么在这段时间，你们又会干些什么事情呢？作为程序员，这里提供一个「有趣的」方式，用你的微信来监控你的模型在训练期间的一举一动。
    
    </summary>
    
      <category term="TensorFlow" scheme="http://randolph.pro/categories/TensorFlow/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
      <category term="TensorFlow" scheme="http://randolph.pro/tags/TensorFlow/"/>
    
      <category term="WeChat" scheme="http://randolph.pro/tags/WeChat/"/>
    
  </entry>
  
  <entry>
    <title>「TensorFlow」 Tensorboard</title>
    <link href="http://randolph.pro/2017/03/13/%E3%80%8CTensorFlow%E3%80%8D%20Tensorboard/"/>
    <id>http://randolph.pro/2017/03/13/「TensorFlow」 Tensorboard/</id>
    <published>2017-03-12T16:00:00.000Z</published>
    <updated>2017-08-08T13:34:13.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4399/36206153111_6662041dd1_o.png" alt=""></p>
<p>有关「TensorFlow」的其他学习笔记系列：<a href="http://randolph.pro/categories/TensorFlow/">「TensorFlow」</a></p>
<h1 id="TensorFlow-Tensorboard"><a href="#TensorFlow-Tensorboard" class="headerlink" title="TensorFlow Tensorboard"></a>TensorFlow Tensorboard</h1><p>Tensorboard 可以看做是我们构建的 Graph 的可视化工具，对于我们初学者理解网络架构、每层网络的细节都是很有帮助的。由于前几天刚接触 TensorFlow，所以在尝试学习 Tensorboard 的过程中，遇到了一些问题。在此基础上，参考了 TensorFlow 官方的 Tensorboard Tutorials 以及网上的一些文章。由于前不久 TensorFlow 1.0 刚发布，网上的一些学习资源或者是 tensorboard 代码在新的版本中并不适用，所以自己改写并实现了官方网站上提及的三个实例的 Tensorboard 版本：</p>
<ol>
<li>最基础简单的「Linear Model」</li>
<li>基于 MNIST 手写体数据集的 「Softmax Regression」模型</li>
<li>基于 MNIST 手写体数据集的「CNN」模型</li>
</ol>
<p>文章不会详细介绍 TensorFlow 以及 Tensorboard 的知识，主要是模型的代码以及部分模型实验截图。</p>
<p>注意：文章前提默认读者们知晓 TensorFlow，知晓 Tensorboard，以及 TensorFlow 的一些主要概念<strong><code>Variables</code></strong>、<strong><code>Placeholder</code></strong>。还有，默认你已经将需要用到的 MNIST 数据集下载到了你代码当前所在文件夹。</p>
<h2 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h2><p><strong>OS: macOS Sierra 10.12.x</strong></p>
<p><strong>Python Version: 3.x</strong></p>
<p><strong>TensorFlow: 1.0</strong></p>
<h2 id="Tensorboard"><a href="#Tensorboard" class="headerlink" title="Tensorboard"></a>Tensorboard</h2><p>Tensorboard 有几大模块：</p>
<ul>
<li><strong>SCALARS</strong>：记录单一变量的，使用 <strong><code>tf.summary.scalar()</code></strong> 收集构建。</li>
<li><strong>IMAGES</strong>：收集的图片数据，当我们使用的数据为图片时（选用）。</li>
<li><strong>AUDIO</strong>：收集的音频数据，当我们使用数据为音频时（选用）。</li>
<li><strong>GRAPHS</strong>：构件图，效果图类似流程图一样，我们可以看到数据的流向，使用 <strong><code>tf.name_scope()</code></strong> 收集构建。</li>
<li><strong>DISTRIBUTIONS</strong>：用于查看变量的分布值，比如 W（Weights）变化的过程中，主要是在 0.5 附近徘徊。</li>
<li><strong>HISTOGRAMS</strong>：用于记录变量的历史值（比如 weights 值，平均值等），并使用折线图的方式展现，使用 <strong><code>tf.summary.histogram()</code></strong> 进行收集构建。</li>
</ul>
<h2 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h2><ul>
<li>最简单的线性回归模型（Tensorboard 绘图）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(layoutname, inputs, in_size, out_size, act = None)</span>:</span></div><div class="line">	<span class="keyword">with</span> tf.name_scope(layoutname):</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</div><div class="line">			weights = tf.Variable(tf.random_normal([in_size, out_size]), name = <span class="string">'weights'</span>)</div><div class="line">			w_hist = tf.summary.histogram(<span class="string">'weights'</span>, weights)</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</div><div class="line">			biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>, name = <span class="string">'biases'</span>)</div><div class="line">			b_hist = tf.summary.histogram(<span class="string">'biases'</span>, biases)</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</div><div class="line">			Wx_plus_b = tf.add(tf.matmul(inputs, weights), biases)</div><div class="line"></div><div class="line">		<span class="keyword">if</span> act <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">			outputs = Wx_plus_b</div><div class="line">		<span class="keyword">else</span> :</div><div class="line">			outputs = act(Wx_plus_b)</div><div class="line">		<span class="keyword">return</span> outputs</div><div class="line"></div><div class="line">x_data = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">300</span>)[:,np.newaxis]</div><div class="line">noise = np.random.normal(<span class="number">0</span>,<span class="number">0.05</span>, x_data.shape)</div><div class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Input'</span>):</div><div class="line">	xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>], name = <span class="string">"input_x"</span>)</div><div class="line">	ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>], name = <span class="string">"target_y"</span>)</div><div class="line"></div><div class="line"></div><div class="line">l1 = add_layer(<span class="string">"first_layer"</span>, xs, <span class="number">1</span>, <span class="number">10</span>, act = tf.nn.relu)</div><div class="line">l1_hist = tf.summary.histogram(<span class="string">'l1'</span>, l1)</div><div class="line"></div><div class="line">y = add_layer(<span class="string">"second_layout"</span>, l1, <span class="number">10</span>, <span class="number">1</span>, act = <span class="keyword">None</span>)</div><div class="line">y_hist = tf.summary.histogram(<span class="string">'y'</span>, y)</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'loss'</span>): </div><div class="line">	loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - y), </div><div class="line">							reduction_indices = [<span class="number">1</span>]))</div><div class="line">	tf.summary.histogram(<span class="string">'loss'</span>, loss)</div><div class="line">	tf.summary.scalar(<span class="string">'loss'</span>, loss)</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</div><div class="line">	train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</div><div class="line"></div><div class="line">init = tf.global_variables_initializer()</div><div class="line">merged = tf.summary.merge_all()</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">	fig = plt.figure()</div><div class="line">	ax = fig.add_subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</div><div class="line">	ax.scatter(x_data, y_data)</div><div class="line">	plt.ion()</div><div class="line">	plt.show()</div><div class="line">	</div><div class="line">	writer = tf.summary.FileWriter(<span class="string">'logs/'</span>, sess.graph)</div><div class="line">	sess.run(init)</div><div class="line">	</div><div class="line">	<span class="keyword">for</span> train <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">		sess.run(train_step, feed_dict = &#123;xs: x_data, ys: y_data&#125;)</div><div class="line">		<span class="keyword">if</span> train % <span class="number">50</span> == <span class="number">0</span>:</div><div class="line">			<span class="keyword">try</span>:</div><div class="line">				ax.lines.remove(lines[<span class="number">0</span>])</div><div class="line">			<span class="keyword">except</span> Exception:</div><div class="line">				<span class="keyword">pass</span></div><div class="line">			summary_str = sess.run(merged, feed_dict = &#123;xs: x_data, ys: y_data&#125;)</div><div class="line">			writer.add_summary(summary_str, train)</div><div class="line"></div><div class="line">			print(train, sess.run(loss, feed_dict = &#123;xs: x_data, ys: y_data&#125;))</div><div class="line">			</div><div class="line">			prediction_value = sess.run(y, feed_dict = &#123;xs: x_data&#125;)</div><div class="line">			lines = ax.plot(x_data, prediction_value, <span class="string">'r-'</span>, lw = <span class="number">5</span>)</div><div class="line">			plt.pause(<span class="number">1</span>)</div></pre></td></tr></table></figure>
<ul>
<li>基于 MNIST 手写体数据集的 「Softmax Regression」模型（Tensorboard 绘图）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 基于 MNIST 手写体数据集的 「Softmax Regression」模型</span></div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(layoutname, inputs, in_size, out_size, act = None)</span>:</span></div><div class="line">	<span class="keyword">with</span> tf.name_scope(layoutname):</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</div><div class="line">			weights = tf.Variable(tf.zeros([in_size, out_size]), name = <span class="string">'weights'</span>)</div><div class="line">			w_hist = tf.summary.histogram(<span class="string">"weights"</span>, weights)</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</div><div class="line">			biases = tf.Variable(tf.zeros(out_size), name = <span class="string">'biases'</span>)</div><div class="line">			b_hist = tf.summary.histogram(<span class="string">"biases"</span>, biases)</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</div><div class="line">			Wx_plus_b = tf.add(tf.matmul(inputs, weights), biases)</div><div class="line">		</div><div class="line">		<span class="keyword">if</span> act <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">			outputs = Wx_plus_b</div><div class="line">		<span class="keyword">else</span>:</div><div class="line">			outputs = act(Wx_plus_b)</div><div class="line">		<span class="keyword">return</span> outputs</div><div class="line">		</div><div class="line"><span class="comment"># Import data</span></div><div class="line">mnist_data_path = <span class="string">'MNIST_data/'</span></div><div class="line">mnist = input_data.read_data_sets(mnist_data_path, one_hot = <span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Input'</span>):</div><div class="line">	x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">28</span> * <span class="number">28</span>], name = <span class="string">'input_x'</span>)</div><div class="line">	y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>], name = <span class="string">'target_y'</span>)</div><div class="line"></div><div class="line">y = add_layer(<span class="string">"hidden_layout"</span>, x, <span class="number">28</span>*<span class="number">28</span>, <span class="number">10</span>, act = tf.nn.softmax)</div><div class="line">y_hist = tf.summary.histogram(<span class="string">'y'</span>, y)</div><div class="line"></div><div class="line"><span class="comment"># labels 真实值 logits 预测值</span></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'loss'</span>):</div><div class="line">	cross_entroy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_,</div><div class="line">					logits = y))</div><div class="line">	tf.summary.histogram(<span class="string">'cross entropy'</span>, cross_entroy)</div><div class="line">	tf.summary.scalar(<span class="string">'cross entropy'</span>, cross_entroy)</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</div><div class="line">	train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entroy)</div><div class="line"></div><div class="line"><span class="comment"># Test trained model</span></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'test'</span>):</div><div class="line">	correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</div><div class="line">	accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">	tf.summary.scalar(<span class="string">'accuracy'</span>, accuracy)</div><div class="line"></div><div class="line">init = tf.global_variables_initializer()</div><div class="line">merged = tf.summary.merge_all()</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">	<span class="comment">#logpath = r'/Users/randolph/PycharmProjects/TensorFlow/logs'</span></div><div class="line">	writer = tf.summary.FileWriter(<span class="string">'logs/'</span>, sess.graph)</div><div class="line">	sess.run(init)</div><div class="line"></div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">		<span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</div><div class="line">			feed = &#123;x: mnist.test.images, y_: mnist.test.labels&#125;</div><div class="line">			result = sess.run([merged, accuracy], feed_dict = feed)</div><div class="line">			summary_str = result[<span class="number">0</span>]</div><div class="line">			acc = result[<span class="number">1</span>]</div><div class="line">			writer.add_summary(summary_str, i)</div><div class="line">			print(i, acc)</div><div class="line">		<span class="keyword">else</span>:</div><div class="line">			batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">			feed = &#123;x: batch_xs, y_: batch_ys&#125;</div><div class="line">			sess.run(train_step, feed_dict = feed)</div><div class="line"></div><div class="line">	print(<span class="string">'final result:'</span>, sess.run(accuracy, feed_dict = &#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</div></pre></td></tr></table></figure>
<ul>
<li>基于 MNIST 手写体数据集的「CNN」模型（Tensorboard 绘图）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 基于 MNIST 数据集 的 「CNN」（tensorboard 绘图）</span></div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></div><div class="line">	initial = tf.truncated_normal(shape, stddev = <span class="number">0.1</span>)</div><div class="line">	<span class="keyword">return</span> tf.Variable(initial)</div><div class="line">	</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></div><div class="line">	initial = tf.constant(<span class="number">0.1</span>, shape = shape)</div><div class="line">	<span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></div><div class="line">	<span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></div><div class="line">	<span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">	</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_summaries</span><span class="params">(var)</span>:</span></div><div class="line">	<span class="string">"""Attach a lot of summaries to a Tensor (for TensorBoard visualization)."""</span></div><div class="line">	<span class="keyword">with</span> tf.name_scope(<span class="string">'summaries'</span>):</div><div class="line">		mean = tf.reduce_mean(var)</div><div class="line">		tf.summary.scalar(<span class="string">'mean'</span>, mean)</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'stddev'</span>):</div><div class="line">			stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))</div><div class="line">		tf.summary.scalar(<span class="string">'stddev'</span>, stddev)</div><div class="line">		tf.summary.scalar(<span class="string">'max'</span>, tf.reduce_max(var))</div><div class="line">		tf.summary.scalar(<span class="string">'min'</span>, tf.reduce_min(var))</div><div class="line">		tf.summary.histogram(<span class="string">'histogram'</span>, var)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(input_tensor, weights_shape, biases_shape, layer_name, act = tf.nn.relu, flag = <span class="number">1</span>)</span>:</span></div><div class="line">	<span class="string">"""Reusable code for making a simple neural net layer.</span></div><div class="line"></div><div class="line">	It does a matrix multiply, bias add, and then uses relu to nonlinearize.</div><div class="line">	It also sets up name scoping so that the resultant graph is easy to read,</div><div class="line">	and adds a number of summary ops.</div><div class="line">"""</div><div class="line">	<span class="keyword">with</span> tf.name_scope(layer_name):</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</div><div class="line">			weights = weight_variable(weights_shape)</div><div class="line">			variable_summaries(weights)</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</div><div class="line">			biases = bias_variable(biases_shape)</div><div class="line">			variable_summaries(biases)</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</div><div class="line">			<span class="keyword">if</span> flag == <span class="number">1</span>:</div><div class="line">				preactivate = tf.add(conv2d(input_tensor, weights), biases)</div><div class="line">			<span class="keyword">else</span>:</div><div class="line">				preactivate = tf.add(tf.matmul(input_tensor, weights), biases)</div><div class="line">			tf.summary.histogram(<span class="string">'pre_activations'</span>, preactivate)</div><div class="line">		<span class="keyword">if</span> act == <span class="keyword">None</span>:</div><div class="line">			outputs = preactivate</div><div class="line">		<span class="keyword">else</span>:</div><div class="line">			outputs = act(preactivate, name = <span class="string">'activation'</span>)</div><div class="line">			tf.summary.histogram(<span class="string">'activation'</span>, outputs)</div><div class="line">		<span class="keyword">return</span> outputs</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">	<span class="comment"># Import data</span></div><div class="line">	mnist_data_path = <span class="string">'MNIST_data/'</span></div><div class="line">	mnist = input_data.read_data_sets(mnist_data_path, one_hot = <span class="keyword">True</span>)</div><div class="line">	</div><div class="line">	<span class="keyword">with</span> tf.name_scope(<span class="string">'Input'</span>):</div><div class="line">		x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">28</span>*<span class="number">28</span>], name = <span class="string">'input_x'</span>)</div><div class="line">		y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>], name = <span class="string">'target_y'</span>)</div><div class="line"></div><div class="line">	<span class="comment"># First Convolutional Layer</span></div><div class="line">	x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span> ,<span class="number">1</span>])</div><div class="line">	conv_1 = add_layer(x_image, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>], [<span class="number">32</span>], <span class="string">'First_Convolutional_Layer'</span>, flag = <span class="number">1</span>)</div><div class="line">	</div><div class="line">	<span class="comment"># First Pooling Layer</span></div><div class="line">	pool_1 = max_pool_2x2(conv_1)</div><div class="line">	</div><div class="line">	<span class="comment"># Second Convolutional Layer </span></div><div class="line">	conv_2 = add_layer(pool_1, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>], [<span class="number">64</span>], <span class="string">'Second_Convolutional_Layer'</span>, flag = <span class="number">1</span>)</div><div class="line"></div><div class="line">	<span class="comment"># Second Pooling Layer </span></div><div class="line">	pool_2 = max_pool_2x2(conv_2)</div><div class="line"></div><div class="line">	<span class="comment"># Densely Connected Layer</span></div><div class="line">	pool_2_flat = tf.reshape(pool_2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</div><div class="line">	dc_1 = add_layer(pool_2_flat, [<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>, <span class="number">1024</span>], [<span class="number">1024</span>], <span class="string">'Densely_Connected_Layer'</span>, flag = <span class="number">0</span>) </div><div class="line">	</div><div class="line">	<span class="comment"># Dropout</span></div><div class="line">	keep_prob = tf.placeholder(tf.float32)</div><div class="line">	dc_1_drop = tf.nn.dropout(dc_1, keep_prob)</div><div class="line">	</div><div class="line">	<span class="comment"># Readout Layer</span></div><div class="line">	y = add_layer(dc_1_drop, [<span class="number">1024</span>, <span class="number">10</span>], [<span class="number">10</span>], <span class="string">'Readout_Layer'</span>, flag = <span class="number">0</span>)</div><div class="line">	</div><div class="line">	<span class="comment"># Optimizer</span></div><div class="line">	<span class="keyword">with</span> tf.name_scope(<span class="string">'cross_entroy'</span>):</div><div class="line">		cross_entroy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_,</div><div class="line">						logits = y))</div><div class="line">		tf.summary.scalar(<span class="string">'cross_entropy'</span>, cross_entroy)</div><div class="line">		tf.summary.histogram(<span class="string">'cross_entropy'</span>, cross_entroy)</div><div class="line">	</div><div class="line">	<span class="comment"># Train</span></div><div class="line">	<span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</div><div class="line">		train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entroy)</div><div class="line">	</div><div class="line">	<span class="comment"># Test</span></div><div class="line">	<span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'correct_prediction'</span>):</div><div class="line">			correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</div><div class="line">		<span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</div><div class="line">			accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">		tf.summary.scalar(<span class="string">'accuracy'</span>, accuracy)</div><div class="line">		</div><div class="line">	sess = tf.InteractiveSession()</div><div class="line">	merged = tf.summary.merge_all()</div><div class="line">	train_writer = tf.summary.FileWriter(<span class="string">'train/'</span>, sess.graph)</div><div class="line">	test_writer = tf.summary.FileWriter(<span class="string">'test/'</span>)</div><div class="line">	tf.global_variables_initializer().run()</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">feed_dict</span><span class="params">(train)</span>:</span></div><div class="line">		<span class="keyword">if</span> train:</div><div class="line">			batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">			k = <span class="number">0.5</span></div><div class="line">		<span class="keyword">else</span>:</div><div class="line">			batch_xs, batch_ys = mnist.test.images, mnist.test.labels</div><div class="line">			k = <span class="number">1.0</span></div><div class="line">		<span class="keyword">return</span> &#123;x: batch_xs, y_: batch_ys, keep_prob: k&#125;</div><div class="line">	</div><div class="line">	<span class="comment"># Train the model, and also write summaries.</span></div><div class="line">	<span class="comment"># Every 10th step, measure test-set accuracy, and write test summaries</span></div><div class="line">	<span class="comment"># All other steps, run train_step on training data, &amp; add training summaries</span></div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</div><div class="line">		<span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:	<span class="comment"># Record summaries and test-set accuracy</span></div><div class="line">			summary, acc = sess.run([merged, accuracy], feed_dict = feed_dict(<span class="keyword">False</span>))</div><div class="line">			test_writer.add_summary(summary, i)</div><div class="line">			print(<span class="string">"step %d, training accuracy %g"</span> %(i, acc))</div><div class="line">		<span class="keyword">else</span>:	<span class="comment"># Record train set summaries, and train</span></div><div class="line">			<span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">99</span>:	<span class="comment"># Record execution stats</span></div><div class="line">				run_options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE)</div><div class="line">				run_metadata = tf.RunMetadata()</div><div class="line">				summary, _ = sess.run([merged, train_step], feed_dict = feed_dict(<span class="keyword">True</span>), </div><div class="line">										options = run_options, run_metadata = run_metadata)</div><div class="line">				train_writer.add_run_metadata(run_metadata, <span class="string">'step %d'</span> % i)</div><div class="line">				train_writer.add_summary(summary, i)</div><div class="line">				print(<span class="string">'Adding run metadata for'</span>, i)</div><div class="line">			<span class="keyword">else</span>:</div><div class="line">				summary, _ = sess.run([merged, train_step], feed_dict = feed_dict(<span class="keyword">True</span>))</div><div class="line">				train_writer.add_summary(summary, i)</div><div class="line">main()</div></pre></td></tr></table></figure>
<p>可能对于最后一个模型 CNN 的代码，需要一些 CNN 卷积神经网络的一些知识。例如什么是卷积、池化，还需要了解 TensorFlow 中用到的相应函数，例如<strong><code>tf.nn.conv2d()</code></strong>，<strong><code>tf.nn.max_pool()</code></strong>，这里不再赘述，可以参考我的这篇文章：<a href="">「TensorFlow」CNN Introduction</a>。</p>
<p>贴上最后一个模型的部分截图：</p>
<ul>
<li>代码部分：</li>
</ul>
<p><img src="https://farm4.staticflickr.com/3681/32385203053_a1a401b062_o.png" alt=""></p>
<p>说明：<strong>上图右侧是 CNN 网络训练的步数以及对应的结果，程序需要运行挺久时间的，CPU 占用率也很高，建议挂在晚上跑，人去休息睡觉。可以根据自身机器条件修改参数 range(10000)。</strong>。</p>
<hr>
<p>上述代码运行完成之后，命令行中跳转到代码生成的「train」文件夹中（其和代码文件存在于同一文件夹中），然后输入 <strong><code>tensorboard --logdir .</code></strong>，等待程序反应之后，浏览器访问<strong><code>localhost:6006</code></strong>（当然你也可以自己定义端口）。如果不出意外，你会得到以下内容：</p>
<ul>
<li><p>Scalars:</p>
<p><img src="https://farm3.staticflickr.com/2671/33073147421_7e52b090a1_o.png" alt=""></p>
</li>
<li><p>Graphs:</p>
<p><img src="https://farm1.staticflickr.com/740/32818486500_bdd7dacc7f_o.png" alt=""></p>
</li>
<li><p>Distributions:</p>
<p><img src="https://farm4.staticflickr.com/3703/32818489270_ae5fc65e5a_o.png" alt=""></p>
</li>
<li><p>Histograms:</p>
<p><img src="https://farm3.staticflickr.com/2741/33073150051_260717b598_o.png" alt=""></p>
</li>
</ul>
<p>关于各个模块的作用，以及各个变量的意义，开篇已经提及，我在此就不再赘述了。</p>
<p>代码文件放置在我的 Github 对应的 repository 当中: <a href="https://github.com/RandolphVI/TensorFlow-Learning" target="_blank" rel="external">「TensorFlow Learning」</a></p>
<p>另外，在自己的机器模型在训练期间（特别是深度网络），训练时间通常几小时到十几小时不等，甚至可能会花上好几天，那么在这段时间，你们又会干些什么事情呢？作为程序员，这里提供一个「有趣的」方式，可以使用你的微信来监控你的模型在训练期间的一举一动，具体做法参考我的另一篇文章 <a href="http://randolph.pro/2017/03/13/%3CTensorflow%3E%20Use%20WeChat%20to%20Monitor%20Your%20Network/">「TensorFlow」Use WeChat to Monitor Your Network</a></p>
]]></content>
    
    <summary type="html">
    
      本文主要介绍 TensorFlow 的 Tensorboard 功能。
    
    </summary>
    
      <category term="TensorFlow" scheme="http://randolph.pro/categories/TensorFlow/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
      <category term="TensorFlow" scheme="http://randolph.pro/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>♜「Papers」 About Text Classification based on CNN</title>
    <link href="http://randolph.pro/2017/03/10/%E2%99%9C%E3%80%8CPapers%E3%80%8DAbout%20Text%20Classification%20based%20on%20CNN/"/>
    <id>http://randolph.pro/2017/03/10/♜「Papers」About Text Classification based on CNN/</id>
    <published>2017-03-09T16:00:00.000Z</published>
    <updated>2017-08-08T13:34:55.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4373/35507009504_3298ce3029_o.jpg" alt=""></p>
<p>有关「Machine Learning」的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/">「Machine Learning」</a><br>有关「Papers」的其他论文学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/Papers/">「Papers」</a></p>
<h1 id="Embedding-layer"><a href="#Embedding-layer" class="headerlink" title="Embedding layer"></a>Embedding layer</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>), tf.name_scope(<span class="string">"embedding"</span>):</div><div class="line">    W = tf.Variable(</div><div class="line">        tf.random_uniform([vocab_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>),</div><div class="line">        name=<span class="string">"W"</span>)</div><div class="line">    self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)</div><div class="line">    self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, <span class="number">-1</span>)</div></pre></td></tr></table></figure>
<p>存储全部 word vector 的矩阵 $W$，$W$ 初始化时是随机 random 出来的，也就是 paper 中的第一种模型 CNN-rand，训练过程中并不是每次都会使用全部的 vocabulary，而只是产生一个 batch（batch 中都是 sentence，每个 sentence 标记了出现哪些 word(最大长度为 max_seq_len)，因此 batch 相当于一个二维列表），这个 batch 就是 input_x：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.input_x = tf.placeholder(tf.int32, [<span class="keyword">None</span>, sequence_length], name=<span class="string">"input_x"</span>)</div></pre></td></tr></table></figure>
<p><code>tf.nn.embedding_lookup</code>：查找 input_x 中所有的 ids，获取它们的 word vector。batch 中的每个 sentence 的每个 word 都要查找。所以得到的 embedded_chars 的 shape 应该是 <code>[None, max_seq_len, embedding_size]</code>。<br>但是，输入的 word vectors 得到之后，下一步就是输入到卷积层，用到 <code>tf.nn.conv2d</code> 函数，<code>conv2d</code> 的参数列表：</p>
<p><code>input: [batch, in_height, in_width, in_channels]</code><br><code>filter: [filter_height, filter_width, in_channels, out_channels]</code></p>
<p>对比可以发现，就差一个 in_channels 了，而最 simple 的版本也就只有 1 通道（Yoon 的第四个模型用到了 multichannel）。因此需要 <code>expand dim</code> 来适应 <code>conv2d</code> 的 input 要求，万能的 tensorflow 已经提供了这样的功能：</p>
<blockquote>
<p>This operation is useful if you want to add a batch dimension to a single element. For example, if you have a single image of shape [height, width, channels], you can make it a batch of 1 image with expand_dims(image, 0), which will make the shape [1, height, width, channels].</p>
</blockquote>
<p>Example:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 't' is a tensor of shape [2]</div><div class="line">shape(expand_dims(t, -1)) ==&gt; [2, 1]</div></pre></td></tr></table></figure>
<p>因此只需要<code>tf.expand_dims(self.embedded_chars, -1)</code>，就能在 embedded_chars 后面加一个 in_channels=1</p>
<h1 id="Conv-and-Max-pooling"><a href="#Conv-and-Max-pooling" class="headerlink" title="Conv and Max-pooling"></a>Conv and Max-pooling</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create a convolution + maxpool layer for each filter size</span></div><div class="line">pooled_outputs = []</div><div class="line"><span class="keyword">for</span> i, filter_size <span class="keyword">in</span> enumerate(filter_sizes):</div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"conv-maxpool-%s"</span> % filter_size):</div><div class="line">        <span class="comment"># Convolution Layer</span></div><div class="line">        filter_shape = [filter_size, embedding_size, <span class="number">1</span>, num_filters]</div><div class="line">        W = tf.Variable(tf.truncated_normal(filter_shape, stddev=<span class="number">0.1</span>), name=<span class="string">"W"</span>)</div><div class="line">        b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[num_filters]), name=<span class="string">"b"</span>)</div><div class="line">        conv = tf.nn.conv2d(</div><div class="line">            self.embedded_chars_expanded,</div><div class="line">            W,</div><div class="line">            strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</div><div class="line">            padding=<span class="string">"VALID"</span>,</div><div class="line">            name=<span class="string">"conv"</span>)</div><div class="line">        <span class="comment"># Apply nonlinearity</span></div><div class="line">        h = tf.nn.relu(tf.nn.bias_add(conv, b), name=<span class="string">"relu"</span>)</div><div class="line">        <span class="comment"># Maxpooling over the outputs</span></div><div class="line">        pooled = tf.nn.max_pool(</div><div class="line">            h,</div><div class="line">            ksize=[<span class="number">1</span>, sequence_length - filter_size + <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</div><div class="line">            strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</div><div class="line">            padding=<span class="string">'VALID'</span>,</div><div class="line">            name=<span class="string">"pool"</span>)</div><div class="line">        pooled_outputs.append(pooled)</div><div class="line"></div><div class="line"><span class="comment"># Combine all the pooled features</span></div><div class="line">num_filters_total = num_filters * len(filter_sizes)</div><div class="line">self.h_pool = tf.concat(<span class="number">3</span>, pooled_outputs)</div><div class="line">self.h_pool_flat = tf.reshape(self.h_pool, [<span class="number">-1</span>, num_filters_total])</div></pre></td></tr></table></figure>
<p>首先，对 <code>filter_sizes</code> 中的每一个 <code>filter_window_size</code> 都要进行卷积（每一种 <code>size</code> 都要产生 <code>num_filters</code> 个 <code>filter maps</code>），所以外层就是一个大的 for 循环。<br>由于在 for 循环内部，<code>filter_size</code> 是固定了的，因此可以结合：<code>[filter_height, filter_width, in_channels, out_channels]</code> 得到，<code>filter_shape = [filter_size, embedding_size, 1, num_filters]</code>之所以要弄清楚 <code>filter shape</code> 是因为要对 filter 的权重矩阵 $w$ 进行初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">W = tf.Variable(tf.truncated_normal(filter_shape, stddev=<span class="number">0.1</span>), name=<span class="string">"W"</span>)</div></pre></td></tr></table></figure>
<p>这里为什么要使用 <code>tf.truncated_normal()</code> 函数？</p>
<p>这是因为 tensorflow 中提供了两个 normal 函数：</p>
<ul>
<li><code>tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</code></li>
<li><code>tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</code></li>
</ul>
<p>对比了一下，这两个函数的参数列表完全相同，不同之处我就直接引用文档中的说明，讲解的很清楚：</p>
<blockquote>
<p>Outputs random values from a truncated normal distribution.<br>The generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.</p>
</blockquote>
<p>也就是说 random 出来的值的范围都在 <code>[mean - 2 standard_deviations, mean + 2 standard_deviations]</code> 内。<br>下图可以告诉你这个范围在哪：</p>
<p><img style="width:50%" src="https://farm3.staticflickr.com/2320/32925087411_1151d71bc1_o.jpg"></p>
<script type="math/tex; mode=display">
c_{i} = f(w⋅x_{i:i+h-1}+b)</script><p><code>conv2d()</code> 得到的其实是公式中的 $w⋅x $ 的部分，还要加上 <code>bias</code> 项：<code>tf.nn.bias_add(conv, b)</code>，并且通过激励函数 relu：<code>tf.nn.relu</code>。最终得到卷积层的输出 $h$。</p>
<p>那究竟卷积层的输出的 shape 是什么样呢？<br>官方文档中有一段话解释了卷积后得到的输出结果：</p>
<ol>
<li>Flattens the filter to a 2-D matrix with shape <code>[filter_height*filter_width*in_channels, output_channels]</code> .</li>
<li>Extracts image patches from the input tensor to form a virtual tensor of shape <code>[batch, out_height, out_width, filter_height*filter_width*in_channels]</code> .</li>
<li>For each patch, right_multiplies the filter matrix and the image patch over.</li>
</ol>
<p>第三步进行了 right-multiply 之后得到的结果就是 <code>[batch, out_height, out_width, output_channels]</code>，但是还是不清楚这里的 <code>out_height</code> 和 <code>out_width</code> 到底是什么。<br>那就看看 wildml 中怎么说的吧 </p>
<blockquote>
<p>“VALID” padding means that we slide the filter over our sentence without padding the edges, performing a narrow convolution that gives us an output of shape [1, sequence_length - filter_size + 1, 1, 1]. </p>
</blockquote>
<p>哦，这句话的意思是说 <code>out_height</code> 和 <code>out_width</code> 其实和 <code>padding</code> 的方式有关系，这里选择了 <code>&#39;VALID&#39;</code>的方式，也就是不在边缘加 padding，得到：</p>
<ul>
<li><code>out_height = sequence_length - filter_size + 1</code></li>
<li><code>out_width = 1</code></li>
</ul>
<p>因此，综合上面的两个解释，我们知道 <code>conv2d-(+bias)-relu</code> 之后得到 $h$ 的 shape：</p>
<p><code>[batch, sequence_length - filter_size + 1, 1, num_filters]</code></p>
<p>接下来的工作就是 max-pooling 了，来看一下 tensorflow 中给出的函数:<br><code>tf.nn.max_pool(value, ksize, strides, padding, data_format=&#39;NHWC&#39;, name=None)</code></p>
<p>其中最重要的两个参数是 <code>value</code> 和 <code>ksize</code>。</p>
<ul>
<li><code>value</code> ：相当于是 max pooling 层的输入，在整个网络中就是刚才我们得到的 $h$，check 了一下它俩的 shape 是一致的，说明可以直接传递到下一层。</li>
<li><code>ksize</code>：官方解释说是 input tensor 每一维度上的 window size。仔细想一下，其实就是想定义多大的范围来进行 max-pooling，比如在图像中常见的 2*2 的小正方形区域对整个 $h$ 得到 feature map 进行 pooling；但是在 nlp 中，刚才说到了每一个 feature map 现在是 <code>[batch, sequence_length - filter_size + 1, 1, num_filters]</code> 维度的，我们想知道每个 <code>output_channels</code>（每个 channel 是一个 vector）的最大值，也就是最重要的 feature 是哪一个，那么就是在第二个维度上设定 <code>window=sequence_length - filter_size + 1</code></li>
</ul>
<p>根据 <code>ksize</code> 的设置，和 <code>value</code> 的 shape，可以得到 <code>pooled</code> 的 shape：<code>[batch, 1, 1, num_filters]</code></p>
<p>这是一个 <code>filter_size</code> 的结果（比如 <code>filter_size = 3</code>），pooled 存储的是当前 filter_size 下每个 sentence 最重要的<code>num_filters</code> 个 features，将结果 append 到 pooled_outputs 列表中存起来，再对下一个 <code>filter_size</code> （比如 <code>filter_size = 4</code>）进行相同的操作。</p>
<p>等到 for 循环结束时，也就是所有的 <code>filter_size</code> 全部进行了 convolution 和 max-pooling 之后，首先需要把相同 <code>filter_size</code> 的所有 pooled 结果 concat 起来，再将不同的 filter_size 之间的结果 concat 起来，最后的到的应该类似于二维数组：<code>[batch, all_pooled_result]</code><br><code>all_pooled_result</code> 一共有 <code>num_filters\（100）*len(filter_sizes)</code>个，比如 300 个 <br> 连接的过程需要使用 <code>tf.concat</code>，官方给出的例子很容易理解。<br>最后得到的 h_pool_flat 也就是 <code>[batch, 300]</code> 维的 tensor。</p>
<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Add dropout</span></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"dropout"</span>):</div><div class="line">    self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)</div></pre></td></tr></table></figure>
<p>Dropout 仅对 hiddenlayer 的输出层进行 drop，使得有些结点的值不输出给 softmax 层。</p>
<h1 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Final (unnormalized) scores and predictions</span></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"output"</span>):</div><div class="line">    W = tf.get_variable(</div><div class="line">        <span class="string">"W"</span>,</div><div class="line">        shape=[num_filters_total, num_classes],</div><div class="line">        initializer=tf.contrib.layers.xavier_initializer())</div><div class="line">    b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[num_classes]), name=<span class="string">"b"</span>)</div><div class="line">    l2loss += tf.nn.l2loss(W)</div><div class="line">    l2loss += tf.nn.l2loss(b)</div><div class="line">    self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=<span class="string">"scores"</span>)</div><div class="line">    self.predictions = tf.argmax(self.scores, <span class="number">1</span>, name=<span class="string">"predictions"</span>)</div></pre></td></tr></table></figure>
<p>输出层其实是个 softmax 分类器，没什么可讲的，但是要注意 <code>L2</code> 正则 <br> 还有一个奇怪的地方是，这一层按道理说应该是一个 softmax layer，但是并没有使用到 softmax 函数，在 Yoon 的文章中也是直接得到输出的。因此，我们也按照这种方式写代码，得到所有类别的 score，并且选出最大值的那个类别(argmax)。y 的 shape 为：<code>[batch, num_classes]</code>，因此 <code>argmax()</code> 的时候是选取每行的 max，所以<code>dimention=1</code> 。因此，最后 scores 的 shape 为：<code>[batch, 1]</code></p>
<h1 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h1><p>得到了整个网络的输出之后，也就是我们得到了 <code>y_prediction</code>，但还需要和真实的 <code>y_label</code> 进行比较，以此来确定预测好坏。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># CalculateMean cross-entropy loss</span></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"loss"</span>):</div><div class="line">    losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)</div><div class="line">    self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss</div></pre></td></tr></table></figure>
<p>还是使用常规的 cross_entropy 作为 loss function。最后一层是全连接层，为了防止过拟合，最后还要在 loss func 中加入 L2 正则项，即 <code>l2_loss</code>。<code>l2_reg_lambda</code> 来确定惩罚的力度。</p>
<h1 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Accuracy</span></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"accuracy"</span>):</div><div class="line">    correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, <span class="number">1</span>))</div><div class="line">    self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, <span class="string">"float"</span>), name=<span class="string">"accuracy"</span>)</div></pre></td></tr></table></figure>
<p><code>tf.equal(x, y)</code> 返回的是一个 bool tensor，如果 x y 对应位置的值相等就是 <code>true</code>，否则 <code>false</code>。得到的 tensor 是 <code>[batch, 1]</code> 的。<br><code>tf.cast(x, dtype)</code> 将 bool tensor 转化成 float 类型的 tensor，方便计算。</p>
<p><code>tf.reduce_mean()</code> 本身输入的就是一个 float 类型的 vector（元素要么是 <code>0.0</code>，要么是<code>1.0</code>），直接对这样的 vector 计算 mean 得到的就是 accuracy，不需要指定 reduction_indices。</p>
]]></content>
    
    <summary type="html">
    
      CNN Text Classification.
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
      <category term="Papers" scheme="http://randolph.pro/categories/Machine-Learning/Papers/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Papers" scheme="http://randolph.pro/tags/Papers/"/>
    
  </entry>
  
  <entry>
    <title>「Machine Learning」 CNN Introduction</title>
    <link href="http://randolph.pro/2017/03/07/%E3%80%8CMachine%20Learning%E3%80%8D%20CNN%20Introduction/"/>
    <id>http://randolph.pro/2017/03/07/「Machine Learning」 CNN Introduction/</id>
    <published>2017-03-06T16:00:00.000Z</published>
    <updated>2017-08-12T14:30:16.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4300/35460446383_aafc34ca3c_o.jpg" alt=""></p>
<p>有关「Machine Learning」的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/">「Machine Learning」</a></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>在 20 世纪 60 年代，Hubel 和 Wiesel 在研究猫脑皮层中用于局部敏感和方向选择的神经元时发现其独特的网络结构可以有效地降低反馈神经网络的复杂性，继而出了卷积神经网络（Convolutional Neural Networks- 简称 CNN）。</p>
<p>卷积神经网络（Convolutional Neural Network）虽然很早被出，但是却是近些年才得以发展起来并引起广泛重视的。它是深度学习技术中极具代表的网络结构之一，也是近些年语音分析和图像识别领域的研究热点，后来发现其在 NLP 自然语言处理上的效果同样不俗。它以其特有的局部连接、权值共享网络结构，有效地降低了深度神经网络模型的复杂度，极大地减少了权值的数量。</p>
<h2 id="Perceptron"><a href="#Perceptron" class="headerlink" title="Perceptron"></a>Perceptron</h2><p>在理解 CNN 卷积神经网络之前，有必要了解神经网络的机制以及缺点，而其中典型的神经元网络就是 MLP （Multi-Layer Perceptron）多层感知器。</p>
<p>而多层感知器中的 Multi-Layer 是针对单层感知器（Perceptron）而言，单层感知器是最简单的前馈人工神经网络，就是通常人们所说的“神经元”。 </p>
<h2 id="Build-a-Multilayer-Convolutional-Network"><a href="#Build-a-Multilayer-Convolutional-Network" class="headerlink" title="Build a Multilayer Convolutional Network"></a>Build a Multilayer Convolutional Network</h2><p>在 CNN 卷积神经网络中，主要有四个操作：</p>
<ol>
<li>卷积</li>
<li>非线性处理（ReLU）</li>
<li>池化或者亚采样</li>
<li>分类（全连接层）</li>
</ol>
<p>这些操作对于各个卷积神经网络来说都是基本组件，因此理解它们的工作原理有助于充分了解卷积神经网络。下面我们将会尝试理解各步操作背后的原理。</p>
<h2 id="What’s-CNN"><a href="#What’s-CNN" class="headerlink" title="What’s CNN"></a>What’s CNN</h2><h3 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h3><p>卷积的主要目的是为了从输入图像中提取特征。卷积可以通过从输入的一小块数据中学到图像的特征，并可以保留像素间的空间关系。让我们举个例子来尝试理解一下卷积是如何处理图像的：</p>
<p>正如我们上面所说，每张图像都可以看作是像素值的矩阵。考虑一下一个 5 x 5 的图像，它的像素值仅为 0 或者 1（注意对于灰度图像而言，像素值的范围是 0 到 255，下面像素值为 0 和 1 的绿色矩阵仅为特例）：</p>
<p><img src="https://farm3.staticflickr.com/2446/32873275652_9f6261728c_o.png" alt=""></p>
<p>同时，考虑下另一个 3 x 3 的矩阵，如下所示：</p>
<p><img src="https://farm3.staticflickr.com/2563/32873276412_6330affec6_o.png" alt=""></p>
<p>接下来，5 x 5 的图像和 3 x 3 的矩阵的卷积可以按下图所示的动画一样计算：</p>
<p><img src="https://farm3.staticflickr.com/2233/32873279482_bf439eb5c7_o.gif" alt=""></p>
<p>现在停下来好好理解下上面的计算是怎么完成的。我们用橙色的矩阵在原始图像（绿色）上滑动，每次滑动一个像素（也叫做「步长」），在每个位置上，我们计算对应元素的乘积（两个矩阵间），并把乘积的和作为最后的结果，得到输出矩阵（粉色）中的每一个元素的值。注意，3 x 3 的矩阵每次步长中仅可以看到输入图像的一部分。</p>
<p>在 CNN 的术语中，3x3 的矩阵叫做「滤波器」(filter) 或「核」(kernel) 或者 「特征检测器」(feature detector)，通过在图像上滑动滤波器并计算点乘得到矩阵叫做「卷积特征」(Convolved Feature) 或者 「激活图」(Activation Map) 或者 「特征图」(Feature Map)。记住，滤波器在原始输入图像上的作用是特征检测器。</p>
<p>从上面图中的动画可以看出，对于同样的输入图像，不同值的滤波器将会生成不同的特征图。比如，对于下面这张输入图像：</p>
<p><img src="https://farm1.staticflickr.com/711/32873351892_a02b2be853_o.png" alt=""></p>
<p>在下表中，我们可以看到不同滤波器对上图卷积的效果。正如表中所示，通过在卷积操作前修改滤波矩阵的数值，我们可以进行诸如边缘检测、锐化和模糊等操作 —— 这表明不同的滤波器可以从图中检测到不同的特征，比如边缘、曲线等。</p>
<p><img style="width:50%" src="https://farm3.staticflickr.com/2919/32213941213_beabd8f9e5_o.png"></p>
<p>另一个直观理解卷积操作的好方法是看下面这张图的动画：</p>
<p><img src="https://farm3.staticflickr.com/2235/32902525261_d8091fe768_o.gif" alt=""></p>
<p>滤波器（红色框）在输入图像滑过（卷积操作），生成一个特征图。另一个滤波器（绿色框）在同一张图像上卷积可以得到一个不同的特征图。注意卷积操作可以从原图上获取局部依赖信息。同时注意这两个不同的滤波器是如何从同一张图像上生成不同的特征图。记住上面的图像和两个滤波器仅仅是我们上面讨论的数值矩阵。</p>
<p>在实践中，CNN 会在训练过程中学习到这些滤波器的值（尽管我们依然需要在训练前指定诸如滤波器的个数、滤波器的大小、网络架构等参数）。我们使用的滤波器越多，提取到的图像特征就越多，网络所能在未知图像上识别的模式也就越好。</p>
<p>特征图的大小（卷积特征）由下面三个参数控制，我们需要在卷积前确定它们：</p>
<ul>
<li><p>深度（Depth）：<strong>深度对应的是卷积操作所需的滤波器个数</strong>。在下图的网络中，我们使用三个不同的滤波器对原始图像进行卷积操作，这样就可以生成三个不同的特征图。你可以把这三个特征图看作是堆叠的 2d 矩阵，那么，特征图的「深度」就是 3。</p>
</li>
<li><p>步长（Stride）：<strong>步长是我们在输入矩阵上滑动滤波矩阵的像素数</strong>。当步长为 1 时，我们每次移动滤波器一个像素的位置。当步长为 2 时，我们每次移动滤波器会跳过 2 个像素。步长越大，将会得到更小的特征图。</p>
</li>
<li><p>零填充（Zero-padding）：<strong>有时，在输入矩阵的边缘使用零值进行填充，这样我们就可以对输入图像矩阵的边缘进行滤波。</strong>零填充的一大好处是可以让我们控制特征图的大小。使用零填充的也叫做泛卷积，不适用零填充的叫做严格卷积。</p>
<p><img style="width:50%" src="https://farm3.staticflickr.com/2929/32183844484_8be44cdea0_o.png"></p>
</li>
</ul>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>ReLU 表示修正线性单元（Rectified Linear Unit），是一个非线性操作。</p>
<p><img style="width:50%" src="https://farm3.staticflickr.com/2745/32986518056_d320066568_o.png"></p>
<script type="math/tex; mode=display">
Output = Max(zero, Input)</script><ol>
<li><p>为什么要引入非线性激励函数？</p>
<p>如果不用激励函数（其实相当于激励函数是 $f(x) = x$ ），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐层效果相当，这种情况就是最原始的感知机（Perceptron）了。</p>
<p>正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。最早的想法是 sigmoid 函数或者 tanh 函数，输出有界，很容易充当下一层输入（以及一些人的生物解释 balabala）。</p>
</li>
<li><p>为什么要引入 ReLU 而不是其他的非线性函数（例如 Sigmoid 函数）？</p>
<ul>
<li>采用 sigmoid 等函数，<strong>算激活函数时（指数运算），计算量大</strong>，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用 Relu 激活函数，整个过程的计算量节省很多。</li>
<li>对于深层网络，<strong>sigmoid 函数反向传播时，很容易就会出现梯度消失的情况 </strong>（在 sigmoid 接近饱和区时，变换太缓慢，导数趋于 0，这种情况会造成<strong> 信息丢失</strong>），从而无法完成深层网络的训练。</li>
<li><strong>Relu 会使一部分神经元的输出为 0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生</strong>（以及一些人的生物解释 balabala）。</li>
</ul>
<p>当然现在也有一些对 relu 的改进，比如 prelu，random relu 等，在不同的数据集上会有一些训练速度上或者准确率上的改进，具体的可以找相关的 paper 看。</p>
<p>（多加一句，现在主流的做法，会多做一步 batch normalization，尽可能保证每一层网络的输入具有相同的分布。而最新的 paper，他们在加入 bypass connection 之后，发现改变 batch normalization 的位置会有更好的效果。）</p>
</li>
<li><p>ReLU 的优点与缺点？</p>
<p>优点：</p>
<ul>
<li>解决了 gradient vanishing 问题 (在正区间)</li>
<li>计算速度非常快，只需要判断输入是否大于 0</li>
<li>收敛速度远快于 sigmoid 和 tanh</li>
</ul>
<p>缺点：</p>
<ul>
<li>ReLU 的输出不是 zero-centered</li>
<li><strong>Dead ReLU Problem</strong>，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate 太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用 Xavier 初始化方法，以及避免将 learning rate 设置太大或使用 adagrad 等自动调节 learning rate 的算法。</li>
</ul>
</li>
</ol>
<blockquote>
<p>几十年的机器学习发展中，我们形成了这样一个概念：非线性激活函数要比线性激活函数更加先进。</p>
<p>尤其是在布满 Sigmoid 函数的 BP 神经网络，布满径向基函数的 SVM 神经网络中，往往有这样的幻觉，非线性函数对非线性网络贡献巨大。</p>
<p>该幻觉在 SVM 中更加严重。核函数的形式并非完全是 SVM 能够处理非线性数据的主力功臣（支持向量充当着隐层角色）。</p>
<p>那么在深度网络中，对非线性的依赖程度就可以缩一缩。另外，在上一部分提到，稀疏特征并不需要网络具有很强的处理线性不可分机制。</p>
<p>综合以上两点，在深度学习模型中，使用简单、速度快的线性激活函数可能更为合适。</p>
</blockquote>
<p>ReLU 操作可以从下面的图中理解。这里的输出特征图也可以看作是“修正”过的特征图。</p>
<p><img style="width:80%" src="https://farm3.staticflickr.com/2421/32214171823_0cbab38971_o.png"></p>
<p>所谓麻雀虽小，五脏俱全，ReLU 虽小，但也是可以改进的。</p>
<h4 id="ReLU 的种类"><a href="#ReLU 的种类" class="headerlink" title="ReLU 的种类"></a>ReLU 的种类</h4><p><strong>ReLU 的区分主要在负数端，根据负数端斜率的不同来进行区分</strong>，大致如下图所示。</p>
<p><img src="https://farm3.staticflickr.com/2929/33186208875_5ab850f2ab_o.png" alt=""></p>
<p>普通的 ReLU 负数端斜率是 0，Leaky ReLU 则是负数端有一个比较小的斜率，而 PReLU 则是在后向传播中学习到斜率。而 Randomized Leaky ReLU 则是使用一个均匀分布在训练的时候随机生成斜率，在测试的时候使用均值斜率来计算。</p>
<h4 id="效果"><a href="# 效果" class="headerlink" title="效果"></a>效果 </h4><p> 其中，NDSB 数据集是 Kaggle 的比赛，而 RReLU 正是在这次比赛中崭露头角的。</p>
<p><img src="https://farm4.staticflickr.com/3852/32803703170_7605fcdbc2_o.png" alt=""></p>
<p>通过上述结果，可以看到四点：</p>
<ul>
<li>对于 Leaky ReLU 来说，如果斜率很小，那么与 ReLU 并没有大的不同，当斜率大一些时，效果就好很多。</li>
<li>在训练集上，PReLU 往往能达到最小的错误率，说明 PReLU 容易过拟合。</li>
<li>在 NSDB 数据集上 RReLU 的提升比 cifar10 和 cifar100 上的提升更加明显，而 NSDB 数据集比较小，从而可以说明，RReLU 在与过拟合的对抗中更加有效。</li>
<li>对于 RReLU 来说，还需要研究一下随机化得斜率是怎样影响训练和测试过程的。</li>
</ul>
<h4 id="参考文献"><a href="# 参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><p>[1]. Xu B, Wang N, Chen T, et al. Empirical evaluation of rectified activations in convolutional network[J]. arXiv preprint arXiv:1505.00853, 2015.</p>
<h3 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h3><p>空间池化（Spatial Pooling）（也叫做亚采用或者下采样）<strong>降低了各个特征图的维度，但可以保持大部分重要的信息</strong>。空间池化有下面几种方式：最大化、平均化、加和等等。</p>
<p>对于最大池化（Max Pooling），我们定义一个空间邻域（比如，2x2 的窗口），并从窗口内的修正特征图中取出最大的元素。除了取最大元素，我们也可以取平均（Average Pooling）或者对窗口内的元素求和。<strong>在实际中，最大池化被证明效果更好一些。</strong></p>
<p>下面的图展示了使用 2x2 窗口在修正特征图（在卷积 + ReLU 操作后得到）使用最大池化的例子。</p>
<p><img style="width:50%" src="https://farm3.staticflickr.com/2036/32647435560_470bdc2c7b_o.png"></p>
<p>我们以 2 个元素（也叫做“步长”）滑动我们 2x2 的窗口，并在每个区域内取最大值。如上图所示，这样操作可以降低我们特征图的维度。</p>
<p>在下图展示的网络中，池化操作是分开应用到各个特征图的（注意，因为这样的操作，我们可以从三个输入图中得到三个输出图）。</p>
<p><img style="width:50%" src="https://farm3.staticflickr.com/2202/32873928362_3a7b8b86cf_o.png"></p>
<p>下图展示了我们在 ReLU 操作之后得到的修正特征图的池化操作的效果：</p>
<p><img style="width:80%" src="https://farm4.staticflickr.com/3881/32214568423_416fed1642_o.png"></p>
<p>池化函数可以逐渐降低输入表示的空间尺度。特别地，Pooling 的好处是:</p>
<ul>
<li><p>使输入表示（特征维度）变得更小，并且网络中的参数和计算的数量更加可控的减小，因此，可以控制过拟合。</p>
</li>
<li><p>使网络对于输入图像中更小的变化、冗余和变换变得不变性（输入的微小冗余将不会改变池化的输出——因为我们在局部邻域中使用了最大化 / 平均值的操作）。</p>
</li>
<li><p>帮助我们获取图像最大程度上的尺度不变性（准确的词是“不变性”）。它非常的强大，因为我们可以检测图像中的物体，无论它们位置在哪里。</p>
<p>​</p>
</li>
</ul>
<p>到目前为止我们了解了卷积、ReLU 和池化是如何操作的。理解这些层是构建任意 CNN 的基础是很重要的。正如下图所示，<strong>我们有两组卷积、ReLU &amp; 池化层 —— 第二组卷积层使用六个滤波器对第一组的池化层的输出继续卷积，得到一共六个特征图</strong>。接下来对所有六个特征图应用 ReLU。接着我们对六个修正特征图分别进行最大池化操作。</p>
<p>这些层一起就可以从图像中提取有用的特征，并在网络中引入非线性，减少特征维度，同时保持这些特征具有某种程度上的尺度变化不变性。</p>
<p><img src="https://farm3.staticflickr.com/2044/32988985296_3a7106f13d_o.png" alt=""></p>
<p>第二组池化层的输出作为全连接层的输入，接下来我们将介绍全连接层。</p>
<h3 id="Connect"><a href="#Connect" class="headerlink" title="Connect"></a>Connect</h3><p>全连接层是传统的多层感知器，在输出层使用的是 softmax 激活函数（也可以使用其他像 SVM 的分类器，但在本文中只使用 softmax）。「全连接」(Fully Connected) 这个词表明前面层的所有神经元都与下一层的所有神经元连接。</p>
<p>卷积和池化层的输出表示了输入图像的高级特征。全连接层的目的是为了使用这些特征把输入图像基于训练数据集进行分类。比如，在下面图中我们进行的图像分类有四个可能的输出结果（注意下图并没有显示全连接层的节点连接）。</p>
<p><img style="width:80%" src="https://farm1.staticflickr.com/350/32875720272_88dd409c3f_o.png"></p>
<p>除了分类，添加一个全连接层也（一般）是学习这些特征的非线性组合的简单方法。从卷积和池化层得到的大多数特征可能对分类任务有效，但这些特征的组合可能会更好。</p>
<p>从全连接层得到的输出概率和为 1。这个可以在输出层使用 softmax 作为激活函数进行保证。softmax 函数输入一个任意大于 0 值的矢量，并把它们转换为零一之间的数值矢量，其和为一。</p>
<h3 id="Use-Backpropagation-to-Train-whole-network"><a href="#Use-Backpropagation-to-Train-whole-network" class="headerlink" title="Use Backpropagation to Train whole network"></a>Use Backpropagation to Train whole network</h3><p>正如上面讨论的，卷积 + 池化层的作用是从输入图像中提取特征，而全连接层的作用是分类器。</p>
<p>注意在下面的图中，因为输入的图像是船，对于船这一类的目标概率是 1，而其他三类的目标概率是 0，即</p>
<ul>
<li><p>输入图像 = 船</p>
</li>
<li><p>目标矢量 = [0, 0, 1, 0]</p>
<p><img src="https://farm1.staticflickr.com/739/32216466493_38095200db_o.png" alt=""></p>
</li>
</ul>
<p>完整的卷积网络的训练过程可以总结如下：</p>
<ul>
<li>第一步：我们初始化所有的滤波器，使用随机值设置参数 / 权重</li>
<li>第二步：网络接收一张训练图像作为输入，通过前向传播过程（卷积、ReLU 和池化操作，以及全连接层的前向传播），找到各个类的输出概率</li>
<li><ul>
<li>我们假设船这张图像的输出概率是 [0.2, 0.4, 0.1, 0.3]</li>
<li>因为对于第一张训练样本的权重是随机分配的，输出的概率也是随机的</li>
</ul>
</li>
<li>第三步：在输出层计算总误差（计算 4 类的和）</li>
<li><ul>
<li>Total Error = ∑  ½ (target probability – output probability) ²</li>
</ul>
</li>
<li>第四步：使用反向传播算法，根据网络的权重计算误差的梯度，并使用梯度下降算法更新所有滤波器的值 / 权重以及参数的值，使输出误差最小化</li>
<li><ul>
<li>权重的更新与它们对总误差的占比有关</li>
<li>当同样的图像再次作为输入，这时的输出概率可能会是 [0.1, 0.1, 0.7, 0.1]，这就与目标矢量 [0, 0, 1, 0] 更接近了</li>
<li>这表明网络已经通过调节权重 / 滤波器，可以正确对这张特定图像的分类，这样输出的误差就减小了</li>
<li>像滤波器数量、滤波器大小、网络结构等这样的参数，在第一步前都是固定的，在训练过程中保持不变——仅仅是滤波器矩阵的值和连接权重在更新</li>
</ul>
</li>
<li>第五步：对训练数据中所有的图像重复步骤 1 ~ 4</li>
</ul>
<p>上面的这些步骤可以 <strong> 训练</strong> ConvNet —— 这本质上意味着对于训练数据集中的图像，ConvNet 在更新了所有权重和参数后，已经优化为可以对这些图像进行正确分类。</p>
<p>当一张新的（未见过的）图像作为 ConvNet 的输入，网络将会再次进行前向传播过程，并输出各个类别的概率（对于新的图像，输出概率是使用已经在前面训练样本上优化分类的参数进行计算的）。如果我们的训练数据集非常的大，网络将会（有希望）对新的图像有很好的泛化，并把它们分到正确的类别中去。</p>
<p><strong>注 1</strong>: 上面的步骤已经简化，也避免了数学详情，只为提供训练过程的直观内容。</p>
<p><strong>注 2</strong>: 在上面的例子中我们使用了两组卷积和池化层。然而请记住，这些操作可以在一个 ConvNet 中重复多次。实际上，现在有些表现最好的 ConvNet 拥有多达十几层的卷积和池化层！同时，每次卷积层后面不一定要有池化层。如下图所示，我们可以在池化操作前连续使用多个卷积 + ReLU 操作。还有，请注意 ConvNet 的各层在下图中是如何可视化的。</p>
<p><img style="width:80%" src="https://farm3.staticflickr.com/2113/32875828582_ce237c84d2_o.png"></p>
<h3 id="Visualization-on-CNN"><a href="#Visualization-on-CNN" class="headerlink" title="Visualization on CNN"></a>Visualization on CNN</h3><p>一般而言，越多的卷积步骤，网络可以学到的识别特征就越复杂。比如，ConvNet 的图像分类可能在第一层从原始像素中检测出边缘，然后在第二层使用边缘检测简单的形状，接着使用这些形状检测更高级的特征，比如更高层的人脸。下面的图中展示了这些内容——我们使用 <a href="http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf" target="_blank" rel="external"> 卷积深度置信网络 </a> 学习到的特征，这张图仅仅是用来证明上面的内容（这仅仅是一个例子：真正的卷积滤波器可能会检测到对我们毫无意义的物体）。</p>
<p><img src="https://farm4.staticflickr.com/3738/33031508435_aee3cd62ca_o.png" alt=""></p>
<p>Adam Harley 创建了一个卷积神经网络的可视化结果，使用的是 MNIST 手写数字的训练集。我强烈建议使用它来理解 CNN 的工作原理。</p>
<p>我们可以在下图中看到网络是如何识别输入 「8」 的。注意下图中的可视化并没有单独展示 ReLU 操作。</p>
<p><img src="https://farm4.staticflickr.com/3934/32186606394_d904c8de2e_o.png" alt=""></p>
<p>输入图像包含 1024 个像素（32 x 32 大小），第一个卷积层（卷积层 1）由六个独特的 5x5 （步长为 1）的滤波器组成。如图可见，使用六个不同的滤波器得到一个深度为六的特征图。</p>
<p>卷积层 1 后面是池化层 1，在卷积层 1 得到的六个特征图上分别进行 2x2 的最大池化（步长为 2）的操作。你可以在池化层上把鼠标移动到任意的像素上，观察在前面卷积层（如上图所示）得到的 4x4 的小格。你会发现 4x4 小格中的最大值（最亮）的像素将会进入到池化层。</p>
<p><img src="https://farm1.staticflickr.com/682/32186608014_f69038eb43_o.png" alt=""></p>
<p>池化层 1 后面的是六个 5x5 （步长为 1）的卷积滤波器，进行卷积操作。后面就是池化层 2，进行 2x2 的最大池化（步长为 2）的操作。这两层的概念和前面描述的一样。</p>
<p>接下来我们就到了三个全连接层。它们是：</p>
<ul>
<li>第一个全连接层有 120 个神经元</li>
<li>第二层全连接层有 100 个神经元</li>
<li>第三个全连接层有 10 个神经元，对应 10 个数字——也就做输出层</li>
</ul>
<p>注意在下图中，输出层中的 10 个节点的各个都与第二个全连接层的所有 100 个节点相连（因此叫做全连接）。</p>
<p>同时，注意在输出层那个唯一的亮的节点是如何对应于数字 “8” 的——这表明网络把我们的手写数字正确分类（越亮的节点表明从它得到的输出值越高，即，8 是所有数字中概率最高的）。</p>
<p><img src="https://farm3.staticflickr.com/2830/32216998463_c7897cf1d5_o.png" alt=""></p>
<p>同样的 3D 可视化可以在 <a href="http://scs.ryerson.ca/~aharley/vis/conv/" target="_blank" rel="external"> 这里 </a> 看到。</p>
<h3 id="Other-ConvNet"><a href="#Other-ConvNet" class="headerlink" title="Other ConvNet"></a>Other ConvNet</h3><p>卷积神经网络从上世纪 90 年代初期开始出现。我们上面提到的 LeNet 是早期卷积神经网络之一。其他有一定影响力的架构如下所示：</p>
<ul>
<li>LeNet (1990s)： 本文已介绍。</li>
<li>1990s to 2012：在上世纪 90 年代后期至 2010 年初期，卷积神经网络进入孵化期。随着数据量和计算能力的逐渐发展，卷积神经网络可以处理的问题变得越来越有趣。</li>
<li>AlexNet (2012) – 在 2012，Alex Krizhevsky （与其他人）发布了 <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external">AlexNet</a>，它是比 LeNet 更深更宽的版本，并在 2012 年的 ImageNet 大规模视觉识别大赛（ImageNet Large Scale Visual Recognition Challenge，ILSVRC）中以巨大优势获胜。这对于以前的方法具有巨大的突破，当前 CNN 大范围的应用也是基于这个工作。</li>
<li>ZF Net (2013) – ILSVRC 2013 的获胜者是来自 Matthew Zeiler 和 Rob Fergus 的卷积神经网络。它以 <a href="http://arxiv.org/abs/1311.2901" target="_blank" rel="external">ZFNet</a> （Zeiler &amp; Fergus Net 的缩写）出名。它是在 AlexNet 架构超参数上进行调整得到的效果提升。</li>
<li>GoogLeNet (2014) – ILSVRC 2014 的获胜者是来自于 Google 的 <a href="http://arxiv.org/abs/1409.4842" target="_blank" rel="external">Szegedy</a>等人的卷积神经网络。它的主要贡献在于使用了一个 Inception 模块，可以大量减少网络的参数个数（4M，AlexNet 有 60M 的参数）。</li>
<li>VGGNet (2014) – 在 ILSVRC 2014 的领先者中有一个 <a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/" target="_blank" rel="external">VGGNet</a> 的网络。它的主要贡献是展示了网络的深度（层数）对于性能具有很大的影响。</li>
<li>ResNets (2015) – <a href="http://arxiv.org/abs/1512.03385" target="_blank" rel="external">残差网络 </a> 是何凯明（和其他人）开发的，并赢得 ILSVRC 2015 的冠军。ResNets 是当前卷积神经网络中最好的模型，也是实践中使用 ConvNet 的默认选择（截至到 2016 年五月）。</li>
<li>DenseNet (2016 八月) – 近来由 Gao Huang （和其他人）发表的，<a href="http://arxiv.org/abs/1608.06993" target="_blank" rel="external">the Densely Connected Convolutional Network</a> 的各层都直接于其他层以前向的方式连接。DenseNet 在五种竞争积累的目标识别基准任务中，比以前最好的架构有显著的提升。可以在 <a href="https://github.com/liuzhuang13/DenseNet" target="_blank" rel="external"> 这里 </a> 看 Torch 实现。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      本文主要介绍 Deep Learning 中的 CNN 卷积神经网络。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>「Machine Learning」 Factorization Machines</title>
    <link href="http://randolph.pro/2017/03/03/%E3%80%8CMachine%20Learning%E3%80%8D%20Factorization%20Machines/"/>
    <id>http://randolph.pro/2017/03/03/「Machine Learning」 Factorization Machines/</id>
    <published>2017-03-02T16:00:00.000Z</published>
    <updated>2017-08-06T12:57:41.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4300/35460446383_aafc34ca3c_o.jpg" alt=""></p>
<p> 有关「Machine Learning」的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/">「Machine Learning」</a></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Factorization Machines（FM），可译为“隐因子分解机”，由 Steffen Rendle 于 2010 年提出，并发布开源工具 <strong>libFM</strong> 。他凭借 FM 单个模型，他在 KDD Cup 2012 上，取得 Track1 的第 2 名和 Track2 的第 3 名。</p>
<h1 id="Compared-with-Other-Models"><a href="#Compared-with-Other-Models" class="headerlink" title="Compared with Other Models"></a>Compared with Other Models</h1><ul>
<li> 在数据非常稀疏时（如推荐系统），SVM 不能取得很好的效果。</li>
<li> 对带非线性核函数的 SVM，需要在对偶问题上进行求解。</li>
<li> 目前还有很多不同的 factorization models ，比如 matrix factorization 和一些特殊的模型 SVD++, PITF, FPMC。这些模型的一个缺点是它们只适用于某些特定的输入数据，优化算法也需要根据问题专门设计。</li>
<li>FM 适用于实数值的特征向量。并且经过一些变换，可以看出 FM 囊括了以上方法。</li>
</ul>
<h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><p> 首先考虑线性模型：</p>
<script type="math/tex; mode=display">
\hat y (x) = w_0 + w_1x_1 + w_2x_2+ \dots + w_nx_n = w_0 + \sum_{i=1}^{n}w_ix_i</script><p> 各特征分量之间是相互孤立的。</p>
<p>$\hat y(x)$ 仅考虑单个的特征分量，而没有考虑特征分量之间的相互关系。</p>
<p> 考虑任意两个特征分量之间的关系：</p>
<script type="math/tex; mode=display">
\hat y (x) := w_0 +  \sum_{i=1}^{n}w_ix_i + \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}W_{ij}x_ix_j</script><p> 在数据高度系数的场景中，上述模型由很大的缺陷：</p>
<p> 当 <script type="math/tex">x_i</script> 与 <script type="math/tex">x_j</script> 未出现过交互时，不能对相应的参数 <script type="math/tex">w_{ij}</script> 进行估计， <script type="math/tex">w_{ij}</script> 一定为 0 。</p>
<p> 因此，需要对每个维度的特征分量 $x_i$ ，引入：</p>
<script type="math/tex; mode=display">
v_i = (v_{i1},v_{i2}, \dots ,v_{ik})^T \epsilon \ \mathbb{R} ^k, \ i = 1,2,\dots,n</script><p> 改写 $w_{ij}$ :</p>
<script type="math/tex; mode=display">
\hat w_{ij} = v_i^Tv_j := \sum_{l=1}^{k}v_{il}v_{jl}</script><h2 id="2-way-FM"><a href="#2-way-FM" class="headerlink" title="2-way FM"></a>2-way FM</h2><script type="math/tex; mode=display">
\hat y(x) := w_0 + \sum_{i=1}^{n}w_ix_i + \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\left \langle  v_i, v_j\right \rangle x_ix_j</script><script type="math/tex; mode=display">
w_0 \ \epsilon\  \mathbb{R},w \ \epsilon\  \mathbb{R}^n, V \ \epsilon\  \mathbb{R}^{n\times k}</script><script type="math/tex; mode=display">
\left \langle  v_i, v_j\right \rangle := \sum_{f=1}^{k}v_{i,f} \cdot v_{j,f}</script><ul>
<li>$v_i$ 用 $k$ 个因子描述第 $i$ 个变量。</li>
<li> 正整数 $k$ 是超参，决定了分解的维度。</li>
</ul>
<h2 id="Complexity-Analysis"><a href="#Complexity-Analysis" class="headerlink" title="Complexity Analysis"></a>Complexity Analysis</h2><p>FM 模型中需要估计的参数包括 <script type="math/tex">w_0 \ \epsilon\  \mathbb{R},w \ \epsilon\  \mathbb{R}^n, V \ \epsilon\  \mathbb{R}^{n\times k}</script>，共 <script type="math/tex">1+n+n*k</script> 个，<script type="math/tex">w_{0}</script> 为整体的偏置量，$w$ 对特征向量的各个分量的强度进行建模，$V$ 对特征向量中任意两个分量之间的关系进行建模。</p>
<p> 直观上看，上述模型的计算复杂度是 $O(kn^2)$ ，但是经过下面的改写后：</p>
<script type="math/tex; mode=display">
\begin{align}
& \sum_{i=1}^{n}\sum_{j=i+1}^{n}\left \langle  v_i, v_j\right \rangle x_ix_j \cr
=& \frac{1}{2} \sum_{i=1}^{n}\sum_{j=1}^{n}\left \langle  v_i, v_j\right \rangle x_ix_j - \frac{1}{2} \sum_{i=1}^{n}\left \langle  v_i, v_i\right \rangle x_ix_i\cr 
=& \frac{1}{2} \left (\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{f=1}^{k} v_{i,f} \cdot v_{j,f} \ x_ix_j - \sum_{i=1}^{n}\sum_{f=1}^{k} v_{i,f} \cdot v_{i,f} \ x_ix_i \right) \cr
=& \frac{1}{2} \sum_{f=1}^{k} \left(\left( \sum_{i=1}^{n}v_{i,f}x_{i}\right) \left(\sum_{j=1}^{n}v_{j,f}x_{j}\right) - \sum_{i=1}^{n}v_{i,f}^2 x_{i}^2\right) \cr
=& \frac{1}{2}  \sum_{f=1}^{k} \left(\left( \sum_{i=1}^{n}v_{i,f}x_{i}\right)^2 - \sum_{i=1}^{n}v_{i,f}^2 x_{i}^2\right)\cr
\end{align}</script><p> 计算复杂度经过改写后降低到线性的 $O(kn)$ 。</p>
<h2 id="d-way-FM"><a href="#d-way-FM" class="headerlink" title="d-way FM"></a>d-way FM</h2><p> 方程同时刻画 $l(1 \leq l \leq d)$ 个特征向量之间的相互关系：</p>
<script type="math/tex; mode=display">
\hat y(x) := w_0 + \sum_{i=1}^{n}w_ix_i + \sum_{l=2}^{d}\sum_{i_{1}=1}^{n}\dots \sum_{i_{t} = i_{t-1}+1}^{n}\left (\prod_{j=1}^{l}x_{i_{j}} \right)\left (\prod_{f=1}^{k_{l}}\prod_{j=1}^{l}v_{i_{j},f}^{(l)} \right)</script><h2 id="Problem-Solving"><a href="#Problem-Solving" class="headerlink" title="Problem Solving"></a>Problem Solving</h2><p> 最小化优化目标函数：</p>
<script type="math/tex; mode=display">
L = \sum_{i=1}^{N} loss(\hat y(x^{(i)}),y^{(i)})</script><p> 回归问题，损失函数可取为最小平方误差，即：</p>
<script type="math/tex; mode=display">
l^{LS}(y_1,y_2) := (y_1-y_2)^2</script><p> 二分类问题，损失函数可为 hinge loss 函数或 logistic loss 函数：</p>
<script type="math/tex; mode=display">
l^{C}(y_1,y_2) := -ln\sigma (y_1y_2)</script><script type="math/tex; mode=display">
\sigma (x) = \frac{1}{1+e^{-x}}</script><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><ul>
<li><p><strong> 随机梯度下降法 (StochasticGradient Descent, SGD)</strong></p>
</li>
<li><p><strong> 交替最小二乘法 (AlternatingLeast-Squares, ALS)</strong></p>
</li>
<li><p><strong> 马尔可夫链蒙特卡洛法 (MarkovChain Monte Carlo, MCMC)</strong></p>
</li>
</ul>
<h2 id="Multilinearity"><a href="#Multilinearity" class="headerlink" title="Multilinearity"></a>Multilinearity</h2><p>FM 的一个重要性质—<strong>Multilinearity</strong>，对于 FM 的任意参数 $\theta$ ，存在两个与 $\theta $ 的取值无关的函数 $g(\theta)$ 和 $h(\theta)$ 使得：</p>
<script type="math/tex; mode=display">
\hat y(x) = g_{\theta}(x) + \theta h_{\theta}(x) \qquad \forall\theta \in \Theta</script><p> 其中：</p>
<script type="math/tex; mode=display">
h_{\theta}(x) = \frac{\partial \hat y(x)}{\partial \theta} = 
\begin{cases}
1,  & \text{if $\theta$ is $w_{0}$} \\
x_{l}, & \text{if $\theta$ is $w_{l}$}  \\
x_{l}\sum_{j \neq l}v_{j,f}x_{f}, & \text{if $\theta$ is $v_{l,f}$}
\end{cases}</script><p>$g(\theta)$ 相对复杂，计算时，使用 <script type="math/tex">g_{\theta}(x) = \hat y(x) - \theta h_{\theta}(x)</script> 代替。</p>
<h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><p> 最小化损失函数的和：</p>
<script type="math/tex; mode=display">
OPT(S) := argmin \sum_{(x, y) \in S} l(\hat y(x \mid \Theta), y)</script><p> 加入 L2 正则：</p>
<script type="math/tex; mode=display">
OPTREG(S,\lambda) := argmin\left(\sum_{(x, y)\in S}l(\hat y(x \mid \Theta), y) + \sum_{\theta \in \Theta}\lambda_{\theta}\theta^2\right)</script><h2 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h2><p>  ​</p>
]]></content>
    
    <summary type="html">
    
      本文介绍了 Factorization Machines 隐因子分解机模型，包括模型数学知识介绍以及与其他模型的对比。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>♜「Papers」 About Dropout</title>
    <link href="http://randolph.pro/2017/03/01/%E2%99%9C%E3%80%8CPapers%E3%80%8DAbout%20Dropout/"/>
    <id>http://randolph.pro/2017/03/01/♜「Papers」About Dropout/</id>
    <published>2017-02-28T16:00:00.000Z</published>
    <updated>2017-08-06T12:56:57.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4373/35507009504_3298ce3029_o.jpg" alt=""></p>
<p>有关「Machine Learning」的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/">「Machine Learning」</a><br>有关「Papers」的其他论文学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/Papers/">「Papers」</a></p>
<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>开篇明义，dropout 是指在深度学习网络的训练过程中，对于神经网络单元，按照 <strong> 一定的概率 </strong> 将其 <strong> 暂时 </strong> 从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个 mini-batch 都在训练不同的网络。</p>
<p>Dropout 是 CNN 中防止过拟合提高效果的一个大杀器，但对于其为何有效，却众说纷纭。在下读到两篇代表性的论文，代表两种不同的观点，特此分享给大家。</p>
<h1 id="组合派"><a href="# 组合派" class="headerlink" title="组合派"></a>组合派 </h1><p> 参考文献中第一篇中的观点，Hinton 在 2014 年提出的。</p>
<h2 id="观点"><a href="# 观点" class="headerlink" title="观点"></a>观点 </h2><p> 该论文从神经网络的难题出发，一步一步引出 dropout 为何有效的解释。大规模的神经网络有两个缺点：</p>
<ul>
<li>费时</li>
<li>容易过拟合</li>
</ul>
<p>这两个缺点是深度学习上的两大瓶颈，过拟合是很多机器学习的通病，过拟合了，得到的模型基本就废了。而为了解决过拟合问题，一般会采用 ensemble 方法，即训练多个模型做组合，此时，费时就成为一个大问题，不仅训练起来费时，测试起来多个模型也很费时。总之，几乎形成了一个死锁。</p>
<p>Dropout 的出现很好的可以解决这个问题，每次做完 dropout，相当于从原始的网络中找到一个更瘦的网络，如下图所示：</p>
<p><img src="https://farm4.staticflickr.com/3878/33037649431_ab442383e2_o.png" alt=""></p>
<p>因而，对于一个有 $N$ 个节点的神经网络，有了 dropout 后，就可以看做是 $2^n$ 个模型的集合了，但此时要训练的参数数目却是不变的，这就解脱了费时的问题。</p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>虽然直观上看 dropout 是 ensemble 在分类性能上的一个近似，然而实际中，dropout 毕竟还是在一个神经网络上进行的，只训练出了一套模型参数。那么他到底是因何而有效呢？这就要从动机上进行分析了。论文中作者对 dropout 的动机做了一个十分精彩的类比：</p>
<blockquote>
<p>A motivation for dropout comes from a theory of the role of sex in evolution (Livnat et al., 2010). Sexual reproduction involves taking half the genes of one parent and half of the other, adding a very small amount of random mutation, and combining them to produce an offspring. The asexual alternative is to create an offspring with a slightly mutated copy of the parent’s genes. It seems plausible that asexual reproduction should be a better way to optimize individual fitness because a good set of genes that have come to work well together can be passed on directly to the offspring. On the other hand, sexual reproduction is likely to break up these co-adapted sets of genes, especially if these sets are large and, intuitively, this should decrease the fitness of organisms that have already evolved complicated co- adaptations. However, sexual reproduction is the way most advanced organisms evolved.</p>
<p><strong>One possible explanation for the superiority of sexual reproduction is that, over the long term, the criterion for natural selection may not be individual fitness but rather mix-ability of genes.</strong> The ability of a set of genes to be able to work well with another random set of genes makes them more robust. Since a gene cannot rely on a large set of partners to be present at all times, it must learn to do something useful on its own or in collaboration with a small number of other genes. <strong>According to this theory, the role of sexual reproduction is not just to allow useful new genes to spread throughout the population, but also to facilitate this process by reducing complex co-adaptations that would reduce the chance of a new gene improving the fitness of an individual.</strong> Similarly, each hidden unit in a neural network trained with dropout must learn to work with a randomly chosen sample of other units. This should make each hidden unit more robust and drive it towards creating useful features on its own without relying on other hidden units to correct its mistakes. However, the hidden units within a layer will still learn to do different things from each other. One might imagine that the net would become robust against dropout by making many copies of each hidden unit, but this is a poor solution for exactly the same reason as replica codes are a poor way to deal with a noisy channel.</p>
<p>A closely related, but slightly different motivation for dropout comes from thinking about successful conspiracies. Ten conspiracies each involving five people is probably a better way to create havoc than one big conspiracy that requires fifty people to all play their parts correctly. If conditions do not change and there is plenty of time for rehearsal, a big conspiracy can work well, but with non-stationary conditions, the smaller the conspiracy the greater its chance of still working. Complex co-adaptations can be trained to work well on a training set, but on novel test data they are far more likely to fail than multiple simpler co-adaptations that achieve the same thing.</p>
</blockquote>
<p>大概的意思，讲的是：</p>
<p>在自然界中，在中大型动物中，一般是有性繁殖，有性繁殖是指后代的基因从父母两方各继承一半。但是从直观上看，似乎无性繁殖更加合理，因为无性繁殖可以保留大段大段的优秀基因。而有性繁殖则将基因随机拆了又拆，破坏了大段基因的联合适应性。</p>
<p>但是自然选择中毕竟没有选择无性繁殖，而选择了有性繁殖，须知物竞天择，适者生存。我们先做一个假设，那就是基因的力量在于混合的能力而非单个基因的能力。不管是有性繁殖还是无性繁殖都得遵循这个假设。为了证明有性繁殖的强大，我们先看一个概率学小知识。</p>
<p>比如要搞一次恐怖袭击，两种方式： </p>
<ul>
<li>集中 50 人，让这 50 个人准确分工，搞一次大爆破。 </li>
<li>将 50 人分成 10 组，每组 5 人，分头行事，去随便什么地方搞点动作，成功一次就算。</li>
</ul>
<p>哪一个成功的概率比较大？ 显然是后者。</p>
<p>那么，类比过来，有性繁殖的方式不仅仅可以将优秀的基因传下来，还可以降低基因之间的联合适应性，使得复杂的大段大段基因联合适应性变成比较小的一个一个小段基因的联合适应性。</p>
<p>Dropout 也能达到同样的效果，它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。</p>
<p>个人补充一点：那就是植物和微生物大多采用无性繁殖，因为他们的生存环境的变化很小，因而不需要太强的适应新环境的能力，所以保留大段大段优秀的基因适应当前环境就足够了。而高等动物却不一样，要准备随时适应新的环境，因而将基因之间的联合适应性变成一个一个小的，更能提高生存的概率。</p>
<h2 id="Dropout 带来的模型的变化"><a href="#Dropout 带来的模型的变化" class="headerlink" title="Dropout 带来的模型的变化"></a>Dropout 带来的模型的变化 </h2><p> 而为了达到 ensemble 的特性，有了 dropout 后，神经网络的训练和预测就会发生一些变化。</p>
<ul>
<li><p>训练层面</p>
<p>无可避免的，训练网络的每个单元要添加一道概率流程。 </p>
<p><img src="https://farm3.staticflickr.com/2588/32782576600_aefb2c6586_o.png" alt=""></p>
<p>对应的公式变化如下如下：</p>
<ul>
<li><p>没有 dropout 的神经网络 </p>
<script type="math/tex; mode=display">
\begin{align}
z_{i}^{(l+1)} & = w_{i}^{(l+1)}y^{l} + b_{i}^{(l+1)} \cr
y_{i}^{(l+1)} & = f(z_{i}^{(l+1)})
\end{align}</script><p>​</p>
</li>
<li><p>有 dropout 的神经网络 </p>
<script type="math/tex; mode=display">
\begin{align}
r_{i}^{(l)} & \sim Bernoulli(p) \cr
\tilde{y}^{(l)} & = r^{(l)}*y^{(l)}\cr
z_{i}^{(l+1)} & = w_{i}^{(l+1)} \tilde{y}^{l} + b_{i}^{(l+1)} \cr
y_{i}^{(l+1)} & = f(z_{i}^{(l+1)})
\end{align}</script><p>​</p>
</li>
</ul>
</li>
<li><p>测试层面</p>
<p>预测的时候，每一个单元的参数要预乘以 $p$。</p>
<p><img src="https://farm4.staticflickr.com/3817/33123833046_48bf6f3dba_o.png" alt=""> </p>
</li>
</ul>
<h2 id="论文中的其他技术点"><a href="# 论文中的其他技术点" class="headerlink" title="论文中的其他技术点"></a>论文中的其他技术点</h2><ul>
<li><p>防止过拟合的方法：</p>
<ul>
<li>提前终止（当验证集上的效果变差的时候）</li>
<li>L1 和 L2 正则化加权</li>
<li>Soft Weight Sharing</li>
<li>Dropout</li>
</ul>
</li>
<li><p>Dropout 率的选择</p>
<ul>
<li>经过交叉验证，隐含节点 dropout 率等于 0.5 的时候效果最好，原因是 0.5 的时候 dropout 随机生成的网络结构最多。</li>
<li>Dropout 也可以被用作一种添加噪声的方法，直接对 input 进行操作。输入层设为更接近 1 的数。使得输入变化不会太大（0.8）。</li>
</ul>
</li>
<li><p>训练过程 </p>
<ul>
<li>对参数 w 的训练进行球形限制（max-normalization），对 dropout 的训练非常有用。</li>
<li>球形半径 c 是一个需要调整的参数。可以使用验证集进行参数调优。</li>
<li>Dropout 单独使用效果不错，但是 <strong>dropout</strong>、<strong>max-normalization</strong>、<strong>large decaying learning rates</strong> 以及 <strong>high momentum</strong> 组合起来效果更好，比如 max-norm regularization 就可以防止大的 learning rate 导致的参数 blow up。</li>
<li>使用 pretraining 方法也可以帮助 dropout 训练参数，在使用 dropout 时，要将所有参数都乘以 $\frac{1}{p}$。</li>
</ul>
</li>
<li><p>部分实验结论</p>
<p>该论文的实验部分很丰富，有大量的评测数据。</p>
<ul>
<li><p>maxout 神经网络中得另一种方法，Cifar-10 上超越 dropout</p>
</li>
<li><p>文本分类上，dropout 效果提升有限，分析原因可能是 Reuters-RCV1 数据量足够大，过拟合并不是模型的主要问题</p>
</li>
<li><p>dropout 与其他 standerd regularizers 的对比 </p>
</li>
<li><ul>
<li>L2 weight decay</li>
<li>lasso</li>
<li>KL-sparsity</li>
<li>max-norm regularization</li>
<li>dropout</li>
</ul>
</li>
<li><p>特征学习 </p>
<ul>
<li>标准神经网络，节点之间的相关性使得他们可以合作去 fix 其他节点中得噪声，但这些合作并不能在 unseen data 上泛化，于是，过拟合，dropout 破坏了这种相关性。在 autoencoder 上，有 dropout 的算法更能学习有意义的特征（不过只能从直观上，不能量化）。</li>
<li>产生的向量具有稀疏性。</li>
<li>保持隐含节点数目不变，dropout 率变化；保持激活的隐节点数目不变，隐节点数目变化。</li>
</ul>
</li>
<li><p>数据量小的时候，dropout 效果不好，数据量大了，dropout 效果好</p>
</li>
<li><p>模型均值预测</p>
<ul>
<li>使用 weight-scaling 来做预测的均值化</li>
<li>使用 mente-carlo 方法来做预测。即对每个样本根据 dropout 率先 sample 出来 k 个 net，然后做预测，k 越大，效果越好。</li>
</ul>
</li>
<li><p>Multiplicative Gaussian Noise </p>
<p>使用高斯分布的 dropou t 而不是伯努利模型 dropout</p>
</li>
<li><p>dropout 的缺点就在于训练时间是没有 dropout 网络的 2-3 倍。</p>
</li>
</ul>
</li>
</ul>
<p>&gt;</p>
<blockquote>
<p>进一步需要了解的知识点</p>
<ul>
<li>dropout RBM</li>
<li>Marginalizing Dropout </li>
<li>具体来说就是将随机化的 dropout 变为确定性的，比如对于 Logistic 回归，其 dropout 相当于加了一个正则化项</li>
<li>Bayesian neural network 对稀疏数据特别有用，比如 medical diagnosis, genetics, drug discovery and other computational biology applications</li>
</ul>
</blockquote>
<h1 id="噪声派"><a href="# 噪声派" class="headerlink" title="噪声派"></a>噪声派 </h1><p> 参考文献中第二篇论文中得观点，也很强有力。</p>
<h2 id="观点 -1"><a href="# 观点 -1" class="headerlink" title="观点"></a>观点 </h2><p> 观点十分明确，就是对于每一个 dropout 后的网络，进行训练时，相当于做了 Data Augmentation，因为，总可以找到一个样本，使得在原始的网络上也能达到 dropout 单元后的效果。 比如，对于某一层，dropout 一些单元后，形成的结果是<code>(1.5,0,2.5,0,1,2,0)</code>，其中 0 是被 drop 的单元，那么总能找到一个样本，使得结果也是如此。这样，每一次 dropout 其实都相当于增加了样本。</p>
<h2 id="稀疏性"><a href="# 稀疏性" class="headerlink" title="稀疏性"></a>稀疏性 </h2><h3 id="知识点 A"><a href="# 知识点 A" class="headerlink" title="知识点 A"></a> 知识点 A</h3><p>首先，先了解一个知识点：</p>
<blockquote>
<p>When the data points belonging to a particular class are distributed along a linear manifold, or sub-space, of the input space, it is enough to learn a single set of features which can span the entire manifold. But when the data is distributed along a highly non-linear and discontinuous manifold, the best way to represent such a distribution is to learn features which can explicitly represent small local regions of the input space, effectively “tiling” the space to define non-linear decision boundaries.</p>
</blockquote>
<p>大致含义就是： </p>
<p>在线性空间中，学习一个整个空间的特征集合是足够的，但是当数据分布在非线性不连续的空间中得时候，则学习局部空间的特征集合会比较好。</p>
<h3 id="知识点 B"><a href="# 知识点 B" class="headerlink" title="知识点 B"></a>知识点 B</h3><p>假设有一堆数据，这些数据由 $M$ 个不同的非连续性簇表示，给定 $K$ 个数据。那么一个有效的特征表示是将输入的每个簇映射为特征以后，簇之间的重叠度最低。使用 $A$ 来表示每个簇的特征表示中激活的维度集合。重叠度是指两个不同的簇的 $A<em>{i}$ 和 $A</em>{j}$ 之间的 Jaccard 相似度最小，那么：</p>
<ul>
<li>当 $K$ 足够大时，即便 $A$ 也很大，也可以学习到最小的重叠度</li>
<li>当 $K$ 小，$M$ 大时，学习到最小的重叠度的方法就是减小 $A$ 的大小，也就是稀疏性</li>
</ul>
<p>上述的解释可能是有点太专业化，比较拗口。主旨意思是这样，我们要把不同的类别区分出来，就要是学习到的特征区分度比较大，在数据量足够的情况下不会发生过拟合的行为，不用担心。但当数据量小的时候，可以通过稀疏性，来增加特征的区分度。</p>
<blockquote>
<p>因而有意思的假设来了，使用了 dropout 后，相当于得到更多的局部簇，同等的数据下，簇变多了，因而为了使区分性变大，就使得稀疏性变大。</p>
</blockquote>
<p>为了验证这个数据，论文还做了一个实验，如下图：</p>
<p><img src="https://farm3.staticflickr.com/2517/33185584305_9d146f80be_o.png" alt=""></p>
<p>该实验使用了一个模拟数据，即在一个圆上，有 15000 个点，将这个圆分为若干个弧，在一个弧上的属于同一个类，一共 10 个类，即不同的弧也可能属于同一个类。改变弧的大小，就可以使属于同一类的弧变多。</p>
<p>实验结论就是当弧长变大时，簇数目变少，稀疏度变低。与假设相符合。</p>
<p>个人观点：该假设不仅仅解释了 dropout 何以导致稀疏性，还解释了 dropout 因为使局部簇的更加显露出来，而根据知识点 A 可得，使局部簇显露出来是 dropout 能防止过拟合的原因，而稀疏性只是其外在表现。</p>
<h2 id="论文中的其他技术知识点"><a href="# 论文中的其他技术知识点" class="headerlink" title="论文中的其他技术知识点"></a>论文中的其他技术知识点</h2><ul>
<li><p>将 dropout 映射回得样本训练一个完整的网络，可以达到 dropout 的效果。</p>
</li>
<li><p>Dropout 由固定值变为一个区间，可以提高效果。</p>
</li>
<li><p>将 dropout 后的表示映射回输入空间时，并不能找到一个样本 x* 使得所有层都能满足 dropout 的结果，但可以为每一层都找到一个样本，这样，对于每一个 dropout，都可以找到一组样本可以模拟结果。</p>
</li>
<li><p>dropout 对应的还有一个 dropConnect，公式如下：</p>
<ul>
<li><p>dropout</p>
<script type="math/tex; mode=display">
h_{n} = \overrightarrow{w_{n}}^{T}(\overrightarrow{r} \odot \overrightarrow{x}) + b_{n}</script><p>​</p>
</li>
<li><p>dropConnect</p>
<script type="math/tex; mode=display">
h_{n} = (\overrightarrow{r_{n}} \odot \overrightarrow{w_{n}})^{T}\overrightarrow{x} + b_{n}</script><p>​</p>
</li>
</ul>
</li>
<li><p>试验中，纯二值化的特征的效果也非常好，说明了稀疏表示在进行空间分区的假设是成立的，一个特征是否被激活表示该样本是否在一个子空间中。</p>
</li>
</ul>
<h1 id="参考文献"><a href="# 参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]. Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: A simple way to prevent neural networks from overfitting[J]. The Journal of Machine Learning Research, 2014, 15(1): 1929-1958.</p>
<p>[2]. Dropout as data augmentation. <a href="http://arxiv.org/abs/1506.08700" target="_blank" rel="external">http://arxiv.org/abs/1506.08700</a></p>
]]></content>
    
    <summary type="html">
    
      Dropout 是 CNN 中防止过拟合提高效果的一个大杀器，但对于其为何有效，却众说纷纭。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
      <category term="Papers" scheme="http://randolph.pro/categories/Machine-Learning/Papers/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Papers" scheme="http://randolph.pro/tags/Papers/"/>
    
  </entry>
  
  <entry>
    <title>♞「Machine Learning」 Classification - KNN &amp; kd Tree</title>
    <link href="http://randolph.pro/2017/01/14/%E2%99%9E%E3%80%8CMachine%20Learning%E3%80%8D%20Classification%20-%20KNN%20&amp;%20kd%20Tree/"/>
    <id>http://randolph.pro/2017/01/14/♞「Machine Learning」 Classification - KNN &amp; kd Tree/</id>
    <published>2017-01-13T16:00:00.000Z</published>
    <updated>2017-08-06T13:18:34.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4307/36174772306_62cfbc8cd6_o.jpg" alt=""></p>
<p>有关该书的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/Book-「Machine-Learning」/">Book:「Machine Learning」</a></p>
<h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><ul>
<li>kNN</li>
<li>线性扫描实现 kNN 算法</li>
<li>kd 树实现 kNN 算法 </li>
</ul>
<hr>
<h1 id="kNN"><a href="#kNN" class="headerlink" title="kNN"></a>kNN</h1><p>$k$- 邻近算法 (kNN)，是一种基本<strong> 分类与回归 </strong> 方法。</p>
<p>此篇主要讲 $k$- 邻近算法在分类问题中的应用。</p>
<p>它的工作原理是： 存在一个样本数据合集 $S$，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较（<strong>距离度量 </strong>），然后算法提取样本集中 $k$ 个特征最相似数据（最近邻）的分类标签（<strong>$k$ 值的选择</strong>）。最后，选择 $k$ 个最相似数据中出现次数最多的分类（<strong> 分类决策规则</strong>），作为新数据的分类。</p>
<p>$k$- 邻近模型由三个基本要素—- 距离度量、$k$ 值的选择和分类决策规定决定。</p>
<h1 id="距离度量"><a href="# 距离度量" class="headerlink" title="距离度量"></a>距离度量 </h1><p> 一般使用的是欧氏距离，我们也可以根据自己的需求选择其他距离，比如更一般的 $L_p$ 距离，也称为<strong>Minkowski 距离</strong>。</p>
<p>这里 $L_p$ 距离中，$p \geq 1$，当 $p=1$ 时，称为曼哈顿距离；当 $p=2$ 时，称为欧式距离；当 $p=\infty$ 时，它是各个坐标距离的最大值，即切比雪夫距离。</p>
<p>设特征空间 $X$ 是 $n$ 维实数向量空间 $R^n$，$x_i$，$x_j$ $\in$ $X$，$x_i = (x_i^{(1)}，x_i^{(2)}，…，x_i^{(n)})^T$，$x_j = (x_j^{(1)}，x_j^{(2)}，…，x_j^{(n)})^T$，$x_i$，$x_j$ 的 $L_p$ 距离（Minkowski 距离）定义为：</p>
<script type="math/tex; mode=display">
L_p(x_i, x_j) = (\sum{l=1}^{n} \left | x{i}^{(l)}-x_{j}^{(l)} \right | ^p)^{1 \over p}</script><p><img src="https://farm1.staticflickr.com/417/31490650822_a861eca015_o.png" alt=""></p>
<p>但是，Minkowski 类型的距离函数存在的一个问题是，它们假定数据分布本质上应当具有对称性，即距离在所有方向上都是相同的。然而很多时候，数据并不符合球状分布，因此不宜采用像 Minkowski 距离这样的对称距离。例如：</p>
<p><img src="https://farm1.staticflickr.com/395/31490660172_23aaf2b39a_o.png" alt=""></p>
<p>对于这种情况，像图那样所示那样围绕数据画出一个标准圆是不可取的，我们应当对数据分布呈椭圆形这个特点予以考虑：</p>
<p><img src="https://farm1.staticflickr.com/289/31521619731_530a2ab0bd_o.png" alt=""></p>
<p>因此，我们需要选择一个能够更好地体现数据分布特点的距离度量方法—- 即 Mahalanobis 距离。Mahalanobis 距离函数会考虑数据在每个维度上的波动性，因此对于数据的每个维度，都存在一个 $s_i$，表示该数据集在此维度上的标准差的变量。Mahalanobis 距离的计算公式如下：</p>
<script type="math/tex; mode=display">
d(x,y) =\sqrt{\sum_{i=1}^{n}\frac{(x_{i} - y_{i})^2}{s_{i}^2}}</script><p>可以看出，该公式与欧式距离非常类似，但是不同的是它考虑到各种特性之间的联系（例如，一条关于身高的信息会带来一条关于体重的信息，因为两者是有关联的，并且是尺度无关的），即独立于测量尺度。</p>
<p>欧式距离就好比一个参照值，它表征的是当所有 <strong> 类别等概率 </strong> 出现的情况下，类别之间的距离。此时决策面中心点的位置就是两个类别中心的连线的中点。如下所示：</p>
<p><img src="https://farm1.staticflickr.com/458/30795845494_a42900cc19_o.png" alt=""></p>
<p>而当 <strong> 类别先验概率并不相等 </strong> 时，显然，如果仍然用中垂线作为决策线是不合理的，将出现判别错误（绿色类的点被判别为红色类），假设上图中绿色类别的先验概率变大，那么决策线将左移，如下图黄线。左移的具体位置，就是通过马氏距离来获得的。马氏距离中引入的协方差参数，表征的是点的稀密程度。</p>
<p><img src="https://farm1.staticflickr.com/458/30795845494_a42900cc19_o.png" alt=""></p>
<p><strong>从哲学上来说，用马氏距离处理数据时，不再把数据单纯的看作是冷冰冰的数字——那个引入的协方差，承认了客观上的差异性，就好像是有了人类的感情倾向，使得模式识别更加“人性化”也更加“视觉直观”。</strong></p>
<p>Mahalanobis 距离是基于样本分布的一种距离。<strong>物理意义就是在规范化的主成分空间中的欧氏距离。</strong>所谓规范化的主成分空间就是利用主成分分析对一些数据进行主成分分解。再对所有主成分分解轴做归一化，形成新的坐标轴。由这些坐标轴张成的空间就是规范化的主成分空间。</p>
<p>换句话说，主成分分析就是把椭球分布的样本改变到另一个空间里，使其成为球状分布。而 Mahalanobis 距离就是在样本呈球状分布的空间里面所求得的欧式距离。</p>
<p>当然，上面的解释只是对椭球分布而言，<strong>对一般分布，只能消除分布的二阶相关性，而不能消除高阶相关性</strong>。</p>
<hr>
<h1 id="k- 值的选择"><a href="#k- 值的选择" class="headerlink" title="$k$ 值的选择"></a>$k$ 值的选择</h1><ul>
<li>如果选择 $k$ 比较小，Bias 会比较低，但是 Variance 会比较高，$k$ 的减小就意味着整体模型变得复杂，容易发生过拟合 overfitting。</li>
<li>如果选择 $k$ 比较大，Variance 会比较低，但是 Bias 会比较高，$k$ 的增大就意味着整体模型变得简单，容易发生欠拟合 underfitting。<ul>
<li>通常来说 $k$ 是一个不大于 20 的整数。</li>
<li>为了确定 $k$ 值，主要有三种方案可供选择：<ol>
<li><strong>猜测</strong></li>
<li><strong>使用启发式策略</strong><ul>
<li>当分类问题中只涉及两个类别时，不要将 $k$ 值取为偶数。<ul>
<li>使 $k$ 与类别总数互质。将 $k$ 取为与类别总数互质的数，可保证投票数并列的情况较少出现。</li>
</ul>
</li>
<li>$k$ 的值应不小于类别总数加一。（使得所有的类别均有被表示的机会）</li>
<li>为避免出现噪音，$k$ 的值应足够小。</li>
</ul>
</li>
<li><strong>通过算法优化</strong><ul>
<li>使用 <strong> 遗传算法 </strong> 或者 <strong> 暴力网络搜索算法</strong>。</li>
<li>基于一个任意的 $k$ 试图将误差最小化称为爬山问题（Hill Climbing Problem）。其主要思想是对一组可能的 $k$ 值轮流进行考察，直至找到一个可接受的误差。利用遗传算法或者暴力网络搜索这样的算法来寻求 $k$ 的最优值的难点在于，当 $k$ 增大时，分类的复杂性也相应增加，从而降低性能。换言之，当增加 $k$ 时，程序的速度会逐渐变慢。<blockquote>
<p>If you want to learn more about genetic algorithms applied to find‐ ing an optimal K, you can read more about it in <strong>Florian Nigsch et al.’s Journal of Chemical Information and Modeling article, “Melting Point Prediction Employing k-Nearest Neighbor Algorithms and Genetic Parameter Optimization”</strong>.</p>
</blockquote>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<hr>
<h1 id="分类决策规则"><a href="# 分类决策规则" class="headerlink" title="分类决策规则"></a>分类决策规则</h1><p>$k$- 邻近法中的分类决策规则往往是“投票法”（多数表决），即由输入实例的 $k$ 个邻近的训练实例中的多数类决定输入实例的类。</p>
<p>因为算法思想简单，我们可以用很多方法实现它，这时效率就是我们需要慎重考虑的事情，最简单的自然是求出测试样本和训练集所有点的距离然后排序选择前 $k$ 个，这个是 $O(n(\log n))$ 的，而其实从 $N$ 个数据找前 $k$ 个数据是一个很常见的算法题，可以用最大堆（最小堆）实现，其效率是 $O(n(\log k))$ 的，而最广泛的算法是使用 $kd$ 树来减少扫描的点。</p>
<p>目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。</p>
<p><strong>KNN 算法不仅可以用于分类，还可以用于回归。</strong>通过找出一个样本的 $k$ 个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值（weight），如权值与距离成正比。</p>
<p>可以通过数学证明 <strong> 当数据规模趋于无穷时，最近邻分类器（$k=1$）的泛化错误率不超过贝叶斯最优分类器的错误率的两倍。</strong></p>
<blockquote>
<p>For every point in our dataset:    </p>
<ol>
<li>calculate the distance between inX and the current point.</li>
<li>sort the distances in increasing order.</li>
<li>take k items with lowest distances to inX.</li>
<li>find the majority class among these items.</li>
<li>return the majority class as our prediction for the class of inX.</li>
</ol>
</blockquote>
<p>对于未知类别属性的数据集中的每一个点一次执行以下操作：</p>
<ol>
<li>计算已知类别数据集中的点与当前点之间的距离。</li>
<li>按照距离递增次序排序。</li>
<li>选取与当前距离最小的 $k$ 个点。</li>
<li>确定前 $k$ 个点所在类别的出现频率。</li>
<li>返回前 $k$ 个点出现频率最高的类别作为当前点的预测分类。</li>
</ol>
<hr>
<h1 id="kNN 算法（线性扫描实现方法）"><a href="#kNN 算法（线性扫描实现方法）" class="headerlink" title="kNN 算法（线性扫描实现方法）"></a>kNN 算法（线性扫描实现方法）</h1><p>核心代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX, dataSet, labels, k)</span>:</span></div><div class="line">    dataSetSize = dataSet.shape[<span class="number">0</span>]</div><div class="line">    diffMat = tile(inX, (dataSetSize, <span class="number">1</span>)) - dataSet</div><div class="line">    sqDiffMat = diffMat ** <span class="number">2</span></div><div class="line">    sqDistances = sqDiffMat.sum(axis = <span class="number">1</span>)</div><div class="line">    distances = sqDistances ** <span class="number">0.5</span></div><div class="line">    sortedDistIndicies = distances.argsort()</div><div class="line">    classCount = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</div><div class="line">        voteIlabel = labels[sortedDistIndicies[i]]</div><div class="line">        classCount[voteIlabel] = classCount.get(voteIlabel,<span class="number">0</span>) + <span class="number">1</span></div><div class="line">    sortedClassCount = sorted(classCount.items(),key = operator.itemgetter(<span class="number">1</span>),reverse = <span class="keyword">True</span>)</div><div class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>分析一下代码：</p>
<p><strong><code>tile()</code></strong>是 Numpy 中的一个 module。它的用法是：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</div><div class="line">&gt;&gt;&gt; np.tile(a, <span class="number">2</span>)</div><div class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</div><div class="line">&gt;&gt;&gt; np.tile(a, (<span class="number">2</span>, <span class="number">2</span>))</div><div class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</div><div class="line">       [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]])</div><div class="line">&gt;&gt;&gt; np.tile(a, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>))</div><div class="line">array([[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]],</div><div class="line">       [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]]])</div></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; b = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</div><div class="line">&gt;&gt;&gt; np.tile(b, <span class="number">2</span>)</div><div class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>],</div><div class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>]])</div><div class="line">&gt;&gt;&gt; np.tile(b, (<span class="number">2</span>, <span class="number">1</span>))</div><div class="line">array([[<span class="number">1</span>, <span class="number">2</span>],</div><div class="line">       [<span class="number">3</span>, <span class="number">4</span>],</div><div class="line">       [<span class="number">1</span>, <span class="number">2</span>],</div><div class="line">       [<span class="number">3</span>, <span class="number">4</span>]])</div></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; c = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</div><div class="line">&gt;&gt;&gt; np.tile(c,(<span class="number">4</span>,<span class="number">1</span>))</div><div class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</div><div class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</div><div class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</div><div class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]])</div></pre></td></tr></table></figure>
<p>总结一下：<strong><code>tile(A, reps)</code></strong>就是将数组 A 重复 reps 次。</p>
<ul>
<li>A 的类型可以是 <strong><code>array, list, tuple, dict, matrix</code></strong> 以及<strong><code>int, string, float, bool</code></strong>。</li>
<li>reps 的类型可以是 <strong><code>tuple, list, dict, int, bool</code></strong> 但不可以是<strong><code>float, string, matrix</code></strong>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">diffMat = tile(inX, (dataSetSize, <span class="number">1</span>)) - dataSet</div></pre></td></tr></table></figure>
<p><strong><code>inX</code></strong>是我们的未知类别集中的一个点的输入向量（形式为 <script type="math/tex">[x_i,y_i]</script>），也就是我们需要进行分类的一个点。<br><strong><code>dataSetSize</code></strong> 是我们已知类别数据集的大小。</p>
<p>这句代码的目的便是生成一个这样的矩阵<strong><code>matrix</code></strong>:</p>
<script type="math/tex; mode=display">
[[\Delta x_1, \Delta y_1],[\Delta x_2, \Delta y_2],[\Delta x_3, \Delta y_3],...,[\Delta x_{datasize}, \Delta y_{datasize}]]</script><p>很明显，矩阵中存储的就是，未知点与已知类别数据集中的所有点的 $x$ 与 $y$ 坐标差值。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sqDistances = sqDiffMat.sum(axis = <span class="number">1</span>)</div></pre></td></tr></table></figure>
<p>这句代码就是进一步生成的这样的矩阵，用作下一步计算点与点之间距离用：</p>
<script type="math/tex; mode=display">
[[\Delta x_1^2 + \Delta y_1^2],[\Delta x_2^2 + \Delta y_2^2],[\Delta x_3^2 + \Delta y_3^2],...,[\Delta x_{datasize}^2 + \Delta y_{datasize}^2]]</script><hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sortedDistIndicies = distances.argsort()</div></pre></td></tr></table></figure>
<p>这句代码的意思便是返回一组索引值，这个索引值分别对应原数组中的点，但是是根据离未知点从最近到最远来排序，只用使用这个索引值就可以找到对应的已知类别数据中的一个点，从而得到该点标签。</p>
<p>举个例子：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = array([<span class="number">1</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>])</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([<span class="number">1</span>,  <span class="number">9</span>,  <span class="number">7</span>,  <span class="number">8</span>, <span class="number">10</span>,  <span class="number">2</span>,  <span class="number">0</span>,  <span class="number">3</span>])</div><div class="line">&gt;&gt;&gt; b = a.argsort()</div><div class="line">&gt;&gt;&gt; b</div><div class="line">array([<span class="number">6</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>])</div></pre></td></tr></table></figure>
<p>此时我们已经使用 $k$- 邻近算法构造了一个分类器，这个分类器可以处理二分类别任务，也可以处理多类别任务（labels 添加多个标签）。对于分类任务而言，评估分类器好坏的标准，最常用的便是错误率与精度，当然还有其他的评判标准，这里就不展开赘述。接下来在现实具体任务中来使用我们的 $k$- 邻近算法分类器。</p>
<hr>
<h2 id="Improving-matches-from-a-dating-site-with-kNN"><a href="#Improving-matches-from-a-dating-site-with-kNN" class="headerlink" title="Improving matches from a dating site with kNN"></a>Improving matches from a dating site with kNN</h2><p>约会数据 datingTestSet.txt 中包括 3 种 <strong><code>feature</code></strong> 与 1 个<strong><code>label</code></strong>:</p>
<p>(每年获得的飞行常客里程数，玩视频游戏所耗时间百分比，每周消费的冰淇淋公升数，是否喜欢)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">40920	8.326976	0.953952	largeDoses</div><div class="line">14488	7.153469	1.673904	smallDoses</div><div class="line">26052	1.441871	0.805124	didntLike</div><div class="line">75136	13.147394	0.428964	didntLike</div><div class="line">38344	1.669788	0.134296	didntLike</div><div class="line">72993	10.141740	1.032955	didntLike</div><div class="line">35948	6.830792	1.213192	largeDoses</div><div class="line">42666	13.276369	0.543880	largeDoses</div><div class="line">67497	8.631577	0.749278	didntLike</div><div class="line">35483	12.273169	1.508053	largeDoses</div><div class="line">....</div></pre></td></tr></table></figure></p>
<p>为了使得这些约会数据能够变成参数，输入分类器中进行处理，我们需要将文件中的数据转换成分类器所需要的 <strong><code>matrix</code></strong> 矩阵样式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">file2matrix</span><span class="params">(filename)</span>:</span></div><div class="line">    fr = open(filename)</div><div class="line">    arrayOLines = fr.readlines()</div><div class="line">    numberOfLines = len(arrayOLines)</div><div class="line">    returnMat = zeros((numberOfLines,<span class="number">3</span>))</div><div class="line">    classLabelVector = []</div><div class="line">    index = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> arrayOLines:</div><div class="line">        line = line.strip()</div><div class="line">        listFromLine = line.split(<span class="string">'\t'</span>)</div><div class="line">        returnMat[index,:] = listFromLine[<span class="number">0</span>:<span class="number">3</span>]</div><div class="line">        classLabelVector.append(listFromLine[<span class="number">-1</span>])</div><div class="line">        index += <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> returnMat,classLabelVector</div></pre></td></tr></table></figure>
<p>具体做法为：</p>
<ol>
<li>打开约会文件，得到文件的行数，即一共多少条数据。</li>
<li>创建一个大小合适的矩阵，并以零填充，这里简化设置矩阵的维度为 3，我们也可以按照自己的实际需求增加相应代码来适应变化的维度输入值。(这里维度为 3 的时候，<strong><code>returnMat=[[0,0,0],[0,0,0],...,[0,0,0]]</code></strong>)</li>
<li>循环处理每一条数据，<strong><code>line.strip()</code></strong>截取掉所有的回车字符，然后使用 tab 字符 \t 将上一步得到的整行数据分割成一个元素列表，存储到特征矩阵 <strong><code>returnMat</code></strong> 中。</li>
<li>原约会文件的每条数据的最后一项（是否喜欢），单独存储到标签向量<strong><code>classLabelVector</code></strong>，作为类别。</li>
<li>返回得到特征矩阵 <strong><code>returnMat</code></strong> 与标签向量<strong><code>classLabelVector</code></strong>。</li>
</ol>
<p>可以试着使用一下这个函数，检查一下数据内容：</p>
<figure class="highlight prolog"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; datingDataMat,datingLabels = kNN.file2matrix(<span class="string">'datingTestSet.txt'</span>)</div><div class="line">&gt;&gt;&gt; datingDataMat</div><div class="line">array([[<span class="number">7.29170000e+04</span>,   <span class="number">7.10627300e+00</span>,   <span class="number">2.23600000e-01</span>],</div><div class="line">       [<span class="number">1.42830000e+04</span>,   <span class="number">2.44186700e+00</span>,   <span class="number">1.90838000e-01</span>],</div><div class="line">       [<span class="number">7.34750000e+04</span>,   <span class="number">8.31018900e+00</span>,   <span class="number">8.52795000e-01</span>],</div><div class="line">       ...,</div><div class="line">       [<span class="number">1.24290000e+04</span>,   <span class="number">4.43233100e+00</span>,   <span class="number">9.24649000e-01</span>],</div><div class="line">       [<span class="number">2.52880000e+04</span>,   <span class="number">1.31899030e+01</span>,   <span class="number">1.05013800e+00</span>],</div><div class="line">       [<span class="number">4.91800000e+03</span>,   <span class="number">3.01112400e+00</span>,   <span class="number">1.90663000e-01</span>]])</div><div class="line">&gt;&gt;&gt; datingLabels[<span class="number">0</span>:<span class="number">20</span>]</div><div class="line">[<span class="string">'didntLike'</span>, <span class="string">'smallDoses'</span>, <span class="string">'didntLike'</span>, <span class="string">'largeDoses'</span>, <span class="string">'smallDoses'</span>,</div><div class="line"><span class="string">'smallDoses'</span>, <span class="string">'didntLike'</span>, <span class="string">'smallDoses'</span>, <span class="string">'didntLike'</span>, <span class="string">'didntLike'</span>, <span class="string">'largeDoses'</span>, <span class="string">'largeDose s'</span>, <span class="string">'largeDoses'</span>, <span class="string">'didntLike'</span>, <span class="string">'didntLike'</span>, <span class="string">'smallDoses'</span>, <span class="string">'smallDoses'</span>, <span class="string">'didntLike'</span>, <span class="string">'smallDoses'</span>, <span class="string">'didntLike'</span>]</div></pre></td></tr></table></figure>
<p><strong><code>datingDataMat</code></strong>特征矩阵中飞行常客里程数远远大于其他特征值，但是我们认为三种特征是同等重要的，这种不同取值范围的特征值时，我们通常采用的方法将数值归一化，如将取值范围处理为 0 到 1 或者 -1 到 1 之间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">autoNorm</span><span class="params">(dataSet)</span>:</span></div><div class="line">    minVals = dataSet.min(<span class="number">0</span>)</div><div class="line">    maxVals = dataSet.max(<span class="number">0</span>)</div><div class="line">    ranges = maxVals - minVals</div><div class="line">    normDataSet = zeros(shape(dataSet))</div><div class="line">    m = dataSet.shape[<span class="number">0</span>]</div><div class="line">    normDataSet = dataSet - tile(minVals, (m, <span class="number">1</span>))</div><div class="line">    normDataSet = normDataSet/tile(ranges, (m, <span class="number">1</span>))</div><div class="line">    <span class="keyword">return</span> normDataSet, ranges, minVals</div></pre></td></tr></table></figure>
<p>这里需要说明的一点是，我们将每列的最小值放在变量 <strong><code>minVals</code></strong> 中，将最大值放在 <strong><code>maxVals</code></strong> 中，其中 <strong><code>dataSet.min(0)</code></strong> 中的参数 0 使得函数可以从列中选取最小值，而不是选取当前行中的最小值。</p>
<ul>
<li><strong><code>numpy.chararray.min</code></strong></li>
</ul>
<blockquote>
<p><strong><code>chararray.min(axis=None, out=None, keepdims=False)</code></strong></p>
<p>（Return the minimum along a given axis.）</p>
<p> <strong>axis = 1 对行进行操作； axis = 0 对列进行操作；</strong></p>
</blockquote>
<p>虽然改变数值取值范围增加了分类器的复杂度，但可以得到更为准确的结果。</p>
<p>接着我们需要做的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">datingClassTest</span><span class="params">()</span>:</span></div><div class="line">    hoRatio = <span class="number">0.10</span></div><div class="line">    datingDataMat, datingLabels = file2matrix(<span class="string">'datingTestSet2.txt'</span>)</div><div class="line">    normMat, ranges, minVals = autoNorm(datingDataMat)</div><div class="line">    m = normMat.shape[<span class="number">0</span>]</div><div class="line">    numTestVecs = int(m*hoRatio)</div><div class="line">    errorCount = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTestVecs):</div><div class="line">        classifierResult = classify0(normMat[i, :], normMat[numTestVecs:m, :], datingLabels[numTestVecs:m], <span class="number">3</span>)</div><div class="line">        print(<span class="string">"the classifier came back with: %d,the real answer is :%d"</span>%(classifierResult, datingLabels[i]))</div><div class="line">        <span class="keyword">if</span>(classifierResult != datingLabels[i]):errorCount += <span class="number">1.0</span></div><div class="line">    print(<span class="string">"the total error rate is :%f"</span> %(errorCount/float(numTestVecs)))</div></pre></td></tr></table></figure>
<p>其中 <strong><code>numTestVecs</code></strong> 是用于测试的数据集向量，我们可以改变 <strong><code>datingClassTest</code></strong> 内变量 <strong><code>hoRatio</code></strong> 和变量 k 的值，检测错误率是否随着变量值的变化而增加，也就是调参。</p>
<hr>
<h2 id="A-handwriting-recognition-system"><a href="#A-handwriting-recognition-system" class="headerlink" title="A handwriting recognition system"></a>A handwriting recognition system</h2><p>手写识别系统</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span><span class="params">(filename)</span>:</span></div><div class="line">    returnVect = zeros((<span class="number">1</span>, <span class="number">1024</span>))</div><div class="line">    fr = open(filename)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</div><div class="line">        lineStr = fr.readline()</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):</div><div class="line">            returnVect[<span class="number">0</span>, <span class="number">32</span>*i+j] = int(lineStr[j])</div><div class="line">    <span class="keyword">return</span> returnVect</div></pre></td></tr></table></figure>
<p>该函数创建 $1 \times 1024$ 的 NumPy 数组，然后打开给定的文件，循环独读出文件的前 32 行，并将每行的头 32 个字符值存储在 NumPy 数组中，最后返回数组。（$32 \times 32=1024$）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handwritingClassTest</span><span class="params">()</span>:</span></div><div class="line">    hwLabels = []</div><div class="line">    trainingFileList = listdir(<span class="string">'trainingDigits'</span>)</div><div class="line">    m = len(trainingFileList)</div><div class="line">    trainingMat = zeros((m, <span class="number">1024</span>))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">        fileNameStr = trainingFileList[i]</div><div class="line">        fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]</div><div class="line">        classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</div><div class="line">        hwLabels.append(classNumStr)</div><div class="line">        trainingMat[i, :] = img2vector(<span class="string">'trainingDigits/%s'</span> %fileNameStr)</div><div class="line"></div><div class="line">    testFileList = listdir(<span class="string">'testDigits'</span>)</div><div class="line">    errorCount = <span class="number">0.0</span></div><div class="line">    mTest = len(testFileList)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(mTest):</div><div class="line">        fileNameStr = testFileList[i]</div><div class="line">        fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]</div><div class="line">        classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</div><div class="line">        vectorUnderTest = img2vector(<span class="string">'testDigits/%s'</span> %fileNameStr)</div><div class="line">        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, <span class="number">3</span>)</div><div class="line">        print(<span class="string">"the classifier came back with: %d, the real answer is: %d"</span> %(classifierResult, classNumStr))</div><div class="line">        <span class="keyword">if</span>(classifierResult != classNumStr):errorCount += <span class="number">1.0</span></div><div class="line">    print(<span class="string">"\n the total number of errors is: %d"</span> %errorCount)</div><div class="line">    print(<span class="string">"\n the total error rate is: %f"</span> % (errorCount/float(mTest)))</div></pre></td></tr></table></figure>
<p>步骤是：</p>
<ol>
<li>将 trainingDigits 目录中的文件内容存储在列表中 <strong><code>trainingFileList</code></strong>，然后可以得到目录中有多少文件，并将其存储在变量<strong><code>m</code></strong> 中。 </li>
<li>接着，创建一个 m 行 $1024$（$32 \times 32$）列的训练矩阵，该矩阵的每行数据都存储一个图像信息 <strong><code>trainingMat</code></strong>（用到前面定义到的<strong><code>img2vector()</code></strong> 函数）。</li>
<li>从文件名中解析出分类数字，将所有图像类别信息存储在 <strong><code>hwLabels</code></strong> 向量中。（例如 0_0.txt 的类别就是‘0’）</li>
<li>对剩下的 testDigits 目录中的测试数据进行上述 1，2，3 操作，但是不同的是，我们并不将测试数据的图像信息载入到新的矩阵当中，而是使用前面定义的 <strong><code>classify0()</code></strong> 函数测试该目录下的每个数据文件。</li>
</ol>
<p>说明：由于简化了图像信息，图像信息均是以 0 和 1 进行表示，所以不需要使用 <strong><code>autoNorm()</code></strong> 函数对数据进行规范化处理了。</p>
<h1 id="kNN 算法（-kd- 树实现方法）"><a href="#kNN 算法（-kd- 树实现方法）" class="headerlink" title="kNN 算法（$kd$ 树实现方法）"></a>kNN 算法（$kd$ 树实现方法）</h1><p>（Unfinished.）</p>
<h1 id="维度灾难"><a href="# 维度灾难" class="headerlink" title="维度灾难"></a>维度灾难 </h1><p> 在高维情形下出现的 <strong> 数据样本稀疏 </strong>、<strong> 距离计算困难 </strong> 等问题，是所有机器学习方法共同面临的严重障碍，被称为“<strong>维度灾难</strong>”（curse of dimensionality）。</p>
<p>自然会想到的一个解决办法便是 <strong> 降维</strong>（dimension reduction）。这是基于这样的一个事实：在很多时候，人们观测或收集到的数据样本虽然是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间中的一个低维“嵌入”（embedding）。</p>
<ul>
<li>若要求原始空间中样本之间的距离在低维空间中得以保持，我们可以采取“多维缩放”（Multiple Dimensional Scaling, MDS）这样一种经典的降维方法。</li>
</ul>
<p>那么假如我现在要用 kNN 邻近算法来实现分类问题，我会考虑：</p>
<ol>
<li>在训练数据集 $D$ 中寻找 $k$ 个最相似的点的扫描方法的优化，相比线性，<strong>kd 树</strong>（利用最大 / 小堆）是一个更好的选择，这对模型的最终结果无关，但是可以优化程序整体的性能。（试想，我们还可以进一步优化么？）</li>
<li>$k$ 的取值，也是至关决定性的，但一般不考虑超过 20 的整数，可以进行调参，根据结果选定对于当前问题最优的 $k$ 值。</li>
<li>被测点 $x$ 与测试数据集 $D$ 中的点的距离度量，一般采用欧式距离，标准化欧氏距离会更好么？另外，其他的距离度量方法呢？</li>
<li>选出来的 $k$ 个邻居与被测点 $x$ 的距离不同，应当权重也不同，即在分类决策规则中加入“权重”，对可能出现的不同类别，进行权重乘以距离累加，计算各类别得分，比较得分选择最终所属类别。</li>
<li>训练集与测试集的划分，应该会采用 <strong> 交叉验证法</strong>。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      本文是关于周志华「Machine Learning」这本书的 Classification - KNN &amp; kd Tree 的学习笔记。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
      <category term="Book:「Machine Learning」" scheme="http://randolph.pro/categories/Machine-Learning/Book-%E3%80%8CMachine-Learning%E3%80%8D/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>♞「Machine Learning」 Clustering</title>
    <link href="http://randolph.pro/2017/01/03/%E2%99%9E%E3%80%8CMachine%20Learning%E3%80%8D%20Clustering/"/>
    <id>http://randolph.pro/2017/01/03/♞「Machine Learning」 Clustering/</id>
    <published>2017-01-02T16:00:00.000Z</published>
    <updated>2017-08-09T14:09:34.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4307/36174772306_62cfbc8cd6_o.jpg" alt=""></p>
<p>有关该书的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/Book-「Machine-Learning」/">Book:「Machine Learning」</a></p>
<h1 id="聚类介绍"><a href="# 聚类介绍" class="headerlink" title="聚类介绍"></a>聚类介绍</h1><ul>
<li>聚类是从数据集中挖掘相似观测值集合的方法。</li>
<li>聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”（cluster）。通过这样的划分，每个簇可能对应于一些潜在的概念（类别）。</li>
<li>聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者自己来把握。</li>
<li>聚类既能作为一个单独的过程用于寻找数据内在的分布结构，也可以作为分类等其他学习任务的前驱过程。</li>
</ul>
<hr>
<h2 id="聚类算法"><a href="# 聚类算法" class="headerlink" title="聚类算法"></a>聚类算法 </h2><p><strong> 角度 I：</strong></p>
<ul>
<li><strong>基于原型的聚类（Prototype-based Clustering）</strong><ul>
<li><strong>K 均值聚类（K-means）</strong></li>
<li><strong>学习向量量化聚类（Learning Vector Quantization）</strong></li>
<li><strong>高斯混合模型聚类 （Gaussian Mixture Model）</strong></li>
</ul>
</li>
<li><strong>基于密度的聚类 （Density-based Clustering）</strong><ul>
<li><strong>DBSCAN （Density-Based Spatial Clustering of Application with Noise）</strong></li>
<li><strong>OPTICS （Ordering Points To Identify the Clustering Structure）</strong></li>
</ul>
</li>
<li><strong>层次聚类 （Hierarchical Clustering）</strong></li>
<li><strong>基于模型的聚类 （Model-based Clustering）</strong><ul>
<li><strong>混合回归模型 （Mixture Regression Model）</strong></li>
</ul>
</li>
</ul>
<p><strong>角度 II：</strong></p>
<ul>
<li><strong>基于中心的聚类： kmeans 聚类</strong></li>
<li><strong>基于分布的聚类： GMM 聚类</strong></li>
<li><strong>基于密度的聚类： DBSCAN, OPTICS</strong></li>
<li><strong>基于连通性的聚类： 层次聚类</strong></li>
<li><strong>基于模型的聚类： Miture Regression Model</strong></li>
<li><strong>其他聚类方法： 谱聚类, Chameleon, Canopy…</strong></li>
</ul>
<hr>
<h2 id="聚类数据设置"><a href="# 聚类数据设置" class="headerlink" title="聚类数据设置"></a>聚类数据设置 </h2><p> 假定样本集 $ D $ 包含 $n$ 个无标记样本：</p>
<script type="math/tex; mode=display">
D = \{x_1, x_2, \ldots, x_n \}</script><p>每个样本是一个 $p$ 维特征向量：</p>
<script type="math/tex; mode=display">
x_i=(x_{i1}; x_{i2}; \ldots; x_{ip})</script><p>聚类算法将样本集 $D$ 划分为 $k$ 个不相交的簇：</p>
<script type="math/tex; mode=display">
\{C_l|l=1, 2, \ldots, k\}</script><p>其中， <script type="math/tex">(C_{l^{'}} \cap_{l^{'} \neq l} C_{l} = \emptyset)</script> 且 <script type="math/tex">(D=\cup_{l=1}^{k}C_{l})</script>。</p>
<p>相应的，用：</p>
<script type="math/tex; mode=display">
\lambda_{i} \in {1, 2, \ldots, k}</script><p>表示样本 $x_{i}$ 的“簇标记”（cluster label）, 即：</p>
<script type="math/tex; mode=display">
x_{i} \in C_{\lambda_{i}}</script><p>于是，聚类的结果可用包含 $n$ 个元素的簇标记向量表示：</p>
<script type="math/tex; mode=display">
\lambda=(\lambda_{1}; \lambda_{2}, \ldots, \lambda_{n})</script><hr>
<h2 id="聚类性能度量"><a href="# 聚类性能度量" class="headerlink" title="聚类性能度量"></a>聚类性能度量 </h2><p><strong> 聚类性能度量亦称聚类“有效性指标”（validity index）。</strong></p>
<p><strong>设置聚类性能度量的目的:</strong></p>
<ul>
<li>对聚类结果，通过某种性能度量来评估其好坏；</li>
<li>若明确了最终将要使用的性能度量，则可直接将其作为聚类过程的优化目标，从而更好地得到符合要求的聚类结果。</li>
</ul>
<p><strong>什么样的聚类结果比较好？</strong></p>
<ul>
<li>“簇内相似度”（intra-cluster similarity）高</li>
<li>“蔟间相似度”（inter-cluster similarity）低</li>
</ul>
<p><strong>聚类性能度量分类：</strong></p>
<ul>
<li>“外部指标”（external index）：将聚类结果与某个“参考模型”（reference model）进行比较。</li>
<li>“内部指标”（internal index）：直接考察聚类结果而不利用任何参考模型。</li>
</ul>
<h3 id="外部指标"><a href="# 外部指标" class="headerlink" title="外部指标"></a>外部指标 </h3><p> 对数据集  <script type="math/tex">D = \{x_1, x_2, \ldots, x_n\}</script>，假定通过聚类，给出的簇划分为 <script type="math/tex">V=\{v_{1}, v_{2}, \ldots, v_{C}\}</script>，参考模型给出的簇划分为 <script type="math/tex">U=\{u_{1}, u_{2}\, \ldots, u_{R}\}</script>。相应地，令<script type="math/tex">\lambda</script> 与 <script type="math/tex">\lambda^{*}</script> 分别表示与 $V$ 和 $U$ 对应的簇标记向量。我们将样本两两配对考虑，定义：</p>
<script type="math/tex; mode=display">
\begin{align}
a & = |SS|, SS =\left \{(x_i,x_j)|(\lambda_i = \lambda_j, \lambda_i^* =  \lambda_j^* , i < j)\right \} \cr
b & = |SD|, SD =\left \{(x_i,x_j)|(\lambda_i = \lambda_j, \lambda_i^* \neq  \lambda_j^* , i < j)\right \} \cr
c & = |DS|, DS =\left \{(x_i,x_j)|(\lambda_i \neq \lambda_j, \lambda_i^* =  \lambda_j^* , i < j)\right \} \cr
d & = |DD|, DD =\left \{(x_i,x_j)|(\lambda_i \neq \lambda_j, \lambda_i^* \neq  \lambda_j^* , i < j)\right \}
\end{align}</script><p>其中：</p>
<ul>
<li>集合 $SS$ 包含了在 $V$ 中隶属于相同簇且在 $U$ 中也隶属于相同簇的样本对；</li>
<li>集合 $SD$ 包含了在 $V$ 中隶属于相同簇且在 $U$ 中也隶属于不同簇的样本对；</li>
<li>集合 $DS$ 包含了在 $V$ 中隶属于不同簇且在 $U$ 中也隶属于相同簇的样本对；</li>
<li>集合 $DD$ 包含了在 $V$ 中隶属于不同簇且在 $U$ 中也隶属于不同簇的样本对；</li>
</ul>
<p>这样，由于每个样本对 <script type="math/tex">((x_{i}, x_{j})(i<j))</script> 仅能出现在一个集合中，因此有：</p>
<script type="math/tex; mode=display">
a+b+c+d=n(n-1)/2</script><h4 id="JC-Jaccard-Coefficient"><a href="#JC-Jaccard-Coefficient" class="headerlink" title="JC(Jaccard Coefficient)"></a>JC(Jaccard Coefficient)</h4><script type="math/tex; mode=display">
JC = \frac{a}{a+b+c}</script><p>JC 系数的结果分布在 $[0,1]$ 区间，值越大越好。个人总结，JC 系数是一个比较“爱憎分明”的相似性度量指标，对于聚类效果越好的模型的 JC 计算结果会很好，对于聚类效果越差的模型的 JC 计算结果会很差。关于 JC 系数的数学解释及实际意义，可以参考 Wiki 对应词条，讲的非常清晰：<a href="https://en.wikipedia.org/wiki/Jaccard_index" target="_blank" rel="external">Jaccard index</a>。</p>
<p>在此我摘取其中的重点：</p>
<blockquote>
<p>When used for binary attributes, the Jaccard index is very similar to the Simple matching coefficient. The main difference is that the SMC has the term $M_{00}$ in its numerator and denominator, whereas the Jaccard index does not. Thus, the SMC compares the number of matches with the entire set of the possible attributes, whereas the Jaccard index only compares it to tshe attributes that have been chosen by at least A or B.</p>
<p><strong>In market basket analysis for example, the basket of two consumers who we wish to compare might only contain a small fraction of all the available products in the store, so the SMC would always return very small values compared to the Jaccard index. Using the SMC would then induce a bias by systematically considering, as more similar, two customers with large identical baskets compared to two customers with identical but smaller baskets; thus making the Jaccard index a better measure of similarity in that context.</strong></p>
<p>In other contexts, where 0 and 1 carry equivalent information (symmetry), the SMC is a better measure of similarity. For example, vectors of demographic variables stored in dummies variables, such as gender, would be better compared with the SMC than with the Jaccard index since the impact of gender on similarity should be equal, independent of whether male is defined as a 0 and female as a 1 or the other way around. However, when we have symmetric dummy variables, one could replicate the behaviour of the SMC by splitting the dummies into two binary attributes (in this case, male and female), thus transforming them into asymmetric attributes, allowing the use of the Jaccard index without introducing the bias. By using this trick, the Jaccard index can be considered as making the SMC a fully redundant metric. The SMC remains however more computationally efficient in the case of symmetric dummy variables since it doesn’t require adding extra dimensions.</p>
<p><strong>In general, the Jaccard index can be considered as an indicator of local “similarity” while SMC evaluates “similarity” relative to the whole “universe”.</strong>Similarity and dissimilarity must be understood in a relative sense. For example, if there are only 2 attributes (x,y), then A=(1,0) is intuitively very different from B=(0,1). However if there are 10 attributes in the “universe”, A=(1,0,0,0,0,0,0,0,0,0) and B=(0,1,0,0,0,0,0,0,0) are not intuitively so different anymore. If the focus comes back to be just on A and B, the remaining 8 attributes are often considered as redundant. As a result, A and B are very different in a “local” sense (which the Jaccard Index measures efficiently), but less different in a “global” sense (which the SMC measures efficiently). From this point of view, the choice of using SMC or the Jaccard index comes down to more than just symmetry and asymmetry of information in the attributes. The distribution of sets in the defined “universe” and the nature of the problem to be modeled should also be considered.</p>
<p>The Jaccard index is also more general than the SMC and can be used to compare other data types than just vectors of binary attributes, such as Probability measures.</p>
</blockquote>
<p>上段重点的大体意思是：JC 的一个适用场景，例如商场或者电商（亚马逊）的用户们，在买东西的时候，我们对其相似性进行判断的时候，SMC（Simple Matching Coefficient）简单匹配系数并不太适用，是因为 SMC 中添加了（两个顾客都不感兴趣的商品这一信息），而 JC 并没有考虑这一部分。对于琳琅满目的商品信息而言，两个顾客不感兴趣的商品应该远远多于他们感兴趣的商品，简而言之，如果将顾客对所有的商品向量进行标记，感兴趣的为 1，不感兴趣的为 0，那么得到的这个向量应当是一个非常稀疏的。在这种情况下，如果使用 SMC，势必会导致结果很小，但是实际上我们完全可以不考虑两个顾客都不感兴趣的内容，而是考虑两个顾客感兴趣的商品信息之和，对其进行计算，相对于 SMC，得到的结果会好解释许多。就像那句话说的，<strong>JC 就好比是一个局域性的“相似性”比较的衡量指标，而 SMC 就好比是要考虑整个宏观宇宙之下的“相似性”比较</strong>。</p>
<h4 id="FMI-Fowlkes-and-Mallows-Index"><a href="#FMI-Fowlkes-and-Mallows-Index" class="headerlink" title="FMI(Fowlkes and Mallows Index)"></a>FMI(Fowlkes and Mallows Index)</h4><script type="math/tex; mode=display">
FMI = \sqrt{\frac{a}{a+b}\cdot \frac{a}{a+c}}</script><p>FMI 系数的结果分布在 $[0,1]$ 区间，值越大越好。个人总结，FMI 系数是一个比较“温文儒雅”的相似性度量指标，对于聚类效果特别好的模型的 FMI 计算结果不会好得特别夸张，对于聚类效果特别越差的模型的 FMI 计算结果也不会差得特别夸张。同样，FMI 系数的数学解释及实际意义，对应的 Wiki 词条：<a href="https://en.wikipedia.org/wiki/Fowlkes–Mallows_index" target="_blank" rel="external">Fowlkes–Mallows index</a>。</p>
<p>在此我摘取其中的重点：</p>
<blockquote>
<p>Since the index is directly proportional to the number of true positives, a higher index means greater similarity between the two clusterings used to determine the index. <strong>One of the most basic thing to test the validity of this index is to compare two clusterings that are unrelated to each other.</strong> Fowlkes and Mallows showed that on using two unrelated clusterings, the value of this index approaches zero as the number of total data points chosen for clustering increase; whereas the value for the Rand index for the same data quickly approaches making Fowlkes–Mallows index a much more accurate representation for unrelated data. <strong>This index also performs well if noise is added to an existing dataset and their similarity compared. Fowlkes and Mallows showed that the value of the index decreases as the component of the noise increases.</strong> The index also showed similarity even when the noisy dataset had a different number of clusters than the clusters of the original dataset. Thus making it a reliable tool for measuring similarity between two clusters.</p>
</blockquote>
<p>上段重点的大体意思是：“衡量一个相似性度量指标是否可靠，应当将对于两个完全不相关簇的研究也作为重要的判定基础之一”，这样讲可能太绕了，解释一下就是如果我们对一个相似性度量指标的可靠性进行判断，不仅仅需要：我们的模型与“参考模型”两个模型越相似（聚类效果越好），指标系数越大（或越小）；同样的，我们的模型与“参考模型”越不相似（聚类效果越差），指标系数也应该呈现一个单调变大（或变小）的趋势。</p>
<p>FMI 与其他相似性度量指标的区别是，特别是与 JC 系数对比，JC 系数 $[0,1]$ 的特点是假如我们的模型聚类效果越好，结果就越趋向于 1，反之，如果我们的模型聚类效果越不好，结果就越趋向于 0；而 FMI $[0,1]$ 则不会显得那么“爱憎分明”，因为在 $[0,1]$ 范围内根号运算的存在，使得模型越好的结果不会好得特别夸张，但是模型越差的结果也不会显得特别夸张，因为根号运算起到一个“缓冲 ”的作用。另外一点，就是 FMI 系数对于含有噪声的数据集的判定效果仍然不错。</p>
<h4 id="RI-Rand-Index"><a href="#RI-Rand-Index" class="headerlink" title="RI(Rand Index)"></a>RI(Rand Index)</h4><script type="math/tex; mode=display">
RI = \frac{a+d}{a+b+c+d}</script><p>RI 系数的结果分布在 $[0,1]$ 区间，值越大越好。关于 RI 系数的数学解释及实际意义，可以参考 Wiki 对应词条，重点是要看它的改进指标 ARI：<a href="https://en.wikipedia.org/wiki/Rand_index" target="_blank" rel="external">Rand  index</a>。</p>
<h4 id="ARI-Adjusted-Rand-Index"><a href="#ARI-Adjusted-Rand-Index" class="headerlink" title="ARI(Adjusted Rand Index)"></a>ARI(Adjusted Rand Index)</h4><p>  在 <strong>RI（Rand Index）</strong> 的评判基础上，为了实现“在聚类结果随机产生的情况下，指标应该接近零”，<strong>ARI（Adjusted Rand Index）</strong>系数被提出，它具有更高的区分度。</p>
<blockquote>
<p>A problem with the Rand index is that the expected value of the Rand index of two random partitions does not take a constant value (say zero). The adjusted Rand index proposed by [Hubert and Arabie, 1985] assumes the generalized hypergeometric distribution as the model of randomness, i.e., the $U$ and $V$ partitions are picked at random such that the number of objects in the classes and clusters are fixed.</p>
<script type="math/tex; mode=display">
ARI  = \frac{Rand \ Index - expected \ index}{maximum \ index - expected \ index} = \frac{RI - E(RI)}{1-E(RI)}</script></blockquote>
<p>进行进一步的展开推导后：</p>
<script type="math/tex; mode=display">
API = \frac{\frac{a+d}{a+b+c+d} - \frac{(a+b)(a+c)+(c+d)(b+d)}{(a+b+c+d)^2}}{\frac{(a+b)(a+c)+(c+d)(b+d)}{(a+b+c+d)^2}}</script><p>ARI 系数结果分布在 $[-1,1]$ 区间，负数代表结果不好，越接近于 1 意味着聚类结果与真实情况越吻合。个人总结，ARI 系数对于任意数量的聚类中心和样本数，随机聚类的 ARI 都非常接近于 0。ARI 相比于 RI 好在，它是负数的时候就说明我们自己的模型很糟糕，有个更加相对明确的评判标准。（从广义的角度上来说，ARI 衡量的是两个数据分布的吻合程度）</p>
<h4 id="举个具体例子计算 -ARI"><a href="# 举个具体例子计算 -ARI" class="headerlink" title="举个具体例子计算 ARI"></a>举个具体例子计算 ARI</h4><p>Let <script type="math/tex">n_{ij}</script> be the number of objects that are in both class <script type="math/tex">u_{i}</script> and cluster <script type="math/tex">v_{j}</script>. Let <script type="math/tex">n_{i.}</script> and <script type="math/tex">n_{.j}</script> be the number of objects in class <script type="math/tex">u_{i}</script> and cluster <script type="math/tex">v_{j}</script> respectively.The notations are illustrated in Table below:</p>
<script type="math/tex; mode=display">
\begin{array}{c|cccc|c}
Class \ Cluster & v_{1}  & v_{2} & ... & v_{C} & Sums \cr
\hline
u_{1} & n_{11} & n_{12} & ... & n_{1C} & n_{1.}  \cr
u_{2} & n_{21} & n_{22} & ... & n_{2C} & n_{2.}  \cr
... & ... & ... & ... & ... & ...  \cr
u_{R} & n_{R1} & n_{R2} & ... & n_{RC} & n_{R.}  \cr
\hline
Sums & n_{.1} & n_{.2} & ... & n_{.C} & n.. = n  \cr
\end{array}</script><p>Here is the example:</p>
<script type="math/tex; mode=display">
\begin{array}{c|ccc|c}
Class \ Cluster & v_{1}  & v_{2} &  v_{3} & Sums \cr
\hline
u_{1} & 1 & 1 &  0 & 2  \cr
u_{2} & 1 & 2 &  1 & 4  \cr
u_{3} & 0 & 0 &  4 & 4  \cr
\hline
Sums & 2 & 3 & 5 & n = 10  \cr
\end{array}</script><p>$a$ is defined as the number of pairs of objects in the same class $U$ and same cluster in $V$,hence $a$ can be written as <script type="math/tex">\sum_{i,j} \binom{n_{ij}}{2}</script>.In Example,<script type="math/tex">a = \binom{2}{2} + \binom{4}{2} = 7</script>(所有不超过 2 的都不需要考虑，因为<script type="math/tex">\binom{1}{2} = 0</script>).</p>
<p>$b$ is defined as the number of pairs of objects in the same class in $U$ but not in the same cluster in $V$.In terms of the notation in Table, $b$ can be writtern as <script type="math/tex">\sum_{i}\binom{n_{i.}}{2}-\sum_{i,j}\binom{n_{ij}}{2}</script>. In Example, <script type="math/tex">b = \binom{2}{2} + \binom{4}{2} + \binom{4}{2} - 7 = 6</script>.</p>
<p>Similarly, $c$ is defined as the number of pairs of objects in the same cluster in $V$ but not in the same class in $U$, so $c$ can be writtern as <script type="math/tex">\sum_{j}\binom{n_{.j}}{2}-\sum_{i,j}\binom{n_{ij}}{2} = \binom{2}{2} + \binom{3}{2} + \binom{5}{2} -7 = 7</script>.</p>
<p>$d$ is defined as the number of pairs of objects that are not in the same class in $U$ and not in the same cluster in $V$. Since <script type="math/tex">a + b + c + d = \binom{n}{2}</script>, <script type="math/tex">d = \binom{10}{2} -7 - 6 - 7 = 25</script>.</p>
<p>The Rand Index for comparing the two partitions in Example is $\frac{7+25}{45} = 0.711$, while the adjusted Rand Index is 0.311.</p>
<p><strong>The Rand index is much higher than the adjusted Rand index, which is typical. Since the Rand index lies between 0 and 1, the expected value of the Rand index (although not a constant value) must be greater than or equal to 0.On the other hand, the expected value of the adjusted Rand index has value zero and the maximum value of the adjusted Rand is  also 1. Hence, there is a wider range of values that the adjusted Rand index can take on, thus increasing the sensitivity of the index.</strong></p>
<h3 id="内部指标"><a href="# 内部指标" class="headerlink" title="内部指标"></a>内部指标 </h3><p> 根据聚类结果的簇划分 <script type="math/tex">(C={C_{1}, C_{2}, \ldots, C_{k}})</script> , 定义：</p>
<ul>
<li>簇 $C$ 内样本间的平均距离</li>
</ul>
<script type="math/tex; mode=display">
avg(C)=\frac{2}{|C|(|C|-1)}\sum_{1<i<j<|C|}dist(x_{i}, x_{j})</script><ul>
<li><p>簇 $C$ 内样本间的最远距离</p>
<script type="math/tex; mode=display">
diam(C)=max_{1<i<j<|C|}dist(x_{i}, x_{j})</script></li>
<li><p>簇 <script type="math/tex">C_{i}</script> 与簇 <script type="math/tex">C_{j}</script> 最近样本间的距离</p>
<script type="math/tex; mode=display">
d{min}(C_{i}, C_{j})=min_{1<i<j<|C|}dist(x_{i}, x_{j})</script></li>
<li>簇 <script type="math/tex">C_{i}</script> 与簇 <script type="math/tex">C_{j}</script> 中心点间的距离<script type="math/tex; mode=display">
d_{cen}(C_{i}, C_{j})=dist(\mu_{i}, \mu_{j})</script></li>
</ul>
<p>其中：</p>
<ul>
<li>$dist(,)$ 是两个样本之间的距离</li>
<li>$\mu$ 是簇 $C$ 的中心点 <script type="math/tex">\mu=\frac{1}{|C|}\sum_{1<i<|C|}x_{i}</script></li>
</ul>
<h4 id="CP-Compactness"><a href="#CP-Compactness" class="headerlink" title="CP(Compactness)"></a>CP(Compactness)</h4><script type="math/tex; mode=display">
\begin{align}
\overline{CP_{i}} & =\frac{1}{|C_{i}|}\sum_{x_{i} \in C_{i}} dist(x_{i}, \mu_{i}) \cr
\overline{CP} & =\frac{1}{k}\sum_{k=1}^{k} \overline{CP_{k}}
\end{align}</script><p>CP 紧密性，其计算的是每个簇中各个点到簇中心的平均距离，值越低意味着簇内聚类距离越近，缺点就是没有考虑到簇间效果。</p>
<h4 id="SP-Separation"><a href="#SP-Separation" class="headerlink" title="SP(Separation)"></a>SP(Separation)</h4><script type="math/tex; mode=display">
SP=\frac{2}{k^2-k}\sum^{k}_{i=1}\sum^{k}_{j=i+1} d_{cen}(\mu_{i}, \mu_{j})</script><p>SP 间隔性，其计算的是各簇中心两两之间的平均距离，值越高意味着簇间距离越远，缺点是没有考虑簇内效果。</p>
<h4 id="DBI-Davies-Bouldin-Index"><a href="#DBI-Davies-Bouldin-Index" class="headerlink" title="DBI(Davies-Bouldin Index)"></a>DBI(Davies-Bouldin Index)</h4><script type="math/tex; mode=display">
DBI=\frac{1}{k}\sum^{k}_{i=1}\underset{j \neq i}{max}\bigg(\frac{avg(C_{i})+avg(C_{j})}{d_{cen}(\mu_{i}, \mu_{j})}\bigg)</script><p>关于 DBI 系数的数学解释及实际意义，可以参考 Wiki 对应词条：<a href="https://en.wikipedia.org/wiki/Davies–Bouldin_index" target="_blank" rel="external">Davies-Bouldin Index</a>。</p>
<blockquote>
<p>These conditions constrain the index so defined to be symmetric and non-negative. Due to the way it is defined, as a function of the ratio of the within cluster scatter, to the between cluster separation, a lower value will mean that the clustering is better. <strong>It happens to be the average similarity between each cluster and its most similar one, averaged over all the clusters, where the similarity is defined as $S_{i}$ above.</strong> This affirms the idea that no cluster has to be similar to another, and hence the best clustering scheme essentially minimizes the Davies–Bouldin index. This index thus defined is an average over all the $i$ clusters, and hence a good measure of deciding how many clusters actually exists in the data is to plot it against the number of clusters it is calculated over. The number <em>i</em> for which this value is the lowest is a good measure of the number of clusters the data could be ideally classified into. This has applications in deciding the value of $k$ in the <a href="https://en.wikipedia.org/wiki/Kmeans" target="_blank" rel="external">kmeans</a> algorithm, where the value of k is not known apriori. </p>
</blockquote>
<p>DBI 系数结果为非负数，值越小意味着簇内距离越小，同时簇间距离越大。个人总结，DBI 其实就是将几个 <script type="math/tex">d_{max}</script> 的值叠加，当我们模型最后聚集了多少个 Cluster 簇，就会出现多少个 <script type="math/tex">d_{max}</script>，这些<script type="math/tex">d_{max}</script> 值叠加的结果越小，说明我们模型的聚类越合理。可以通过 DBI 系数来确定 k-means 中 $k$ 的最佳值，即不同的 $k$ 值对应不同的 DBI 结果，选取 DBI 结果最小时对应的 $k$ 值，意味着此时这样划分 $k$ 个簇是最合理，效果最佳的。其缺点是，因为使用的是欧式距离，所以对于环状分布，聚类评测比较糟糕。</p>
<h4 id="DI-Dunn-Index"><a href="#DI-Dunn-Index" class="headerlink" title="DI(Dunn Index)"></a>DI(Dunn Index)</h4><script type="math/tex; mode=display">
DI=\underset{1 \leqslant i \leqslant k}{min}\bigg\{\underset{j \neq i}{min}\bigg(\frac{d_{min}(C_{i}, C_{j})}{max_{1 \leqslant l \leqslant k}diam(C_{l})}\bigg)\bigg\}</script><p>关于 DI 系数的数学解释及实际意义，可以参考 Wiki 对应词条：<a href="https://en.wikipedia.org/wiki/Dunn_index" target="_blank" rel="external">Dunn Index</a>。</p>
<blockquote>
<p>Being defined in this way, the <em>DI</em> depends on $k$, the number of clusters in the set. If the number of clusters is not known apriori, the $k$ for which the <em>DI</em> is the highest can be chosen as the number of clusters. There is also some flexibility when it comes to the definition of d(x,y) where any of the well known metrics can be used, like <a href="https://en.wikipedia.org/wiki/Manhattan_distance" target="_blank" rel="external">Manhattan distance</a> or <a href="https://en.wikipedia.org/wiki/Euclidean_distance" target="_blank" rel="external">Euclidean distance</a> based on the geometry of the clustering problem. <strong>This formulation has a peculiar problem, in that if one of the clusters is badly behaved, where the others are tightly packed, since the denominator contains a ‘max’ term instead of an average term, the Dunn Index for that set of clusters will be uncharacteristically low. This is thus some sort of a worst case indicator, and has to be kept in mind.</strong></p>
</blockquote>
<p>DI 系数结果为非负数，值越大意味着簇间距离越大，同时簇内距离越小。个人总结，DI 系数计算的是，任意两个簇的最短距离（簇间）除以任意簇中的最大距离（簇内），其优点是对于离散点的聚类评测效果不错，但缺点是对于环状分布评测效果比较差。还有上述重点提到的，因为分母采用的是取一个 max 距离而不是取平均值 average，所以会出现一个奇怪的问题，对于某个聚类效果特别差的一个簇，它可能”一个老鼠屎坏了一锅粥”这种，导致 DI 系数会特别低，这也是当我们需要使用 DI 系数时，对于某些聚类表现非常差的簇需要注意的原因。</p>
<h2 id="聚类距离计算"><a href="# 聚类距离计算" class="headerlink" title="聚类距离计算"></a>聚类距离计算 </h2><p><strong> 距离度量（distance measure）函数 $dist(,)$ 需满足的基本性质：</strong></p>
<ul>
<li><strong>非负性</strong>：<script type="math/tex">dist(x_{i}, x_{j}) \geqslant 0</script></li>
<li><strong>同一性</strong>：<script type="math/tex">dist(x_{i}, x_{j})=0</script> 当且仅当 <script type="math/tex">x_{i}=x_{j}</script></li>
<li><strong>对称性</strong>：<script type="math/tex">dist(x_{i}, x_{j})=dist(x_{j}, x_{i})</script></li>
<li><strong>直递性</strong>：<script type="math/tex">dist(x_{i}, x_{j}) \leqslant dist(x_{i}, x_{k}) + dist(x_{k}, x_{j})</script> (可不满足)</li>
</ul>
<p><strong>变量属性：</strong></p>
<ul>
<li>连续属性： 闵可夫斯基距离</li>
<li>离散属性<ul>
<li>有序属性： 闵可夫斯基距离</li>
<li>无序属性：VDM (Value Difference Metric)</li>
</ul>
</li>
<li>混合属性：闵可夫斯基距离 与 VDM 混合距离</li>
</ul>
<h3 id="闵可夫斯基距离（Minkowski-distance）"><a href="# 闵可夫斯基距离（Minkowski-distance）" class="headerlink" title="闵可夫斯基距离（Minkowski distance）"></a>闵可夫斯基距离（Minkowski distance）</h3><p>样本：<script type="math/tex">x_{i}=(x_{i1}, x_{i2}, \ldots, x_{in})</script> 与 <script type="math/tex">x_{j}=(x_{j1}, x_{j2}, \ldots, x_{jn})</script></p>
<script type="math/tex; mode=display">
dist_{mk}(x_{i}, x_{j})=\bigg(\sum_{u=1}^{n}|x_{iu}-x_{ju}|^{p}\bigg)^{\frac{1}{p}}</script><h3 id="VDM-Value-Difference-Metric"><a href="#VDM-Value-Difference-Metric" class="headerlink" title="VDM(Value Difference Metric)"></a>VDM(Value Difference Metric)</h3><blockquote>
<p>我们常将属性划分为“连续属性”（continuous attribute）和“离散属性”（categorical attribute），前者在定义域上有无穷多个可能的取值，后者在定义域上是有限个取值。然而，在讨论距离计算时，属性上是否定义了“序”关系更为重要。例如定义域为 $\lbrace 1,2,3 \rbrace$ 的离散属性与连续属性的性质更接近一些， 能直接在属性值上计算距离：“1”与“2”比较接近、与“3”比较远，这样的属性称为“有序属性”（ordinal attribute）; 而定义域为｛飞机，火车，轮船｝这样的离散属性则不能直接在属性值上计算距离，称为“无序属性”（non-ordinal attribute）。显然， 闵可夫斯基距离可用于有序属性。</p>
<p>对无序属性可采用 VDM（Value Difference Metric） 「Stanfill and Waltz, 1986」。</p>
</blockquote>
<p>令 <script type="math/tex">m_{u,a}</script> 表示在属性 $u$ 上取值为 $a$ 的样本数，<script type="math/tex">m_{u, a, i}</script> 表示在第 $i$ 个样本簇中在属性 $u$ 上取值为 $a$ 的样本数，$k$ 为样本簇数，则属性 $u$ 上两个离散值 $a$ 与 $b$ 之间的 VDM 距离为：</p>
<script type="math/tex; mode=display">
VDM_{q}(a, b)=\sum^{k}_{i=1}\bigg|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}\bigg|^{p}.</script><p>（这里的 $p$ 同闵可夫斯基距离中的 $p$ 一样）</p>
<h3 id="闵可夫斯基距离与 VDM 混合距离"><a href="# 闵可夫斯基距离与 VDM 混合距离" class="headerlink" title="闵可夫斯基距离与 VDM 混合距离"></a>闵可夫斯基距离与 VDM 混合距离 </h3><p> 假设有 <script type="math/tex">n_{c}</script> 个有序属性，<script type="math/tex">n-n_{c}</script> 个无序属性，有序属性排列在无序属性之前：</p>
<script type="math/tex; mode=display">
MinkovDM_{p}(x_{i}, x_{j})=\bigg(\sum^{n_{c}}_{u=1}|x_{i,u}-x_{j,u}|^{p}+\sum^{n}_{u=n_{c}+1}VDM_{p}(x_{i,u},x_{j,u})\bigg)^{\frac{1}{p}}</script><h3 id="加权闵可夫斯基距离"><a href="# 加权闵可夫斯基距离" class="headerlink" title="加权闵可夫斯基距离"></a>加权闵可夫斯基距离 </h3><p> 当样本在空间中不同属性的重要性不同时：</p>
<script type="math/tex; mode=display">
dist_{wmk}(x_{i}, x_{j})=(w_{1}\cdot|x_{i1}-x_{j1}|^{p}+w_{2}\cdot|x_{i2}-x_{j2}|^{p}+\ldots+w_{n}\cdot|x_{in}-x_{jn}|^{p})^{\frac{1}{p}}</script><p>其中： 权重<script type="math/tex">w_{i} \geqslant 0(i=1, 2, \ldots, p)</script> 表示不同属性的重要性，通常 <script type="math/tex">\sum_{i=1}^{n}w_{i}=1</script>。</p>
<hr>
<h1 id="聚类算法介绍及实现"><a href="# 聚类算法介绍及实现" class="headerlink" title="聚类算法介绍及实现"></a>聚类算法介绍及实现 </h1><p><strong> 聚类算法类型：</strong></p>
<ul>
<li><p><strong>基于原型的聚类（Prototype-based Clustering）</strong></p>
<ul>
<li><strong>K 均值聚类（K-means）</strong></li>
<li><strong>学习向量量化聚类（Learning vector Quantization）</strong></li>
<li><strong>高斯混合聚类（Mixture-of-Gaussian）</strong></li>
</ul>
</li>
<li><p><strong>基于密度的聚类（Density-based Clustering）</strong></p>
</li>
<li><p><strong>层次聚类（Hierarchical Clustering）</strong></p>
</li>
</ul>
<hr>
<h2 id="基于原型的聚类"><a href="# 基于原型的聚类" class="headerlink" title="基于原型的聚类"></a>基于原型的聚类 </h2><p><strong> 基于原型的聚类（Prototype-based Clustering），此类算法假设聚类结构能通过一组原形刻画。通常情况下，算法先对原型进行初始化，然后对原型进行迭代更新求解，采用不同的原型表示，不同的求解方式，将产生不同的算法。</strong></p>
<ul>
<li><strong>基于原型的聚类（Prototype-based Clustering）</strong><ul>
<li><strong>K 均值聚类（K-means）</strong></li>
<li><strong>学习向量量化聚类（Learning vector Quantization）</strong></li>
<li><strong>高斯混合聚类（Mixture-of-Gaussian）</strong></li>
</ul>
</li>
</ul>
<h3 id="K 均值聚类（K-means）"><a href="#K 均值聚类（K-means）" class="headerlink" title="K 均值聚类（K-means）"></a>K 均值聚类（K-means）</h3><h4 id="算法介绍"><a href="# 算法介绍" class="headerlink" title="算法介绍"></a>算法介绍 </h4><p> 给定样本集 <script type="math/tex">D= \lbrace x_{1}, x_{2}, \ldots, x_{n} \rbrace</script>， K-means 算法针对聚类所得簇划分 <script type="math/tex">C= \lbrace C_{1}, C_{2}, \ldots, C_{k} \rbrace</script>，最小化平方误差：</p>
<script type="math/tex; mode=display">
E = \sum^{k}_{i=1}\sum_{x \in C_{i}}|x-\mu_{i}|_{2}^{2}</script><p>其中 <script type="math/tex">\mu_{i}</script> 是簇 <script type="math/tex">C_{i}</script> 的均值向量：</p>
<script type="math/tex; mode=display">
\mu_{i}=\frac{1}{|C_{i}|}\sum_{x \in C_{i}}x</script><p>直观上看，$E$ 在一定程度上刻画了簇内样本围绕均值向量的紧密程度， $E$ 值越小簇内样本相似度越高。但最小化 $E$ 不容易，是一个 NP 难问题， K-means 算法采用了贪心策略，通过迭代优化来近似求解 $E$ 的最小值。具体算法如下：</p>
<p><img src="https://farm5.staticflickr.com/4303/36049857562_cf0480eb62_o.png" alt="$k$ 均值算法"></p>
<h4 id="算法实现 -（R 语言）"><a href="# 算法实现 -（R 语言）" class="headerlink" title="算法实现 （R 语言）"></a>算法实现 （R 语言）</h4><p>Python 的实现可以参考<a href="http://randolph.pro/2016/03/19/%3CProgramming%20Collective%20Intelligence%3E%20Chapter%203/">「Programming Collective Intelligence」 Chapter 3</a> 这篇文章。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">head(iris)</div></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">  Sepal.Length Sepal.Width Petal.Length Petal.Width Species</div><div class="line"><span class="number">1</span>          <span class="number">5.1</span>         <span class="number">3.5</span>          <span class="number">1.4</span>         <span class="number">0.2</span>  setosa</div><div class="line"><span class="number">2</span>          <span class="number">4.9</span>         <span class="number">3.0</span>          <span class="number">1.4</span>         <span class="number">0.2</span>  setosa</div><div class="line"><span class="number">3</span>          <span class="number">4.7</span>         <span class="number">3.2</span>          <span class="number">1.3</span>         <span class="number">0.2</span>  setosa</div><div class="line"><span class="number">4</span>          <span class="number">4.6</span>         <span class="number">3.1</span>          <span class="number">1.5</span>         <span class="number">0.2</span>  setosa</div><div class="line"><span class="number">5</span>          <span class="number">5.0</span>         <span class="number">3.6</span>          <span class="number">1.4</span>         <span class="number">0.2</span>  setosa</div><div class="line"><span class="number">6</span>          <span class="number">5.4</span>         <span class="number">3.9</span>          <span class="number">1.7</span>         <span class="number">0.4</span>  setosa</div></pre></td></tr></table></figure>
<p>After a little bit of exploration, I found that Petal.Length and Petal. Width were similar among the same species but varied considerably between different species, as demonstrated below:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(ggplot2)</div><div class="line">ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()</div></pre></td></tr></table></figure>
<p><img src="https://farm1.staticflickr.com/33/31680514905_7dba9b551a_o.jpg" alt=""></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">set.seed(<span class="number">20</span>)</div><div class="line">irisCluster &lt;- kmeans(iris[, <span class="number">3</span>:<span class="number">4</span>], centers = <span class="number">3</span>, nstart = <span class="number">20</span>)</div><div class="line">irisCluster</div></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">K-means clustering with <span class="number">3</span> clusters of sizes <span class="number">50</span>, <span class="number">52</span>, <span class="number">48</span></div><div class="line"></div><div class="line">Cluster means:</div><div class="line">  Petal.Length Petal.Width</div><div class="line"><span class="number">1</span>     <span class="number">1.462000</span>    <span class="number">0.246000</span></div><div class="line"><span class="number">2</span>     <span class="number">4.269231</span>    <span class="number">1.342308</span></div><div class="line"><span class="number">3</span>     <span class="number">5.595833</span>    <span class="number">2.037500</span></div><div class="line"></div><div class="line">Clustering vector:</div><div class="line">  [<span class="number">1</span>] <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></div><div class="line"> [<span class="number">33</span>] <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span></div><div class="line"> [<span class="number">65</span>] <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">3</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">3</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span></div><div class="line"> [<span class="number">97</span>] <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">2</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">2</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">2</span> <span class="number">3</span></div><div class="line">[<span class="number">129</span>] <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">2</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span></div><div class="line"></div><div class="line">Within cluster sum of squares by cluster:</div><div class="line">[<span class="number">1</span>]  <span class="number">2.02200</span> <span class="number">13.05769</span> <span class="number">16.29167</span></div><div class="line"> (between_SS / total_SS =  <span class="number">94.3</span> %)</div><div class="line"></div><div class="line">Available components:</div><div class="line"></div><div class="line">[<span class="number">1</span>] <span class="string">"cluster"</span>      <span class="string">"centers"</span>      <span class="string">"totss"</span>        <span class="string">"withinss"</span>    </div><div class="line">[<span class="number">5</span>] <span class="string">"tot.withinss"</span> <span class="string">"betweenss"</span>    <span class="string">"size"</span>         <span class="string">"iter"</span>        </div><div class="line">[<span class="number">9</span>] <span class="string">"ifault"</span></div></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">table(irisCluster$cluster, iris$Species)</div></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"> </div><div class="line">  setosa versicolor virginica</div><div class="line"><span class="number">1</span>     <span class="number">50</span>          <span class="number">0</span>         <span class="number">0</span></div><div class="line"><span class="number">2</span>      <span class="number">0</span>         <span class="number">48</span>         <span class="number">4</span></div><div class="line"><span class="number">3</span>      <span class="number">0</span>          <span class="number">2</span>        <span class="number">46</span></div></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; irisCluster$cluster &lt;- as.factor(irisCluster$cluster)</div><div class="line">&gt; ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point()</div></pre></td></tr></table></figure>
<p><img src="https://farm1.staticflickr.com/353/31565301411_fc7281b525_o.jpg" alt=""></p>
<hr>
<h3 id="学习向量量化聚类（Learning-vector-Quantization）"><a href="# 学习向量量化聚类（Learning-vector-Quantization）" class="headerlink" title="学习向量量化聚类（Learning vector Quantization）"></a>学习向量量化聚类（Learning vector Quantization）</h3><h4 id="算法介绍 -1"><a href="# 算法介绍 -1" class="headerlink" title="算法介绍"></a>算法介绍</h4><p><strong>LVQ 假设数据样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类。</strong></p>
<p>给定样本集 <script type="math/tex">D=\lbrace (x_{1}, y_{1}), (x_{2}, y_{2}), \ldots, (x_{m}, y_{m}) \rbrace</script>，每个样本 <script type="math/tex">x_{j}</script> 是由 $n$ 个属性描述的特征向量 <script type="math/tex">(x_{j1}; x_{j2}; \ldots; x_{jn})</script>,<script type="math/tex">y_{j} \in \mathcal{Y}</script> 是样本 <script type="math/tex">x_{j}</script> 的类别标记。LVQ 的目标是学得一组 $n$ 维原型向量<script type="math/tex">\lbrace p_{1}, p_{2}, \ldots, p_{q} \rbrace</script>, 每个原型向量代表一个聚类簇，簇标记为<script type="math/tex">t_{i}\in \mathcal{Y}</script>。</p>
<p>具体算法如下：</p>
<p><img src="https://farm5.staticflickr.com/4320/36049858912_c400743b7f_o.png" alt="学习向量量化方法"></p>
<p><strong>算法解释</strong></p>
<ul>
<li>算法第 1 行：对原型向量进行初始化。例如：对第 $i,i=(1,2,\ldots,q)$ 个簇，可以从类别标记为 $t_{i}$ 的样本中随机选取一个作为原型向量。</li>
<li><p>算法第 2-12 行：对原型向量进行迭代优化。在每一轮迭代中，算法随机选取一个有标记训练样本，找出与其距离最近的原型向量，并根据两者的类别标记是否一致来对原型向量进行相应的更新。</p>
<ul>
<li>算法第 5 行：这是竞争学习的“胜者为王“的策略。SOM 是基于无标记样本的聚类算法，而 LVQ 可以看作是 SOM 基于监督信息的扩展。</li>
<li><p>算法第 6-10 行：如何更新原型向量。对样本 $x_{j}$，</p>
<ul>
<li><p>若距离 <script type="math/tex">x_{j}</script> 最近的原型向量 <script type="math/tex">p_{i^{*}}</script> 与 <script type="math/tex">x_{j}</script> 的标记相同，则令 <script type="math/tex">p_{i^{*}}</script> 向 <script type="math/tex">x_{j}</script> 的方向靠拢，此时新的原型向量为 </p>
<script type="math/tex; mode=display">
p' = p_{i^{*}} + \eta \cdot (x_{j}-p_{i^{*}})</script><p>​        $p^{‘}$ 与 $x_{j}$ 之间的距离为 </p>
<script type="math/tex; mode=display">
||p'-x_{j}||_{2}=(1-\eta) \cdot ||p_{i^{*}}-x_{j}||_{2}</script><p>​       原型向量 <script type="math/tex">p_{i^{*}}</script> 更新为 <script type="math/tex">p'</script>之后将更接近 <script type="math/tex">x_{j}</script>。</p>
</li>
<li>若距离 <script type="math/tex">x_{j}</script> 最近的原型向量 <script type="math/tex">p_{i^{*}}</script> 与 <script type="math/tex">x_{j}</script> 的标记不同，则令 <script type="math/tex">p_{i^{*}}</script> 向 <script type="math/tex">x_{j}</script> 的方向远离，此时新的原型向量为 <script type="math/tex; mode=display">
p'= p_{i^{*}} - \eta \cdot (x_{j}-p_{i^{*}})</script>​       $p^{‘}$ 与 $x_{j}$ 之间的距离为 <script type="math/tex; mode=display">
||p'-x_{j}|_{2}=(1+\eta) \cdot |p_{i^{*}}-x_{j}||_{2}</script>​       原型向量 <script type="math/tex">p_{i^{*}}</script> 更新为 <script type="math/tex">p'</script>之后将更远离 <script type="math/tex">x_{j}</script>。</li>
</ul>
</li>
</ul>
</li>
<li>算法第 12 行：若算法的停止条件已满足(例如已达到最大迭代轮数，或原型向量更新很小甚至不再更新)，则将当前原型向量作为最终结果返回。</li>
<li>在学得一组原型向量 <script type="math/tex">\lbrace p_{1}, p_{2}, \ldots, p_{q} \rbrace</script> 后即可实现对样本空间 <script type="math/tex">\mathcal{X}</script> 的簇划分。<ul>
<li>对任意样本 $x$, 他将被划入与其距离最近的原型向量所代表的簇中，每个原型向量<script type="math/tex">p_{i}</script> 定义了与之相关的一个区域 <script type="math/tex">R_{i}</script>，该区域中每个样本与 <script type="math/tex">p_{i}</script> 的距离不大于他与其他原型向量 <script type="math/tex">p_{i'} (i \neq i')</script>，即 <script type="math/tex; mode=display">
R_{i}= \lbrace x \in \mathcal{X} \ |\ ||x-p_{i}||_{2} \leqslant ||x-p_{i'}||_{2}, i \neq i'\rbrace</script>​       由此形成了对样本空间 $\mathcal{X}$ 的簇划分 <script type="math/tex">{R_{1}, R_{2}, \ldots, R_{q}}</script>，该划分通常称为“Voronoi 剖分”（Voronoi tessellation）。</li>
</ul>
</li>
</ul>
<h4 id="算法实现"><a href="# 算法实现" class="headerlink" title="算法实现"></a>算法实现</h4><p>（Unfinished.）</p>
<h4 id="补充"><a href="# 补充" class="headerlink" title="补充"></a>补充 </h4><h5 id="竞争型学习"><a href="# 竞争型学习" class="headerlink" title="竞争型学习"></a> 竞争型学习 </h5><p> 竞争型学习是神经网络中一种常见的无监督学习策略，在使用该策略时，网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制。这种机制亦称为”胜者通吃”(winner-take-all)原则。</p>
<h5 id="ART- 网络"><a href="#ART- 网络" class="headerlink" title="ART 网络"></a>ART 网络</h5><p>ART(Adaptive Resonance Theory, 自适应谐振理论）网络「Carpenter and Grossberg, 1987」是竞争型学习的重要代表。该网络由比较层、识别层、识别阈值和重置模块构成。其中，比较层负责接收输入样本，并将其传递给识别层神经元。识别层每个神经元对应一个模式类（模式类可以认为是某类别的“子类”），神经元数目可在训练过程中动态增长以增加新的模式类。</p>
<p>在接收到比较层的输入信号后，识别层神经元之间相互竞争以产生获胜神经元。竞争的最简单方式是，计算输入向量与每个识别层神经元所对应的模式类的代表向量之间的距离，距离最小者胜．获胜神经元将向其他识别层神经元发送信号，抑制其激活。若输入向量与获胜神经元所对应的代表向量之间的相似度大于识别阈值，则当前输入样本将被归为该代表向量所属类别，同时，网络连接权将会更新，使得以后在接收到相似输入样本时该模式类会计算出更大的相似度，从而使该获胜神经元有更大可能获胜；若相似度不大于识别阈值，则重置模块将在识别层增设一个新的神经元，其代表向量就设置为当前输入向量。</p>
<p>显然，识别阈值对 ART 网络的性能有重要影响。当识别阈值较高时，输入样本将会被分成比较多、比较精细的模式类，而如果识别阈值较低，则会产生比较少、比较粗略的模式类。</p>
<p>ART 比较好地缓解了竞争型学习中的“可塑性一稳定性窘境”（stability— plasticity dilemma），可塑性是指神经网络要有学习新知识的能力，而稳定性则是指神经网络在学习新知识时要保持对旧知识的记忆．这就使得 ART 网络具有一个很重要的优点：可进行 <strong> 增量学习（incremental learning）</strong>或 <strong> 在线学习（online learning)</strong>。</p>
<p>早期的 ART 网络只能处理布尔型输入数据，此后 ART 发展成了一个算法族，包括能处理实值输入的 ART2 网络、结合模糊处理的 FuzzyART 网络，以及可进行监督学习的 ARTMAP 网络等。 </p>
<h5 id="SOM- 网络"><a href="#SOM- 网络" class="headerlink" title="SOM 网络"></a>SOM 网络</h5><p>SOM(Self-Organizing Map，自组织映射）网络「Kohollen, 1982」是一种竞争学习型的无监督神经网络，它能将高维输入数据映射到低维空间（通常为二维），同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元。</p>
<p><img src="https://farm6.staticflickr.com/5569/31429001070_b16d937809_o.png" alt=""></p>
<p>如图所示，SOM 网络中的输出层神经元以矩阵方式排列在二维空间中，每个神经元都拥有一个权向量，网络在接收输入向量后，将会确定输出层获胜神经元，它决定了该输入向量在低维空间中的位置。SOM 的训练目标就是为每个输出层神经元找到合适的权向量，以达到保持拓扑结构的目的。 SOM 的训练过程很简单：在接收到一个训练样本后，每个输出层神经元会计算该样本与自身携带的权向量之间的距离，距离最近的神经元成为竞争获胜者，称为最佳匹配单元（best matching unit)。然后，最佳匹配单元及其邻近神经元的权向量将被调整，以使得这些权向量与当前输入样本的距离缩小。这个过程不断迭代，直至收敛。</p>
<hr>
<h3 id="高斯混合聚类（Mixture-of-Gaussian）"><a href="# 高斯混合聚类（Mixture-of-Gaussian）" class="headerlink" title="高斯混合聚类（Mixture-of-Gaussian）"></a>高斯混合聚类（Mixture-of-Gaussian）</h3><h4 id="算法介绍 -2"><a href="# 算法介绍 -2" class="headerlink" title="算法介绍"></a>算法介绍 </h4><p> 与 $k$ 均值、LVQ 用原型向量来刻画聚类结构不同，<strong>高斯混合聚类 (Mixture-of-Gaussian) 采用概率模型来表达聚类原型</strong>。</p>
<h5 id="（多元）高斯分布："><a href="#（多元）高斯分布：" class="headerlink" title="（多元）高斯分布："></a>（多元）高斯分布：</h5><p>对 $n$ 维样本空间 $\mathcal{X}$ 中的随机向量 $x$，若 $x$ 服从 (多元) 高斯分布，其概率密度函数为：</p>
<script type="math/tex; mode=display">
p(x| \mu, \Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} e^{- \frac{1}{2} (x-\mu)^{T} \Sigma^{-1} (x-\mu)}</script><p>其中：$\mu​$ 是 $n​$ 维均值向量，$\Sigma​$ 是 $n \times n​$ 协方差矩阵。($\Sigma​$：对称正定矩阵；$|\Sigma|​$：$\Sigma​$ 的行列式；$\Sigma^{-1}​$：$\Sigma​$ 的逆矩阵)</p>
<h5 id="（多元）高斯混合分布："><a href="#（多元）高斯混合分布：" class="headerlink" title="（多元）高斯混合分布："></a>（多元）高斯混合分布：</h5><p>对 $n$ 维样本空间 $\mathcal{X}$ 中的随机向量 $x$，若 $x$ 服从（多元）高斯混合分布，其概率密度函数为：</p>
<script type="math/tex; mode=display">
p_{\mathcal{M}}(x)=\sum_{i=1}^{k}\alpha_{i} \cdot p(x| \mu_{i}, \Sigma_{i})</script><p>该分布由 $k$ 个混合成分组成，每个成分对应一个（多元）高斯分布，其中：<script type="math/tex">\mu_{i}</script>, <script type="math/tex">\Sigma_{i}</script> 是第 $i$ 个高斯混合成分的参数， 而 <script type="math/tex">\alpha_{i}</script> 为相应的“混合系数”(mixture coeffcient)， <script type="math/tex">\sum_{i=1}^{k}\alpha_{i}=1</script>。</p>
<h5 id="样本集的生成模型："><a href="# 样本集的生成模型：" class="headerlink" title="样本集的生成模型："></a>样本集的生成模型：</h5><p>假设样本集 <script type="math/tex">D= \lbrace x_{1}, x_{2}, \ldots, x_{n} \rbrace</script> 的生成过程由高斯混合分布给出：</p>
<ul>
<li><p>首先：根据 <script type="math/tex">\alpha_{1}, \alpha_{2}, \ldots,\alpha_{k}</script> 定义的先验分布选择高斯混合成分，其中 <script type="math/tex">\alpha_{i}</script> 为选择第 $i$ 个混合成分的概率；</p>
</li>
<li><p>然后：根据被选择的混合成分的概率密度函数进行采样， 生成相应的样本。</p>
</li>
</ul>
<p>令随机变量 <script type="math/tex">z_{j} \in \lbrace 1, 2, \ldots,k \rbrace</script> 表示生成样本 <script type="math/tex">x_{j}</script> 的高斯混合成分， 其取值未知。</p>
<p>$z_{j}$ 的先验概率：</p>
<script type="math/tex; mode=display">
P(z{j}=i)=\alpha{i} \quad (i=1, 2, \ldots,k).</script><p>由 Bayesian 定理得 $z_{j}$ 的后验分布为：</p>
<script type="math/tex; mode=display">
\begin{align*}
P_{\mathcal{M}}(z_{j}=i|x_{j}) &=\frac{P(z_{j}=i) \cdot p_{\mathcal{M}}(x_{j}|z_{j}=i)}{p_{\mathcal{M}}(x_{j})} \cr
& =\frac{\alpha_{i}\cdot p(x_{j}|\mu_{i},\Sigma_{i})}{\sum^{k}_{l=1}\alpha_{l}\cdot p(x_{j}|\mu_{l},\Sigma_{l})}
\end{align*}</script><p>可知，<script type="math/tex">P_{\mathcal{M}}(z_{j}=i|x_{j})</script> 给出了样本 <script type="math/tex">x_{j}</script> 由第 $i$ 个高斯混合成分生成的后验概率，记：</p>
<script type="math/tex; mode=display">
\gamma_{ji}=P_{\mathcal{M}}(z_{j}=i|x_{j}) \quad (i=1, 2, \ldots,k)</script><h5 id="高斯混合聚类策略："><a href="# 高斯混合聚类策略：" class="headerlink" title="高斯混合聚类策略："></a>高斯混合聚类策略：</h5><ul>
<li>若（多元）高斯混合分布 $p_{\mathcal{M}}(x)$ 已知，高斯混合聚类将把样本集 $D$ 划分为 $k$ 个簇：<script type="math/tex; mode=display">
C= \lbrace C_{1}, C_{2}, \ldots,C_{k} \rbrace</script></li>
</ul>
<p>每个样本 <script type="math/tex">x_{j}</script> 的簇标记 <script type="math/tex">\lambda_{j}</script> 为:</p>
<script type="math/tex; mode=display">
\lambda_{j}=arg \underset{i \in {1, 2, \ldots,k}}{max}\gamma_{ji}</script><ul>
<li>（多元）高斯混合分布 <script type="math/tex">p_{\mathcal{M}}(x)</script> 参数 <script type="math/tex">\lbrace (\alpha_{i},\mu_{i}, \Sigma_{i})|1 \leqslant i \leqslant k \rbrace</script> 的求解采用极大似然估计(MLE):</li>
</ul>
<p>给定样本集 $D$， 最大化（对数）似然函数：</p>
<script type="math/tex; mode=display">
\begin{align*}
LL(D) & = ln\Big(\prod^{n}_{j=1}p_{\mathcal{M}}(x_{j})\Big) \cr
&=\sum^{n}_{j=1}\Big(\sum^{k}_{i=1}\alpha_{i}\cdot p(x_{j}|\mu_{i}, \Sigma_{i})\Big)
\end{align*}</script><p>MLE 解为：</p>
<script type="math/tex; mode=display">
\begin{align}
\mu_{i} & = \frac{\sum^{n}_{j=1}\gamma_{ji}x_{j}}{\sum^{n}_{j=1}\gamma{ji}} \cr
\Sigma_{i} & = \frac{\sum^{n}_{j=1}\gamma_{ji}(x_{j}-\mu_{i})(x_{j}-\mu_{i})^{T}}{\sum^{n}_{j=1}\gamma_{ji}} \cr
\alpha_{i} &= \frac{1}{n}\sum^{n}_{j=1}\gamma_{ji}
\end{align}</script><p>具体算法如下：<br><img src="https://farm5.staticflickr.com/4295/35824129960_2e09d0a8b8_o.png" alt="高斯混合聚类算法"></p>
<h4 id="算法实现 -1"><a href="# 算法实现 -1" class="headerlink" title="算法实现"></a>算法实现</h4><p>（Unfinished）</p>
<hr>
<h1 id="各聚类算法过程"><a href="# 各聚类算法过程" class="headerlink" title="各聚类算法过程"></a><strong>各聚类算法过程 </strong></h1><p> 使用西瓜数据集合：</p>
<script type="math/tex; mode=display">
\begin{array}{ccc|ccc|ccc}
\hline
编号 & 密度 & 含糖率 & 编号 & 密度 & 含糖率 & 编号 & 密度 & 含糖率 \cr
\hline
1 & 0.697 & 0.460 & 11 & 0.245 & 0.057 & 21 & 0.748 & 0.232 \cr
2 & 0.774 & 0.376 & 12 & 0.343 & 0.099 & 22 & 0.714 & 0.346 \cr
3 & 0.634 & 0.264 & 13 & 0.639 & 0.161 & 23 & 0.483 & 0.312 \cr
4 & 0.608 & 0.318 & 14 & 0.657 & 0.198 & 24 & 0.478 & 0.437 \cr
5 & 0.556 & 0.215 & 15 & 0.360 & 0.370 & 25 & 0.525 & 0.369 \cr
6 & 0.403 & 0.237 & 16 & 0.593 & 0.042 & 26 & 0.751 & 0.489 \cr
7 & 0.481 & 0.149 & 17 & 0.719 & 0.103 & 27 & 0.532 & 0.472 \cr
8 & 0.437 & 0.211 & 18 & 0.359 & 0.188 & 28 & 0.473 & 0.376 \cr
9 & 0.666 & 0.091 & 19 & 0.339 & 0.241 & 29 & 0.725 & 0.445 \cr
10 & 0.243 & 0.267 & 20 & 0282 & 0.257 & 30 & 0.446 & 0.459 \cr
\end{array}</script><p>（其中编号为 9~21 的类别是”坏瓜”，其他样本的类别是”好瓜”） </p>
<h2 id="使用 -k- 均值算法"><a href="# 使用 -k- 均值算法" class="headerlink" title="使用 $k$ 均值算法"></a>使用 $k$ 均值算法 </h2><p> 假定聚类簇数 $k=3$，算法开始时随机选取的三个样本 <script type="math/tex">x_{6},x_{12},x_{27}</script> 作为初始均值向量，即：</p>
<script type="math/tex; mode=display">
\mu_{1} = (0.403;0.237), \mu_{2} = (0.343;0.099),\mu_{3} = (0.532;0.472).</script><p>考察样本 <script type="math/tex">x_{1}=(0.697;0.460)</script>，它与当前均值向量 <script type="math/tex">\mu_{1},\mu_{2},\mu_{3}</script>的距离分别是<script type="math/tex">0.369, 0.506, 0.166</script>，因此 <script type="math/tex">x_{1}</script> 将被划入簇 <script type="math/tex">C_{3}</script> 中。类似的，对数据集中的所有样本考察一遍后，可得当前簇划分为：</p>
<script type="math/tex; mode=display">
\begin{align}
C_{1} & = \lbrace x_{5},x_{6},x_{7},x_{8},x_{9},x_{10},x_{13},x_{14},x_{15},x_{17},x_{18},x_{19},x_{20},x_{23} \rbrace; \cr
C_{2} & = \lbrace x_{11},x_{12},x_{16} \rbrace; \cr
C_{3} & = \lbrace x_{1},x_{2},x_{3},x_{4},x_{21},x_{22},x_{24},x_{25},x_{26},x_{27},x_{28},x_{29},x_{30} \rbrace
\end{align}</script><p>于是，可以从 <script type="math/tex">C_{1},C_{2},C_{3}</script> 分别求出新的均值向量：</p>
<script type="math/tex; mode=display">
\mu_{1}^{'} = (0.473;0.214), \mu_{2}^{'} = (0.394;0.066),\mu_{3}^{'} = (0.623;0.388).</script><p>更新当前均值向量后，不断重复上述过程，如图所示，第五轮迭代产生的结果与第四轮迭代结果相同，于是算法停止，得到最终的簇划分，其中样本点与均值向量分别用”●”与”+”表示，红色虚线显示出簇划分。</p>
<p><img src="https://farm1.staticflickr.com/403/30990399723_0db34a3cde_o.png" alt=""></p>
<h2 id="使用学习向量量化"><a href="# 使用学习向量量化" class="headerlink" title="使用学习向量量化"></a>使用学习向量量化 </h2><p> 令数据集中编号为 9-21 的样本的类别标记为 <script type="math/tex">c_{2}</script> ，其他样本的类别标记为 <script type="math/tex">c_{1}</script>。假定 $q=5$，即学习目标是找到 5 个原型向量 <script type="math/tex">p_{1},p_{2},p_{3},p_{4},p_{5}</script>，并假定其对应的类别标记分别为<script type="math/tex">c_{1},c_{2},c_{2},c_{1},c_{1}</script> 。</p>
<p>算法开始时，根据样本的类别标记和簇的预设类别标记，对原型向量进行随机初始化，假定初始化为样本 <script type="math/tex">x_{5},x_{12},x_{18},x_{23},x_{29}</script> 。在第一轮迭代中，假定随机选取的样本为 <script type="math/tex">x_{1}</script> ，该样本与当前原型向量 <script type="math/tex">p_{1},p_{2},p_{3},p_{4},p_{5}</script> 的距离分别为 <script type="math/tex">0.283, 0.506, 0.434, 0.260, 0.032</script> 。由于 <script type="math/tex">p_{5}</script> 与 <script type="math/tex">x_{1}</script> 距离最近且两者具有相同的类别标记 <script type="math/tex">c_{2}</script> ，假定学习率 $\eta = 0.1$ ，则 LVQ 更新 <script type="math/tex">p_{5}</script> 得到新原型向量：</p>
<script type="math/tex; mode=display">
\begin{align}
p^{'} & = p_{5} + \eta \cdot (x_{1} - p_{5}) \cr
      & = (0.725;0.445) + 0.1 \cdot ((0.697;0.460) - (0.725;0.445)) \cr
      & = (0.722;0.442).
\end{align}</script><p>将 <script type="math/tex">p_{5}</script> 更新为 <script type="math/tex">p^{'}</script> 后，不断重复上述过程，不同轮数之后的聚类结果如下图所示，其中 <script type="math/tex">c_{1}</script> ， <script type="math/tex">c_{2}</script> 类别样本点与原型向量分别用”●”,”○”与”+”表示，红色虚线显示出聚类形成的 Voronoi 剖分。</p>
<p><img src="https://farm1.staticflickr.com/715/31427609470_fe9357f0cc_o.png" alt=""></p>
<h2 id="使用高斯混合聚类"><a href="# 使用高斯混合聚类" class="headerlink" title="使用高斯混合聚类"></a>使用高斯混合聚类 </h2><p> 令高斯混合成分的个数 $k=3$。算法开始时，假定将高斯混合分布的模型参数初始化为：<script type="math/tex">\alpha_{1} = \alpha_{2} = \alpha_{3} = \frac{1}{3}</script>；<script type="math/tex">\mu_{1} = x_{6}</script>，<script type="math/tex">\mu_{2}=x_{22}</script>，<script type="math/tex">\mu_{3} = x_{27}</script>；<script type="math/tex">\Sigma_{1} = \Sigma_{2} = \Sigma_{3} = \binom{0.1 \ 0.0}{0.0 \ 0.1}</script>。</p>
<p>在第一轮迭代中，先计算样本由各混合成分生成的后验概率。</p>
]]></content>
    
    <summary type="html">
    
      本文是关于周志华「Machine Learning」这本书的 Clustering 的学习笔记。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
      <category term="Book:「Machine Learning」" scheme="http://randolph.pro/categories/Machine-Learning/Book-%E3%80%8CMachine-Learning%E3%80%8D/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>♞「Machine Learning」 Chapter 2</title>
    <link href="http://randolph.pro/2016/12/30/%E2%99%9E%E3%80%8CMachine%20Learning%E3%80%8D%20Chapter%202/"/>
    <id>http://randolph.pro/2016/12/30/♞「Machine Learning」 Chapter 2/</id>
    <published>2016-12-29T16:00:00.000Z</published>
    <updated>2017-08-15T08:17:01.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4307/36174772306_62cfbc8cd6_o.jpg" alt=""></p>
<p>有关该书的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/Book-「Machine-Learning」/">Book:「Machine Learning」</a></p>
<h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><ul>
<li><strong>过拟合与欠拟合</strong></li>
<li><strong>评估方法</strong><ul>
<li><strong>留出法</strong></li>
<li><strong>交叉验证法</strong></li>
<li><strong>自助法</strong></li>
</ul>
</li>
<li><strong>性能度量</strong><ul>
<li><strong>错误率与精度</strong></li>
<li><strong>查准率、查全率与 F1</strong></li>
<li><strong>ROC 与 AUC</strong></li>
<li><strong>代价敏感错误率与代价曲线</strong></li>
<li><strong>比较检验</strong></li>
<li><strong>偏差与方差（Bias-Variance）</strong></li>
</ul>
</li>
</ul>
<hr>
<h1 id="Key"><a href="#Key" class="headerlink" title="Key:"></a>Key:</h1><h2 id="过拟合与欠拟合"><a href="# 过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合 </h2><p> 当学习器把训练样本学得“太好”了的时候，很可能已经把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降。这种现象就是“过拟合”（Overfitting）。</p>
<p>与“过拟合”相对的是“欠拟合”（Underfitting），这是指对训练样本的一般性质尚未学好。</p>
<p>下图形象生动地说明了过拟合、欠拟合的直观类比：</p>
<p><img src="https://farm1.staticflickr.com/445/31598501856_151da1837a_o.png" alt=""></p>
<p>欠拟合比较容易克服，例如在决策树学习中扩展分支、在神经网络学习中增加训练轮数等，而过拟合则很麻烦。然而过拟合是机器学习面临的关键障碍，各类学习算法都必然带有一些针对过拟合的措施，过拟合是无法彻底避免的，我们所能做的只是“缓解”，或者说减小其风险。</p>
<p><strong>机器学习面临的问题通常是 NP 难甚至更难，而有效的学习算法必然是在多项式时间内运行完成，若可以彻底避免过拟合，则通过经验误差最小化就能获得最优解，这就意味着我们构造性地证明了「 P=NP 」；因此，只要我们是相信「 P≠NP 」，过拟合就不可避免。</strong></p>
<hr>
<h2 id="评估方法"><a href="# 评估方法" class="headerlink" title="评估方法"></a>评估方法 </h2><p> 假设我们现有一个包含 $m$ 个样例的数据集 $D = \left \lbrace (x_1,y_1),(x_2,y_2),…,(x_m,y_m) \right \rbrace$ ，我们需要对数据集 $D$ 进行适当的处理，从中划分出训练集 $S$ 以及测试集 $T$ 。那么有以下几种划分方法：</p>
<h3 id="留出法 -hold-out"><a href="# 留出法 -hold-out" class="headerlink" title="留出法 (hold-out)"></a> 留出法(hold-out)</h3><ul>
<li><p>需要注意的是，训练 / 测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响。</p>
</li>
<li><p>单次使用留出法得到的估计结果往往不够稳定可靠谱，在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。例如进行 100 次随机划分，每次产生一个训练 / 测试集用于实验评估， 100 次后就得到 100 个结果，而留出法返回的则是这 100 个结果的平均值。</p>
</li>
<li><p>此外，我们希望评估的是用 $D$ 训练出的模型的性能，但留出法需划分训练 / 测试集，这就会导致一个窘境：若令训练集 $S$ 包含绝大多数样本，则训练出的模型可能更接近于用 $D$ 训练出的模型，但由于 $T$ 比较小，评估结果可能不够稳定准确；若令测试集 $T$ 多包含一些样本，则训练集 $S$ 与 $D$ 差别更大了，被评估的模型与用 $D$ 训练出的模型相比可能有较大差别，从而降低了评估结果的保真性（fidelity)。<strong>这个问题没有完美的解决方案，常见做法是将大约 2/3 ~ 4/5 的样本用于训练，剩余样本用于测试。</strong> </p>
</li>
<li><p>可从“偏差一方差”的角度来理解：测试集小时，评估结果的方差较大；训练集小时，评估结果的偏差较大。 </p>
</li>
<li><p>一般而言，测试集至少应含 30 个样例 「 Mitchell, 1997 」 。</p>
</li>
</ul>
<h3 id="交叉验证法 -cross-validation"><a href="# 交叉验证法 -cross-validation" class="headerlink" title="交叉验证法 (cross validation)"></a> 交叉验证法 (cross validation)</h3><p> 其做法是将数据集 $D$ 划分为 $k$ 个大小相似的互斥子集，即：</p>
<script type="math/tex; mode=display">
D=D_1 \cup  D_2 \cup ... \cup D_k, D_i \cap  D_j = \varnothing (i \neq j)</script><p>每个子集 $D$ 都尽可能保持数据分布的一致性，即从 $D$ 中通过分层采样得到。然后，每次用 $k-1$ 个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得 $k$ 组训练 / 测试集，从而可进行 $k$ 次训练和测试，最终返回的是这 $k$ 个测试结果的均值。</p>
<ul>
<li><strong>显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于 $k$ 的取值，为强调这一点，通常把交叉验证法称为“ $k$ 折交叉验证”($k$-fold cross validation)。</strong>  $k$ 最常用的取值是 10 ，此时称为 10 折交叉验证；其他常用的 $k$ 值有 5 、 20 等。下图是经典的 10 折交叉验证的示意图：</li>
</ul>
<p><img src="https://farm6.staticflickr.com/5556/31263252700_81d0f5a7d0_o.png" alt=""></p>
<p>与留出法相似，将数据集 $D$ 划分为 $k$ 个子集同样存在多种划分方式。为减小因样本划分不同而引入的差别， $k$ 折交叉验证通常要随机使用不同的划分重复 $p$ 次，最终的评估结果是这 $p$ 次 $k$ 折交叉验证结果的均值，例如常见的有 “ 10 次 10 折交叉验证”。</p>
<p>“ 10 次 10 折交又验证法”与“ 100 次留出法”都是进行了 100 次训练 / 测试。</p>
<h3 id="留一法 -Leave-One-Out"><a href="# 留一法 -Leave-One-Out" class="headerlink" title="留一法 (Leave-One-Out)"></a> 留一法 (Leave-One-Out)</h3><p> 假定数据集 $D$ 中包含 $m$ 个样本，若令 $k=m$ 。则得到了交叉验证法的一个特例：留一法（Leave-One-Out，简称 LOO )。显然，留一法不受随机样本划分方式的影响，因为 $m$ 个样本只有唯一的方式划分为 $m$ 个子集，每个子集包含一个样本。</p>
<p>留一法使用的训练集与初始数据集相比只少了一个样本，这就使得在绝大多数情况下，留一法中被实际评估的模型与期望评估的用 $D$ 训练出的模型很相似。<strong>因此，留一法的评估结果往往被认为比较准确 </strong>（优点）。然而，留一法也有其缺陷：<strong> 在数据集比较大时，训练 $m$ 个模型的计算开销可能是难以忍受的（例如数据集包含一百万个样本，则需训练一百万个模型），而这还是在未考虑算法调参的情况下</strong>（缺点）。另外，留一法的估计结果也未必永远比其他评估方法准确；“没有免费的午餐”定理对实验评估方法同样适用。</p>
<h3 id="自主法 -Bootstrapping"><a href="# 自主法 -Bootstrapping" class="headerlink" title="自主法 (Bootstrapping)"></a> 自主法 (Bootstrapping)</h3><p> 我们希望评估的是用 $D$ 训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比 $D$ 小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。有没有什么办法可以减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计呢？ </p>
<p>“自助法”是一个比较好的解决方案，它直接以自助采样法为基础「Efron and Tibshirani,1993」。给定包含 $m$ 个样本的数据集 $D$ ，我们对它进行采样产生数据集 $D’$ ：每次随机从 $D$ 中挑选一个样本，将其拷贝放入 $D’$ ，然后再将该样本放回初始数据集 $D$ 中，使得该样本在下次采样时仍有可能被采到；这个过程重复执行 $m$ 次后，我们就得到了包含了 $m$ 个样本的数据集 $D’$ ，这就是自助采样的结果。显然， $D$ 中有一部分样本会在 $D’$ 中多次出现，而另一部分样本不出现。可以做一个简单的估计，样本在 $m$ 次采样中始终不被采到的概率是 $(1-1/m)^m$ ，取极限得到</p>
<script type="math/tex; mode=display">
\lim_{m\rightarrow \infty}\left (1-\frac{1}{m}\right )^{m} \mapsto \frac{1}{e} \approx 0.368</script><p>($e$ 是自然常数。)</p>
<p>即通过自助采样，初始数据集 $D$ 中约有 36.8％ 的样本未出现在采样数据集 $D’$ 中。于是我们可将 $D’$ 用作训练集， $D-D’$ 用作测试集；这样，实际评估的模型与期望评估的模型都使用 $m$ 个训练样本，而我们仍有数据总量约 1/3 的、没在训练集中出现的样本用于测试。这样的测试结果，亦称 <strong>“包外估计”(out-of-bag estimate)</strong>. <strong> 自助法在数据集较小、难以有效划分训练 / 测试集时很有用 </strong>（优点）；此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。 <strong> 然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差</strong>（缺点）。因此，在初始数据量足够时，留出法和交叉验证法更加常用一些。</p>
<hr>
<h2 id="性能度量"><a href="# 性能度量" class="headerlink" title="性能度量"></a>性能度量</h2><ul>
<li><p>性能度量是任务需求的体现，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果，所以这以为这模型的“好坏”是相对的，什么样的模型是好的，其实不仅仅取决于算法和数据，更是取决于任务需求。</p>
</li>
<li><p>在预测任务中，给定样例集 $D = \left \lbrace (x_1,y_1),(x_2,y_2),…,(x_m,y_m) \right \rbrace$ ，其中 $y_i$ 是示例 $x_i$ 的真实标记要评估。学习器 $f$ 的性能，就要把学习器预测结果 $f(x)$ 与真实标记 $y$ 进行比较。 </p>
</li>
<li><p>在回归任务中，即预测连续值的问题，最常用的性能度量是<strong>“均方误差”（mean squared error）</strong>，很多的经典算法都是采用了 <strong>MSE</strong> 作为评价函数，想必大家都十分熟悉。</p>
<script type="math/tex; mode=display">
E(f;D) = \frac{1}{m}\sum_{i=1}^{m}m (f(x_i) - y_i )^2</script></li>
<li><p>更一般的，对于数据分布 $D$ 和概率密度函数 $p(·)$ ，均方误差可描述为：</p>
</li>
</ul>
<script type="math/tex; mode=display">
E(f;D) = \int_{x\sim D}  (f(x) - y)^2p(x)dx</script><h3 id="错误率与精度"><a href="# 错误率与精度" class="headerlink" title="错误率与精度"></a>错误率与精度</h3><ul>
<li><p>在分类任务中，即预测离散值的问题，最常用的是 <strong> 错误率 </strong> 和<strong>精度</strong>，错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例，易知：错误率 + 精度 = 1 。</p>
</li>
<li><p>错误率定义为：</p>
</li>
</ul>
<script type="math/tex; mode=display">
E(f;D) = \frac{1}{m}\sum_{i=1}^{m}\mathbb{I}(f(x_i) \neq y_i)</script><ul>
<li>精度则定义为：</li>
</ul>
<script type="math/tex; mode=display">
acc(f;D) = \frac{1}{m}\sum_{i=1}^{m}\mathbb{I}(f(x_{i}) = y_{i})=1-E(f;D)</script><h3 id="查准率、查全率与 -F1"><a href="# 查准率、查全率与 -F1" class="headerlink" title="查准率、查全率与 F1"></a>查准率、查全率与 F1</h3><p>错误率和精度虽然常用，但不能满足所有的需求，例如：在推荐系统中，我们只关心推送给用户的内容用户是否感兴趣（即 <strong> 查准率 </strong>），或者说所有用户感兴趣的内容我们推送出来了多少（即<strong> 查全率</strong>）。因此，使用查准 / 查全率更适合描述这类问题。对于二分类问题，分类结果混淆矩阵与查准 / 查全率定义如下：</p>
<p><img src="https://farm1.staticflickr.com/531/31635512885_c29ced14ac_o.png" alt=""></p>
<p>补上一个表，方便理解：</p>
<p><img src="https://farm5.staticflickr.com/4386/36563815415_834d585fb3_o.png" alt=""></p>
<ul>
<li>查准率：</li>
</ul>
<script type="math/tex; mode=display">
P = \frac{TP}{TP+FP}</script><ul>
<li>查全率：</li>
</ul>
<script type="math/tex; mode=display">
R = \frac{TP}{TP+FN}</script><p>正如天下“没有免费的午餐”理论所揭示的那样，查准率和查全率是一对矛盾的度量。例如我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，查全率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样查准率就很低了。</p>
<p>“ P-R 曲线”正是描述查准 / 查全率变化的曲线，P-R 曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的 P 值和 R 值，如下图所示：</p>
<p><img src="https://farm6.staticflickr.com/5567/31598563736_1791303905_o.png" alt=""></p>
<p>P-R 曲线如何评估呢？若一个学习器 A 的 P-R 曲线被另一个学习器 B 的 P-R 曲线完全包住，则称： B 的性能优于 A 。若 A 和 B 的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了<strong>“平衡点”</strong>（Break-Event Point，简称 BEP ），即当 P=R 时的取值，平衡点的取值越高，性能更优。</p>
<p>$P$ 和 $R$ 指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是 F-Measure ，又称 F-Score 。F-Measure 是 $P$ 和 $R$ 的加权调和平均，即：</p>
<script type="math/tex; mode=display">
\frac{1}{F_{\beta}}  = \frac{1}{2}\cdot (\frac{1}{P}+\frac{1}{R})</script><p>也可以写成：</p>
<script type="math/tex; mode=display">
F1  = \frac{2\times P \times R}{P+R}=\frac{2\times TP}{N + TP -TN}</script><p>(其中 N 为样例总数)</p>
<ul>
<li>在一些应用中，对查准率和查全率的重视程度有所不同。例如在商品推荐系统中，为了尽可能少打扰用户，更希望推荐内容确是用户感兴趣的，此时查准率更重要；而在逃犯信息检索系统中，更希望尽可能少漏掉逃犯，此时查全率更重要。$F1$ 度量的一般形式— $F_\beta $ ，能让我们表达出对查准率／查全率的不同偏好，它定义为：</li>
</ul>
<script type="math/tex; mode=display">
F_{\beta}  = \frac{(1+\beta^2) \times P \times R}{(\beta^2 \times P) + R}</script><p><strong>其中 $\beta &gt; 0$ 度量了查全率对查准率的相对重要性「Van Rijsbergen, 1979」。</strong></p>
<p><strong>$\beta = 1$ 时退化为标准的 $F1$ ； $\beta &gt; 1$ 时查全率有更大影响； $\beta &lt; 1$ 时查准率有更大影响。</strong></p>
<p>有时候我们会有多个二分类混淆矩阵，例如：多次训练或者在多个数据集上训练，那么估算全局性能的方法有两种，分为宏观和微观。</p>
<ul>
<li>简单理解，宏观就是先算出每个混淆矩阵的 $P$ 值和 $R$ 值，然后取得平均 $P$ 值 $macro-P$ 和平均 $R$ 值 $macro-R$ ，再算出 $F_\beta $ 或 $F1$ ：</li>
</ul>
<script type="math/tex; mode=display">
\begin{array}{l}
macro-P = \frac{1}{n}\sum_{i=1}^{n}P_{i} \cr 
macro-R = \frac{1}{n}\sum_{i=1}^{n}R_{i} \cr
macro-F1 = \frac{2\times macro-P \times macro-R}{macro-P + macro-R}
\end{array}</script><ul>
<li>而微观则是计算出混淆矩阵的平均 $TP$ 、 $FP$ 、 $TN$ 、 $FN$ ，接着进行计算 $P$ 、 $R$ ，进而求出 $F_\beta$ 或 $F1$ ：</li>
</ul>
<script type="math/tex; mode=display">
\begin{array}{l}
micro-P = \frac{\overline{TP}}{\overline{TP}+\overline{FP}} \cr
micro-R = \frac{\overline{TP}}{\overline{TP}+\overline{FN}} \cr
micro-F1 = \frac{2\times micro-P \times micro-R}{micro-P + micro-R}
\end{array}</script><h3 id="ROC- 与 -AUC"><a href="#ROC- 与 -AUC" class="headerlink" title="ROC 与 AUC"></a>ROC 与 AUC</h3><p>如上所述：学习器对测试样本的评估结果一般为一个实值或概率，设定一个阈值，大于阈值为正例，小于阈值为负例，因此这个实值的好坏直接决定了学习器的泛化性能，若将这些实值排序，则排序的好坏决定了学习器的性能高低。ROC 曲线正是从这个角度出发来研究学习器的泛化性能，ROC 曲线与 P-R 曲线十分类似，都是按照排序的顺序逐一按照正例预测，不同的是 ROC 曲线以“真正例率”（True Positive Rate，简称 TPR ）为横轴，纵轴为“假正例率”（False Positive Rate，简称 FPR ），<strong>ROC 偏重研究基于测试样本评估值的排序好坏</strong>。</p>
<ul>
<li>TPR（True Positive Rate，真正例率）：<script type="math/tex; mode=display">
TPR = \frac{TP}{TP+FN}</script></li>
<li><p>FPR（False Positive Rate，假正例率）：</p>
<script type="math/tex; mode=display">
FPR = \frac{FP}{TN+FP}</script><p><img src="https://farm1.staticflickr.com/141/31490389422_1b83a2b146_o.png" alt=""></p>
</li>
<li><p>简单分析图像，可以得知：当 $FN=0$ 时，$TN$ 也必须为 $0$ ，反之也成立，我们可以画一个队列，试着使用不同的截断点（即阈值）去分割队列，来分析曲线的形状，$(0,0)$ 表示将所有的样本预测为负例，$(1,1)$ 则表示将所有的样本预测为正例，$(0,1)$ 表示正例全部出现在负例之前的理想情况，$(1,0)$ 则表示负例全部出现在正例之前的最差情况。</p>
</li>
</ul>
<p>现实中的任务通常都是有限个测试样本，因此只能绘制出近似 ROC 曲线。</p>
<p>绘制方法：首先根据测试样本的评估值对测试样本排序，接着按照以下规则进行绘制：</p>
<p>绘制出如图 (b) 所示的近似 ROC 曲线，给定 $m^+$ 个正例和 $m^-$ 个反例，根据学习器预测结果对样例进行排序，然后把分类阈值设为最大，即把所有样例均预测为反例，此时真正例率和假正例率均为 $0$，在坐标 $(0,0)$ 处 标记一个点然后，将分类阈值依次设为每个样例的预测值，即依次将每个样例划分为正例。设前一个标记点坐标为 $(x,y)$ ，当前若为真正例，则对应标记点的坐标为 $(x,y + \frac{1}{m^+})$ ：当前若为假正例，则对应标记点的坐标为 $(x + \frac{1}{m^-},y)$ ，然后用线段连接相邻点即得。</p>
<p>同样地，进行模型的性能比较时，若一个学习器 A 的 ROC 曲线被另一个学习器 B 的 ROC 曲线完全包住，则称 B 的性能优于 A 。若 A 和 B 的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。ROC 曲线下的面积定义为<strong>AUC（Area Uder ROC Curve）</strong>，不同于 P-R 曲线下的面积，这里的 AUC 是可估算的，即 AOC 曲线下每一个小矩形的面积之和。易知：AUC 越大，证明排序的质量越好，AUC 为 1 时，证明所有正例排在了负例的前面，AUC 为 0 时，所有的负例排在了正例的前面。</p>
<script type="math/tex; mode=display">
AUC = \frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_{i})\cdot (y_{i}+y_{i+1})</script><h3 id="代价敏感错误率与代价曲线"><a href="# 代价敏感错误率与代价曲线" class="headerlink" title="代价敏感错误率与代价曲线"></a>代价敏感错误率与代价曲线 </h3><p> 上面的方法中，将学习器的犯错同等对待，但在现实生活中，将正例预测成假例与将假例预测成正例的代价常常是不一样的，例如：将无疾病–&gt; 有疾病只是增多了检查，但有疾病–&gt; 无疾病却是增加了生命危险，二者的“代价”明显是不同的。以二分类为例，由此引入了<strong>“代价矩阵”（cost matrix）</strong>。</p>
<p><img src="https://farm1.staticflickr.com/256/31490416932_57f610e40d_o.png" alt=""></p>
<p>在非均等错误代价下，我们希望的是最小化“总体代价”，这样“代价敏感”的错误率为：</p>
<script type="math/tex; mode=display">
E(f;D;cost) = \frac{1}{m}(\sum_{x_{i}\in D^{+}}\mathbb(f(x_{i})\neq y_{i})\times cost_{01}+\sum_{x_{i}\in D^{-}}\mathbb(f(x_{i})\neq y_{i})\times cost_{10})</script><p>同样对于 ROC 曲线，在非均等错误代价下，演变成了<strong>“代价曲线”</strong>，代价曲线横轴是取值在 $[0,1]$ 之间的正例概率代价，式中 $p$ 表示正例的概率，纵轴是取值为 $[0,1]$ 的归一化代价。</p>
<script type="math/tex; mode=display">
P(+)cost = \frac{p \times cost_{01}}{p \times cost_{01}+ (1-p)\times cost_{10}}</script><script type="math/tex; mode=display">
cost_{norm} = \frac{FNR \times p \times cost_{01} + FPR \times (1-p) \times cost_{10}}{p \times cost_{01}+ (1-p)\times cost_{10}}</script><p>代价曲线的绘制很简单：设 ROC 曲线上一点的坐标为 $(TPR，FPR)$ ，则可相应计算出 $FNR$ ，然后在代价平面上绘制一条从 $(0，FPR)$ 到 $(1，FNR)$ 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将 ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如图所示：</p>
<p><img src="https://farm6.staticflickr.com/5762/31637147675_4ebdb0d453_o.png" alt=""></p>
<h3 id="代价敏感错误率与代价曲线 -1"><a href="# 代价敏感错误率与代价曲线 -1" class="headerlink" title="代价敏感错误率与代价曲线"></a>代价敏感错误率与代价曲线 </h3><h3 id="偏差与方差"><a href="# 偏差与方差" class="headerlink" title="偏差与方差"></a> 偏差与方差 </h3><p> 理解 Bias 与 Variance 之间的权衡，有监督学习中，预测误差的来源主要有两部分，分别为 <strong>Bias</strong> 与 <strong>Variance</strong>，模型的性能取决于 bias 与 variance 的 tradeoff，理解 bias 与 variance 有助于我们诊断模型的错误，避免 over-fitting 或者 under-fitting。</p>
<p>我们知道，同样的算法在不同的数据集上得到的模型结果很可能不同，尽管数据集来自于同一个分部。对于观测数据 $X$ 以及待预测的变量 $Y$ ，假设两者服从 $Y = f(X) + \varepsilon$ ， $\varepsilon$ 为噪声，其服从的 $N(0,\delta  _{\varepsilon}^2)$ ，预测任务中需要得到 $Y$ 值，首先在数据集上 $D$ 上通过算法学习一个近似 $f(X)$ 的模型 $\hat{f}(X)$ 来预测得到 $X$ 的输出。给定 $X$ 一个观测值 $x$ ，待预测变量 $y = f(x) + \varepsilon$ 。</p>
<ul>
<li>对于样本数量相同的不同训练集模型 $\hat{f}(X)$ 的期望输出为： $E[\hat{f}(X) ]$ 。</li>
<li>对于样本数量相同的不同训练集模型产生的方差为： $E[\hat{f}(X)-E[\hat{f}(X)]]^2$ 。</li>
</ul>
<p>将模型的误差分解，采用均方损失，模型 $\hat{f}(X)$ 在点 $x$ 的整体预测误差为真实值与模型预测值之间的误差：</p>
<script type="math/tex; mode=display">
E_{rr} (x)= E[(y-\hat{f}(x))^{2}]</script><p>这个式子其实等价于：</p>
<script type="math/tex; mode=display">
E_{rr} (x)= [E\hat{f}(x)-f(x)]^{2} + E([\hat{f}(x)-E\hat{f}(x)]^2)+\delta _{\varepsilon}^2</script><hr>
<p>这里为推导过程，先回忆几个公式：<br>$Var[X] = E[X^2] - E^2[X]$，且由于函数 $f(x)$ 是确定的，所以 $E[f(x)] = f(x)$，且有 $ \varepsilon \sim N(0,\delta_{\varepsilon}^2)$，再结合 $y = f(x) + \varepsilon$ 可以得到：</p>
<script type="math/tex; mode=display">
E(y) =  E[f(x) +  \varepsilon] = E[f(x)] + 0 = E[f(x)] \tag{1.1}</script><script type="math/tex; mode=display">
Var(y) = E[(y - E(y)^2] = E[(f(x) + \varepsilon - E[f(x)])^2] =  E[\varepsilon ^ 2] = \delta_{\varepsilon}^2 \tag{1.2}</script><hr>
<script type="math/tex; mode=display">
\begin{align}
E_{rr} (x) & = E[(y-\hat{f}(x))^{2}] = E[y^2-2y\hat{f}(x)+(\hat{f}(x))^2] \cr
& = E(y^2) {\color{Orchid}{- E^2(y) + E^2(y)}} -2E(y)E(\hat{f}(x)) + E(\hat{f^2}(x)) {\color{Orchid}{- E^2(\hat{f}(x)) + E^2(\hat{f}(x))}} \cr
& =  {\color{Red}{E^2(\hat{f}(x)) -2E(y)E(\hat{f}(x)) + E^2(y)}} + {\color{Green}{E(\hat{f^2}(x))-  E^2(\hat{f}(x))}} + {\color{Blue} {E(y^2)-  E^2(y) }}
\end{align}</script><p>结合上面两条式子：</p>
<script type="math/tex; mode=display">
\begin{align}
E_{rr}(x) & =  {\color{Red}{E^2(\hat{f}(x)) -2E(f(x))E(\hat{f}(x)) + E^2(f(x))}} + {\color{Blue}{E(\hat{f^2}(x))-  E^2(\hat{f}(x))}} + {\color{Green}{E(y^2)-  E^2(y) }} \cr
& =  {\color{Red}{[E(\hat{f}(x))-E(f(x))]^2}} + {\color{Blue}{E[(\hat{f}(x) - E\hat{f}(x))^2]}} + {\color{Green}{Var(y) }} \cr
& =  {\color{Red}{Bias^2[\hat{f}(x)]}} + {\color{Blue}{Var[\hat{f}(x)]}} + {\color{Green}{\delta _{\varepsilon}^{2}}}
\end{align}</script><hr>
<p>其中，$E(\hat{f}(X)) - E(f(x))$ 即为 Bias（偏差），$E[\hat{f}(X) - E\hat{f}(X)]^2$ 为 Variance（方差），$\delta_{\varepsilon}^2$ 即为模型无法避免的 Noise，所以现在对于一个预测模型的误差可以分为如下几部分：</p>
<script type="math/tex; mode=display">
Error = Bias^2 + Variance + Noises</script><p>对于预测模型问题，如果我们能够获得所有可能的数据集合，并在这个数据集合上将 $Error$ 最小化，这样学习到的模型就可以称之为“真实模型”，当然，我们是无论如何都不能获得并训练所有可能的数据的，所以“真实模型”肯定存在，但无法获得，我们的最终目标就是去学习一个模型使其更加接近这个真实模型。为了在有限的训练数据集上达到这个目标，就要使 $Error$ 最小了，$Error$ 分为 $Bias$ 、 $Variance$ 与 $Noise$ ：</p>
<p><strong>Bias</strong>：度量了学习算法的期望输出与真实结果的偏离程度, 刻画了算法的拟合能力，Bias 偏高表示预测函数与真实结果差异很大。</p>
<p><strong>Variance</strong>：则代表“同样大小的不同的训练数据集训练出的模型”与“这些模型的期望输出值”之间的差异。训练集变化导致性能变化， Variance 偏高表示模型很不稳定。</p>
<p><strong>Noise</strong>：刻画了当前任务任何算法所能达到的期望泛化误差的下界，即刻画了问题本身的难度。</p>
<p>由于 Noise 是无法避免的 所以要得到好的模型，就需要低 Bias 与低 Variance 下图给出一个 Bias 与 Variance 的示意图，明显可以看到低 Bias 与低 Variance 次次会命中靶心，而低 Bias 高 Variance 取均值后才会大多命中靶心，其他情况全打歪了。</p>
<p><img src="https://farm6.staticflickr.com/5585/30826966253_eff65097a8_o.png" alt=""></p>
<p>低 Bias 与低 Variance 才会得到低 Error，但低 Bias 与低 Variance 往往是不能兼得的。如果要降低模型的 Bias，就一定程度上会提高模型的 Variance，反之亦然。这里以 K-NN 为例，看一些 K-NN 中 Bias 与 Variance 与其参数 K 的关系，在 K-NN 中，误差形式如下：</p>
<script type="math/tex; mode=display">
Err(x) = [f(x)-\frac{1}{k}\sum_{i=1}^{k}f(x_{i})]^2 + \frac{\delta ^2}{k}+\delta ^2</script><p>这里 $x_{1}，x_{2}，…，x_{k}$ 是 $x$ 在训练数据集中最近的 $k$ 个邻居，当 $k$ 取值很小的时候，Bias 很低，但是 Variance 会比较高。但随着 $k$ 的增大，Bias 变高，Variance 降低，这种现象被称为 <strong> 偏差 - 方差窘境（bias-variance dilemma）</strong>。</p>
<p><strong>因为预测模型试图用有限的训练样本上去得到一个用来预测全数据集的模型，为了降低模型的误差率，就要尽量使模型在训练数据集上更加“准确”，这样做往往会增加 Model Complexity ，但却忽略模型在全数据集的泛化能力，模型在训练数据集的 Bias 减少了，但是对于训练数据集中没有出现的数据，模型对其预测就会很不稳定，这样就会造成高 Variance ，这也就是常说的 over-fitting ，为了避免 over-fitting，就不能完全依赖于有限的训练数据，这时可以加入一些先验信息，先验信息在模型求解的过程中会增加一些限制，提高模型的稳定程度，同时减少 Model Complexity，进而可以降低 Variance，但是由于“不信任”训练数据，会使模型的 Bias 增大, 或者训练不足时，模型拟合能力不够，训练数据的扰动不足以使模型拟合其真实情况，这时候 Bias 太大，模型对训练数据的预测能力就会下降，这便是 under-fitting 了，所以需要要 Bias 与 Variance 之间 寻找一个 tradeoff。</strong></p>
<p>因为 Noise 是不可避免的，这里忽略 Noise ，根据 Bias 、Variance 与 Model  Complexity 之间的关系，可以得到下图左所示的图形，该图表示在训练数据上几个量之间的关系，为了找到最优的模型，</p>
<p>在训练集中 Bias 下降与 Variance 上升的过程中来找到所谓的  tradeoff ，这时的 Model Complexity 是适中的，找到了这个 tradeoff ，在全数据集或者预测集上的预测集上的表现如下图右所示，所以 Bias 与 Variance 需要适中，才会得到在训练集与测试集误差都小的模型。</p>
<p><img src="https://farm1.staticflickr.com/595/30795620304_a815c4ccff_o.png" alt=""></p>
<p>最后谈一下 K-fold Cross Validation 与权衡 Bais-Variance 之间的关系。这个理解其实很简单，先看对于数据的划分:</p>
<p><img src="https://farm1.staticflickr.com/663/30795629194_9a7a48bc13_o.png" alt=""></p>
<p>每次留出一个 fold 用来做“验证集”来计算误差，当 K 设置很大时，每个 fold 都会有很少的数据，小数据集更容易有噪音，所以小的数据集会有大的 Bias ，同时 K 个 fold 代表 K 组数据分别训练一个模型，求平均后 Variance 会很小；相反如果 K 比较小的话，每个 fold 数据相对来说还是很多的，所以 Bias 相对较小，但是由于总模型数较小，所以 Variance 还是比较大的。</p>
<hr>
<h1 id="Correct-errors-in-printing"><a href="#Correct-errors-in-printing" class="headerlink" title="Correct errors in printing:"></a>Correct errors in printing:</h1><ul>
<li>暂无</li>
</ul>
<hr>
<h1 id="Practice"><a href="#Practice" class="headerlink" title="Practice:"></a>Practice:</h1><blockquote>
<p>2. 数据集包含 100 个样本，其中正、反例各一半，假定学习算法所产生的模型是将新样本预测为训练样本数较多的类别（训练样本数相同时进行随机猜测），试给出用 10 折交叉验证法和留一法分别对错误率进行评估所得的结果。</p>
</blockquote>
<ul>
<li><p>10 折交叉检验：由于每次训练样本中正反例数目一样，所以将结果判断为正反例的概率也是一样的，所以错误率的期望是 50% 。 </p>
</li>
<li><p>留一法：测试集为一个样本，训练集九十九个样本且有五十个与测试集真实类别不同，故测试集无法被划分到正确的类，所以错误率是 100% 。</p>
</li>
</ul>
<hr>
<blockquote>
<p>4. 试述真正例率（TPR）、假正例率（FPR）与查准率（P）、查全率（R）之间的联系。</p>
</blockquote>
<ul>
<li>查全率（R）： 真实正例被预测为正例的比例。 </li>
<li>真正例率（TPR）： 真实正例被预测为正例的比例。 </li>
</ul>
<p>（显然查全率与真正例率是相等的，即 $R=TPR$） </p>
<ul>
<li>查准率（P）：预测为正例的实例中真实正例的比例。</li>
<li>假正例率（FPR）： 真实反例被预测为正例的比例。</li>
</ul>
<p>（两者并没有直接的数值关系）</p>
<hr>
<blockquote>
<p>5. 试证明 $AUC = 1 − l_{rank}$ 。</p>
</blockquote>
<p>与 BEP 一样，学习器先将所有测试样本按预测概率排序，越可能是正的排在越前面。然后依次遍历，每扫描到一个位置，里面如果只有正例，则 ROC 曲线垂直向上，如果只有反例，曲线水平往右，如果既有正例也有反例，则斜向上。如图所示：</p>
<p><img src="https://farm1.staticflickr.com/733/31637250505_65b565e6c6_o.png" alt=""></p>
<p>由于 TPR 与 FPR 的分母是常数，所以这里按比例扩大了坐标（分别是真实正例和真实反例的数目倍），可以更好看出曲线走势。</p>
<p>可以看出一共有 20 个测试样本，10 个正，10 个反。学习器排序的结果是：</p>
<ul>
<li>, − , (+,+) , (+,−) , (+,−) , (+,+) , (−,−) , (+,+) , (−,−,−) , + , −。其中括号内的样本排在相同的位置。<br>&lt; (+,+,−,−) 与 (+,−) , (+,−) 是同样的效果 &gt;</li>
</ul>
<p>公式 <2.21> 累加了所有不在正例的反例数目，其中同样的位置标记为 0.5 ，在正例前面标记为 1 。从图中可以看出，折线每次向右 (右上) 延伸，表示扫描到了反例，折线上方对应的面积，就是该反例后面有多少个正例，每个正例是一个正方形，对应的面积是 1 。同位置上的正例是个三角形，对应的面积是 0.5 。计算出总面积后，由于 ROC 图的坐标是归一化的，所以总面积要除以一开始放大的倍数，也就是 <script type="math/tex">m_{+}  m_{-}</script> 。</2.21></p>
<hr>
<blockquote>
<p>6. 试述错误率与 ROC 曲线之间的关系。</p>
</blockquote>
<p>ROC 曲线每个点对应了一个 TPR 与 FPR ，此时对应了一个错误率。 </p>
<script type="math/tex; mode=display">
E_{cost} = \frac{m^+ \times (1-TPR) \times cost_{01}+m^- \times FPR \times cost_{10}}{m^+ + m^-}</script><p>学习器会选择错误率最小的位置作为截断点。</p>
<hr>
<blockquote>
<p>7. 试证明任意一条 ROC 曲线都有一条代价曲线与之对应，反之亦然。</p>
</blockquote>
<p>由定义可以知道 TPR 与 FPR 都是由 0 上升到 1 ，那么 FNR 则是由 1 下降到 0 。 </p>
<p>每条 ROC 曲线都会对应一条代价曲线，由于第一条代价线段的是 $(0,0)$ , $(1,1)$ ，最后是 $(0,1)$ , $(1,0)$ , 所有代价线段总会有一块公共区域，这个区域就是期望总体代价，而这块区域的边界就是代价曲线，且肯定从 $(0,0)$ 到 $(1,0)$ 。 </p>
<p>在有限个样本情况下，ROC 是一条折线，此时根据代价曲线无法还原 ROC 曲线。但若是理论上有无限个样本，ROC 是一条连续的折线，代价曲线也是连续的折线，每个点的切线可以求出 TPR 与 FNR，从而得到唯一的 ROC 曲线。</p>
<hr>
]]></content>
    
    <summary type="html">
    
      本文是关于周志华「Machine Learning」这本书的 Chapter 2 的学习笔记。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
      <category term="Book:「Machine Learning」" scheme="http://randolph.pro/categories/Machine-Learning/Book-%E3%80%8CMachine-Learning%E3%80%8D/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>♞「Machine Learning」 Chapter 1</title>
    <link href="http://randolph.pro/2016/12/26/%E2%99%9E%E3%80%8CMachine%20Learning%E3%80%8D%20Chapter%201/"/>
    <id>http://randolph.pro/2016/12/26/♞「Machine Learning」 Chapter 1/</id>
    <published>2016-12-25T16:00:00.000Z</published>
    <updated>2017-08-14T04:15:11.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4307/36174772306_62cfbc8cd6_o.jpg" alt=""></p>
<p>有关该书的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/Book-「Machine-Learning」/">Book:「Machine Learning」</a></p>
<h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><ul>
<li><strong>归纳偏好</strong></li>
<li><strong>奥卡姆剃刀（Occam’s razor）</strong></li>
<li><strong>NFL 定理（No Free Lunch Theorem）</strong></li>
</ul>
<hr>
<h1 id="Key"><a href="#Key" class="headerlink" title="Key:"></a>Key:</h1><p>我们可以把学习过程看做是一个在所有假设（hypothesis）组成的空间中进行搜索的过程，搜索目标就是找到与训练集“匹配”（fit）的假设。</p>
<h2 id="归纳偏好"><a href="# 归纳偏好" class="headerlink" title="归纳偏好"></a>归纳偏好 </h2><p> 任何一个有效的及其学习算法必有其归纳偏好，否则它将被假设空间中看似在训练集上“等效”的假设所迷惑，而无法产生正确的学习结果。可以想象，如果没有偏好，我们的西瓜学习算法产生的模型每次在进行预测时随机抽选训练集上的等效假设，那么对这个新瓜“（色泽 = 青绿；根蒂 = 蜷缩；敲声 = 沉闷）”，模型告诉我们它是好的，但是时而告诉我们它是不好的，这样的学习结果显然没有意义。</p>
<p>事实上，归纳偏好对应了学习算法本身所作出的关于“什么样的模型更好”的假设。在具体的现实问题中，这个假设是否成立，即算法的归纳偏好是否与问题本身匹配，大多数时候直接决定了算法能否取得好的性能。</p>
<hr>
<h2 id="奥卡姆剃刀"><a href="# 奥卡姆剃刀" class="headerlink" title="奥卡姆剃刀"></a>奥卡姆剃刀</h2><ul>
<li><p>“奥卡姆剃刀”（Occam’s razor）是一种常用的、自然科学研究中最基本的原则，即“若有多个假设与观察一致，则选择最简单的那个”。</p>
</li>
<li><p>然而，奥卡姆剃刀并非唯一可行的原则。书中给出了一个例子进行了解释：</p>
</li>
</ul>
<blockquote>
<p>“假设 1：好瓜 ⟺（色泽 =*）^（根蒂 = 蜷缩）^（敲声 = 浊响）”</p>
<p>“假设 2：好瓜 ⟺（色泽 =*）^（根蒂 = 蜷缩）^（敲声 =*）”</p>
</blockquote>
<p>这两个假设，哪一个更“简单”呢？这个问题并不简单，我们需要借助其他机制才能解决。</p>
<hr>
<h2 id="NFL 定理（No-Free-Lunch-Theorem）"><a href="#NFL 定理（No-Free-Lunch-Theorem）" class="headerlink" title="NFL 定理（No Free Lunch Theorem）"></a>NFL 定理（No Free Lunch Theorem）</h2><h3 id="“没有免费的午餐”定理"><a href="#“没有免费的午餐”定理" class="headerlink" title="“没有免费的午餐”定理"></a>“没有免费的午餐”定理 </h3><p> 简而言之就是，无论学习算法 $L_a$ 多聪明、学习算法 $L_b$ 多笨拙，它们的期望性竟然相同！</p>
<p>那么既然所有学习算法的期望性能都跟随机胡猜差不多，那还有什么好学的？</p>
<blockquote>
<p>我们需注意到， <strong>NFL 定理有一个重要前提：所有“问题”出现的机会相同、或所有问题同等重要。但实际情形并不是这样。</strong></p>
<p>很多时候，我们只关注自已正在试图解决的问题（例如某个具体应用任务），希望为它找到一个解决方案，至于这个解决方案在别的问题、甚至在相似的问题上是否为好方案，我们并不关心。例如，为了快速从 A 地到达 B 地，如果我们正在考虑的 A 地是南京鼓楼、B 地是南京新街口，那么“骑自行车”是很好的解决方案；这个方案对 A 地是南京鼓楼、B 地是北京新街口的情形显然很糟糕，但我们对此并不关心。</p>
</blockquote>
<p><strong>所以， NFL 定理最重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛地谈论“什么学习算法更好”毫无意义，因为若考虑所有潜在的问题，则所有学习算法都一样好。要谈论算法的相对优劣，必须要针对具体的学习问题；在某些问题上表现好的学习算法，在另一些问题上却可能不尽如人意，学习算法自身的归纳偏好与问题是否相配，往往会起到决定性的作用。</strong></p>
<hr>
<h1 id="Correct-errors-in-printing"><a href="#Correct-errors-in-printing" class="headerlink" title="Correct errors in printing:"></a>Correct errors in printing:</h1><ul>
<li><strong>暂无</strong></li>
</ul>
<hr>
<h1 id="Practice"><a href="#Practice" class="headerlink" title="Practice:"></a>Practice:</h1><blockquote>
<p>1. 表 1.1 中若只包含编号为 1 和 4 的两个样例，试给出相应的版本空间.</p>
</blockquote>
<ul>
<li>数据集有 3 个属性，每个属性 2 种取值，一共 $ 3 \times 3 \times 3 + 1 = 28 $ 种假设：</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Condition A</th>
<th style="text-align:center">Condition B</th>
<th style="text-align:center">Condition C</th>
<th style="text-align:center">Condition D</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1. 色泽 = 青绿 根蒂 = 蜷缩 敲声 = 浊响</td>
<td style="text-align:center">2. 色泽 = 青绿 根蒂 = 蜷缩 敲声 = 沉闷</td>
<td style="text-align:center">3. 色泽 = 青绿 根蒂 = 稍蜷 敲声 = 浊响</td>
<td style="text-align:center">4. 色泽 = 青绿 根蒂 = 稍蜷 敲声 = 沉闷</td>
</tr>
<tr>
<td style="text-align:center">5. 色泽 = 乌黑 根蒂 = 蜷缩 敲声 = 浊响</td>
<td style="text-align:center">6. 色泽 = 乌黑 根蒂 = 蜷缩 敲声 = 沉闷</td>
<td style="text-align:center">7. 色泽 = 乌黑 根蒂 = 稍蜷 敲声 = 浊响</td>
<td style="text-align:center">8. 色泽 = 乌黑 根蒂 = 稍蜷 敲声 = 沉闷</td>
</tr>
<tr>
<td style="text-align:center">9. 色泽 = 青绿 根蒂 = 蜷缩 敲声 =*</td>
<td style="text-align:center">10. 色泽 = 青绿 根蒂 = 稍蜷 敲声 =*</td>
<td style="text-align:center">11. 色泽 = 乌黑 根蒂 = 蜷缩 敲声 =*</td>
<td style="text-align:center">12. 色泽 = 乌黑 根蒂 = 稍蜷 敲声 =*</td>
</tr>
<tr>
<td style="text-align:center">13. 色泽 = 青绿 根蒂 =* 敲声 = 浊响</td>
<td style="text-align:center">14. 色泽 = 青绿 根蒂 =* 敲声 = 沉闷</td>
<td style="text-align:center">15. 色泽 = 乌黑 根蒂 =* 敲声 = 浊响</td>
<td style="text-align:center">16. 色泽 = 乌黑 根蒂 =* 敲声 = 沉闷</td>
</tr>
<tr>
<td style="text-align:center">17. 色泽 =* 根蒂 = 蜷缩 敲声 = 浊响</td>
<td style="text-align:center">18. 色泽 =* 根蒂 = 蜷缩 敲声 = 沉闷</td>
<td style="text-align:center">19. 色泽 =* 根蒂 = 稍蜷 敲声 = 浊响</td>
<td style="text-align:center">20. 色泽 =* 根蒂 = 稍蜷 敲声 = 沉闷</td>
</tr>
<tr>
<td style="text-align:center">21. 色泽 = 青绿 根蒂 =* 敲声 =*</td>
<td style="text-align:center">22. 色泽 = 乌黑 根蒂 =* 敲声 =*</td>
<td style="text-align:center">23. 色泽 =* 根蒂 = 蜷缩 敲声 =*</td>
<td style="text-align:center">24. 色泽 =* 根蒂 = 稍蜷 敲声 =*</td>
</tr>
<tr>
<td style="text-align:center">25. 色泽 =* 根蒂 =* 敲声 = 浊响</td>
<td style="text-align:center">26. 色泽 =* 根蒂 =* 敲声 = 沉闷</td>
<td style="text-align:center">27. 色泽 =* 根蒂 =* 敲声 =*</td>
<td style="text-align:center">28. 空集Ø</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>在该 28 种假设中根据 <code> 好瓜 ⟺ ((色泽 = 青绿) ∧ (根蒂 = 蜷缩) ∧ (敲声 = 浊响))</code>这条正例的数据删除数据，需要删除的数据为：<ul>
<li>编号 2−8，10−12，14−16，18−20，22，24，26，27，28。</li>
</ul>
</li>
<li>所以，最终得到的版本空间为：</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Condition A</th>
<th style="text-align:center">Condition B</th>
<th style="text-align:center">Condition C</th>
<th style="text-align:center">Condition D</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1. 色泽 = 青绿 根蒂 = 蜷缩 敲声 = 浊响</td>
<td style="text-align:center">9. 色泽 = 青绿 根蒂 = 蜷缩 敲声 =*</td>
<td style="text-align:center">13. 色泽 = 青绿 根蒂 =* 敲声 = 浊响</td>
<td style="text-align:center">17. 色泽 =* 根蒂 = 蜷缩 敲声 = 浊响</td>
</tr>
<tr>
<td style="text-align:center">21. 色泽 = 青绿 根蒂 =* 敲声 =*</td>
<td style="text-align:center">23. 色泽 =* 根蒂 = 蜷缩 敲声 =*</td>
<td style="text-align:center">25. 色泽 =* 根蒂 =* 敲声 = 浊响</td>
</tr>
</tbody>
</table>
</div>
<p><strong>一般情况下版本空间是正例的泛化，但由于数据集中只有 1 个正例，所以在版本空间中依然包含了这个样本的假设（假设 1）。</strong></p>
<p><img src="https://farm5.staticflickr.com/4366/36147851030_43e856f6b2_o.png" alt=""></p>
<hr>
<blockquote>
<p>2. 与使用单个合取式来进行假设表示相比，用“析合范式”将使得假设空间具有更强的表示能力。例如：</p>
<p><code>好瓜 ⟺ ((色泽 = *) ∧ (根蒂 = 蜷缩) ∧ (敲声 = *)) ∨ ((色泽 = 乌黑) ∧ (根蒂 = *) ∧ (敲声 = 沉闷))</code></p>
<p>会把 <strong>(色泽 = 青绿) ∧ (根蒂 = 蜷缩) ∧ (敲声 = 清脆)</strong> 以及 <strong>(色泽 = 乌黑) ∧ (根蒂 = 硬挺) ∧ (敲声 = 沉闷)</strong> 都分类为“好瓜”。若使用最多包含 $k$ 个合取式的析合范式来表达表 1.1 西瓜分类问题的假设空间，试估算共有多少种可能的假设。 </p>
</blockquote>
<p>（提示：注意冗余情况， 如 <script type="math/tex">(A=a) \vee (A=*)</script> 与 $(A=*)$ 等价。） </p>
<p>表 1.1 包含 4 个样例， 3 种属性，假设空间中有 $3 \times 4 \times 4 + 1 = 49 $ 种假设。在不考虑沉余的情况下，最多包含 $k$ 个合取式来表达假设空间，显然 $k$ 的最大值是 49，每次从中选出 $k$ 个来组成析合式，共 $ \sum C_{49}^k = 249 $ 种可能。（但是其中包含了很多沉余的情况）</p>
<p>如果考虑沉余的情况：<br>在这里忽略空集，一个原因是并不是太明白空集是否应该加入析合式，另外就算需要加入，求出了前面 48 种假设的组合，可以很容易求出加入空集后的组合数（每种可能都可以加上空集，再加上 1 种空集单独的情况）。 </p>
<p>48 种假设中： </p>
<ul>
<li>具体假设： $2 \times 3 \times 3=18$ 种</li>
<li>一个属性泛化假设： $2 \times 3 + 3 \times 3 + 2 \times 3 = 21$ 种 </li>
<li>两个属性泛化假设： $2 + 3 + 3 = 8$ 种 </li>
<li>三属性泛化： $1$  种 </li>
</ul>
<p>当 $k=1$ 时，任选一种假设都可以作为一种没有沉余的假设，共 48 种。<br>$k$ 的最大值是 18 ，当 $k=18$ 时，就是 18 种具体属性假设的析取式，共 1 种。 而当 $k$ 取中间值时，就不好分析了。 </p>
<p>下面提供一种我认为可行的算法：<br>由于属性泛化后，一个泛化的假设可以对应多个具体假设。<br>把所有假设按三属性泛化，二属性泛化，一属性泛化，具体属性排序（这样可以保证排在后面的假设不会包含前面的任何一个假设，所以省略了一些包含判断），进行循环枚举，按顺序遍历所有假设组合 248 种可能（当然绝大部分都提前结束了，不会是那么夸张的量级，虽然也不低）：</p>
<ol>
<li>使用栈来实现非递归，如果当前假设还有没被析合式所包含的具体假设，则认为可以入栈，并当前栈大小的长度计数加 1 ，并继续扫描。</li>
<li>如果当前扫描已经到了最后一个假设，或者所有具体假设已经被全部包含，则退栈。</li>
<li>循环结束条件：当最后一个假设作为第一个压入栈的元素时，认为已经遍历结束。</li>
</ol>
<p>由于一共有 18 种具体假设，可以用一个 32 位整型（变量为 <strong><code>hypos_cur</code></strong> ）的后 18 位来表示每一个具体假设。用 1 表示具体假设没被包含，用 0 表示具体假设已经被析合式包含。初始的析合式为空，可以设初试值为 0X3FFFF 。每个假设也对应一个 32 位整型（假设变量为 <strong><code>hypo_const</code></strong> ），代表着它所对应了哪些具体假设，如果它包含了某种具体假设，则该位为 1 。</p>
<ol>
<li>判断析合式是否包含了全部的具体假设：<strong><code>hypos_cur=0</code></strong> 。</li>
<li>判断该假设是否已经被析合范式包含：用 <strong><code>hypo_const</code></strong> 与 <strong><code>hypos_cur</code></strong> 做与运算（结果用 <strong><code>hypo_tmp</code></strong> 表示），如果为 0 表示已经被包含（判断该假设是否包含了当前的析合式：用 <strong><code>hypo_const</code></strong> 与 <strong><code>hypos_cur</code></strong> 做或运算，如果为 0X3FFFFF ，则认为该假设包含了当前析合式，但由于前面对所有假设做了排序，不可能出现这种情况，所以可以省略该判断）。</li>
<li>当某个假设加入析合范式后（入栈）用 <strong><code>hypos_cur</code></strong> 与 <strong><code>hypo_tmp</code></strong> 做异或运算，来更改析合式所包含的具体假设。</li>
<li>出栈时再次用 <strong><code>hypos_cur</code></strong> 与 <strong><code>hypo_tmp</code></strong> 做异或，回到加入该假设前的情况。</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stack&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="comment">// 按泛化程度排序，保证排在后面的假设不会不会包含前面的任何一个假设 </span></div><div class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">char</span> <span class="built_in">list</span>[] = &#123;</div><div class="line">    <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,</div><div class="line">    <span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">0</span>,</div><div class="line">    <span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">3</span>,</div><div class="line">    <span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,</div><div class="line">    <span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">0</span>,</div><div class="line">    <span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,</div><div class="line">    <span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span></div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="comment">// 用来派生的抽象类</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">hypos</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">int</span> <span class="title">insert</span><span class="params">(<span class="keyword">int</span> cur)</span> </span>= <span class="number">0</span>;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="comment">// 单个的假设类</span></div><div class="line"><span class="comment">/*</span></div><div class="line">hypo_const  假设对应的具体假设集合</div><div class="line">*/</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">hypo</span> :</span><span class="keyword">public</span> hypos &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    hypo(<span class="keyword">int</span> a, <span class="keyword">int</span> b, <span class="keyword">int</span> c) &#123;</div><div class="line">        hypo_const = <span class="number">0</span>;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt;  p[<span class="number">3</span>];</div><div class="line">        <span class="keyword">if</span> (a == <span class="number">0</span>) &#123;</div><div class="line">            p[<span class="number">0</span>].push_back(<span class="number">1</span>);</div><div class="line">            p[<span class="number">0</span>].push_back(<span class="number">2</span>);</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span></div><div class="line">            p[<span class="number">0</span>].push_back(a);</div><div class="line">        <span class="keyword">if</span> (b == <span class="number">0</span>) &#123;</div><div class="line">            p[<span class="number">1</span>].push_back(<span class="number">1</span>);</div><div class="line">            p[<span class="number">1</span>].push_back(<span class="number">2</span>);</div><div class="line">            p[<span class="number">1</span>].push_back(<span class="number">3</span>);</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span></div><div class="line">            p[<span class="number">1</span>].push_back(b);</div><div class="line">        <span class="keyword">if</span> (c == <span class="number">0</span>) &#123;</div><div class="line">            p[<span class="number">2</span>].push_back(<span class="number">1</span>);</div><div class="line">            p[<span class="number">2</span>].push_back(<span class="number">2</span>);</div><div class="line">            p[<span class="number">2</span>].push_back(<span class="number">3</span>);</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span></div><div class="line">            p[<span class="number">2</span>].push_back(c);</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">unsigned</span> <span class="keyword">int</span> i = <span class="number">0</span>;i &lt; p[<span class="number">0</span>].size();i++)</div><div class="line">            <span class="keyword">for</span> (<span class="keyword">unsigned</span> <span class="keyword">int</span> j = <span class="number">0</span>;j &lt; p[<span class="number">1</span>].size();j++)</div><div class="line">                <span class="keyword">for</span> (<span class="keyword">unsigned</span> <span class="keyword">int</span> k = <span class="number">0</span>;k &lt; p[<span class="number">2</span>].size();k++)</div><div class="line">                    hypo_const |= (<span class="number">1</span> &lt;&lt; (p[<span class="number">0</span>][i] * <span class="number">9</span> + p[<span class="number">1</span>][j] * <span class="number">3</span> + p[<span class="number">2</span>][k] - <span class="number">13</span>));</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 判断是否要加入到析合式 如果还有具体假设没被包含，则加入</span></div><div class="line">    <span class="function"><span class="keyword">int</span> <span class="title">insert</span><span class="params">(<span class="keyword">int</span> cur)</span> </span>&#123;</div><div class="line">        <span class="keyword">return</span> (hypo_const &amp; cur);</div><div class="line">    &#125;;</div><div class="line"></div><div class="line"><span class="keyword">private</span>:</div><div class="line">    <span class="keyword">int</span> hypo_const;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="comment">// 用于压入栈的派生类 用来实现非递归</span></div><div class="line"><span class="comment">/*</span></div><div class="line">hypo_tmp    记录这个假设入栈时，带入了哪些具体假设，出栈时要还原</div><div class="line">ptr         记录入栈时的位置</div><div class="line">*/</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">hypo_ss</span> :</span><span class="keyword">public</span> hypos &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    hypo_ss(<span class="keyword">int</span> _ptr,<span class="keyword">int</span> tmp)&#123;</div><div class="line">        hypo_tmp = tmp;</div><div class="line">        ptr = _ptr;</div><div class="line">    &#125;</div><div class="line">    <span class="function"><span class="keyword">int</span> <span class="title">insert</span><span class="params">(<span class="keyword">int</span> cur)</span> </span>&#123;</div><div class="line">        <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">    &#125;;</div><div class="line">    <span class="keyword">int</span> hypo_tmp;</div><div class="line">    <span class="keyword">int</span> ptr;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="comment">// 用来循环遍历的类</span></div><div class="line"><span class="comment">/*</span></div><div class="line">sum     各个长度的析合式各有多少种可能</div><div class="line">ss      用来实现非递归的栈</div><div class="line">hypos_cur   当前没被包含的具体假设 初始值为 0X3FFFF</div><div class="line">hyposs  48 个假设集合</div><div class="line">*/</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Traversal</span> :</span><span class="keyword">public</span> hypos &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    Traversal() &#123;</div><div class="line">        hypos_cur = <span class="number">0x3ffff</span>;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">48</span>;i++)</div><div class="line">            hyposs.push_back(hypo(<span class="built_in">list</span>[<span class="number">3</span>*i], <span class="built_in">list</span>[<span class="number">3</span>*i+<span class="number">1</span>], <span class="built_in">list</span>[<span class="number">3</span>*i+<span class="number">2</span>]));</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 循环顺序遍历的主体</span></div><div class="line">    <span class="comment">//cur  初试的位置 设为 0</span></div><div class="line">    <span class="function"><span class="keyword">int</span> <span class="title">insert</span><span class="params">(<span class="keyword">int</span> cur)</span> </span>&#123;</div><div class="line">        <span class="comment">// 当前指向的位置</span></div><div class="line">        <span class="keyword">int</span> ptr = cur;</div><div class="line">        <span class="keyword">while</span> (<span class="number">1</span>) &#123;</div><div class="line">            <span class="comment">// 退出条件 当最后一个假设作为第一个入栈的元素 表示遍历完成</span></div><div class="line">            <span class="keyword">if</span> (ptr &gt; <span class="number">47</span> &amp;&amp; !ss.size()) <span class="keyword">break</span>;</div><div class="line">            <span class="comment">// 回退条件  扫描到最后或者所有具体假设都被包含 </span></div><div class="line">            <span class="keyword">if</span> (hypos_cur == <span class="number">0</span> || ptr&gt;<span class="number">47</span>) &#123;</div><div class="line">                hypo_ss hypo_tmp = ss.top();</div><div class="line">                hypos_cur ^= hypo_tmp.hypo_tmp;</div><div class="line">                ptr = hypo_tmp.ptr + <span class="number">1</span>;</div><div class="line">                ss.pop();</div><div class="line">                <span class="keyword">continue</span>;</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="comment">// 入栈条件  如果该假设还有未被包含的具体假设 则入栈，并当前栈大小的计数加 1</span></div><div class="line">            <span class="keyword">if</span> (<span class="keyword">int</span> tmp =hyposs[ptr].insert(hypos_cur)) &#123;</div><div class="line">                hypos_cur ^= tmp;</div><div class="line">                ss.push(hypo_ss(ptr, tmp));</div><div class="line">                <span class="keyword">if</span> (sum.size() &lt; ss.size())</div><div class="line">                    sum.push_back(<span class="number">0</span>);</div><div class="line">                sum[ss.size() - <span class="number">1</span>]++;</div><div class="line">            &#125;</div><div class="line">            ptr++;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">    &#125;;</div><div class="line">    <span class="comment">// 输出各个长度的可能数</span></div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">unsigned</span> <span class="keyword">int</span> i = <span class="number">0</span>;i &lt; sum.size();i++)</div><div class="line">            <span class="built_in">printf</span>(<span class="string">"length %d : %d\n"</span>, i + <span class="number">1</span>, sum[i]);</div><div class="line">    &#125;</div><div class="line"><span class="keyword">private</span>:</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; sum;</div><div class="line">    <span class="built_in">stack</span>&lt;hypo_ss&gt; ss;</div><div class="line">    <span class="keyword">int</span> hypos_cur;</div><div class="line">    <span class="built_in">vector</span>&lt;hypo&gt; hyposs;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</div><div class="line">    Traversal traversal;</div><div class="line">    traversal.insert(<span class="number">0</span>);</div><div class="line">    traversal.print();</div><div class="line">    system(<span class="string">"pause"</span>);</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"> 最终输出:</div><div class="line">length 1 : 48</div><div class="line">length 2 : 931</div><div class="line">length 3 : 10332</div><div class="line">length 4 : 72358</div><div class="line">length 5 : 342057</div><div class="line">length 6 : 1141603</div><div class="line">length 7 : 2773332</div><div class="line">length 8 : 4971915</div><div class="line">length 9 : 6543060</div><div class="line">length 10 : 6175660</div><div class="line">length 11 : 4003914</div><div class="line">length 12 : 1676233</div><div class="line">length 13 : 422676</div><div class="line">length 14 : 61884</div><div class="line">length 15 : 5346</div><div class="line">length 16 : 435</div><div class="line">length 17 : 27</div><div class="line">length 18 : 1</div><div class="line">*/</div></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>3. 若数据包含噪声，则假设空间中可能不存在与所有训练样本都一致的假设。在此情形下，试设计一种归纳偏好用于假设选择。</p>
</blockquote>
<p>通常认为两个数据的属性越相近，则更倾向于将他们分为同一类。若相同属性出现了两种不同的分类，则认为它属于与他最临近几个数据的属性。也可以考虑同时去掉所有具有相同属性而不同分类的数据，留下的数据就是没误差的数据，但是可能会丢失部分信息。</p>
<hr>
]]></content>
    
    <summary type="html">
    
      本文是关于周志华「Machine Learning」这本书的 Chapter 1 的学习笔记。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
      <category term="Book:「Machine Learning」" scheme="http://randolph.pro/categories/Machine-Learning/Book-%E3%80%8CMachine-Learning%E3%80%8D/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>♟「Tools」 MathJax</title>
    <link href="http://randolph.pro/2016/11/11/%E2%99%9FMathjax/"/>
    <id>http://randolph.pro/2016/11/11/♟Mathjax/</id>
    <published>2016-11-10T16:00:00.000Z</published>
    <updated>2017-08-15T09:22:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>MathJax</p>
<h1 id="基础"><a href="# 基础" class="headerlink" title="基础"></a>基础</h1><hr>
<h2 id="公式的标记与查看公式"><a href="# 公式的标记与查看公式" class="headerlink" title="公式的标记与查看公式"></a>公式的标记与查看公式 </h2><p> 使用 <code>MathJax</code> 时，需要用一些适当的标记告诉 <code>MathJax</code> 某段文本是公式代码。此外，<code>MathJax</code>中的公式排版有两种方式，<code>inline</code>和 <code>displayed</code>。<code>inline</code> 表示公式嵌入到文本段中，<code>displayed</code>表示公式独自成为一个段落（也就是居中显示）。例如 $f(x)=3*x$ 这是一个 <code>inline</code> 公式，而下面：</p>
<script type="math/tex; mode=display">
f(x)=3*x</script><p>这就是一个 <code>displayed</code> 公式。</p>
<p>在 <code>MathJax</code> 中，默认的 <code>displayed</code> 公式分隔符有 $$$…$$$ 和<code>\[...\]</code>。</p>
<p>而默认的 <code>inline</code> 公式分隔符是<script type="math/tex">...</script>。</p>
<h2 id="希腊字母"><a href="# 希腊字母" class="headerlink" title="希腊字母"></a>希腊字母 </h2><p> 具体见下表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">名称</th>
<th style="text-align:center">大写</th>
<th style="text-align:center">Tex</th>
<th style="text-align:center">小写</th>
<th style="text-align:center">Tex</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">alpha</td>
<td style="text-align:center">A</td>
<td style="text-align:center">A</td>
<td style="text-align:center">$\alpha$</td>
<td style="text-align:center">\alpha</td>
</tr>
<tr>
<td style="text-align:center">beta</td>
<td style="text-align:center">B</td>
<td style="text-align:center">B</td>
<td style="text-align:center">$\beta$</td>
<td style="text-align:center">\beta</td>
</tr>
<tr>
<td style="text-align:center">gamma</td>
<td style="text-align:center">$\Gamma$</td>
<td style="text-align:center">\Gamma</td>
<td style="text-align:center">$\gamma$</td>
<td style="text-align:center">\gamma</td>
</tr>
<tr>
<td style="text-align:center">delta</td>
<td style="text-align:center">$\Delta$</td>
<td style="text-align:center">\Delta</td>
<td style="text-align:center">$\delta$</td>
<td style="text-align:center">\delta</td>
</tr>
<tr>
<td style="text-align:center">epsilon</td>
<td style="text-align:center">E</td>
<td style="text-align:center">E</td>
<td style="text-align:center">$\epsilon$</td>
<td style="text-align:center">\epsilon</td>
</tr>
<tr>
<td style="text-align:center">zeta</td>
<td style="text-align:center">Z</td>
<td style="text-align:center">Z</td>
<td style="text-align:center">$\zeta$</td>
<td style="text-align:center">\zeta</td>
</tr>
<tr>
<td style="text-align:center">eta</td>
<td style="text-align:center">H</td>
<td style="text-align:center">H</td>
<td style="text-align:center">$\eta$</td>
<td style="text-align:center">\eta</td>
</tr>
<tr>
<td style="text-align:center">theta</td>
<td style="text-align:center">$\Theta$</td>
<td style="text-align:center">\Theta</td>
<td style="text-align:center">$\theta$</td>
<td style="text-align:center">\theta</td>
</tr>
<tr>
<td style="text-align:center">iota</td>
<td style="text-align:center">I</td>
<td style="text-align:center">I</td>
<td style="text-align:center">$\iota$</td>
<td style="text-align:center">\iota</td>
</tr>
<tr>
<td style="text-align:center">kappa</td>
<td style="text-align:center">K</td>
<td style="text-align:center">K</td>
<td style="text-align:center">$\kappa$</td>
<td style="text-align:center">\kappa</td>
</tr>
<tr>
<td style="text-align:center">lambda</td>
<td style="text-align:center">$\Lambda$</td>
<td style="text-align:center">\Lambda</td>
<td style="text-align:center">$\lambda$</td>
<td style="text-align:center">\lambda</td>
</tr>
<tr>
<td style="text-align:center">mu</td>
<td style="text-align:center">M</td>
<td style="text-align:center">M</td>
<td style="text-align:center">$\mu$</td>
<td style="text-align:center">\mu</td>
</tr>
<tr>
<td style="text-align:center">nu</td>
<td style="text-align:center">N</td>
<td style="text-align:center">N</td>
<td style="text-align:center">$\nu$</td>
<td style="text-align:center">\nu</td>
</tr>
<tr>
<td style="text-align:center">xi</td>
<td style="text-align:center">$\Xi$</td>
<td style="text-align:center">\Xi</td>
<td style="text-align:center">$\xi$</td>
<td style="text-align:center">\xi</td>
</tr>
<tr>
<td style="text-align:center">omicron</td>
<td style="text-align:center">O</td>
<td style="text-align:center">O</td>
<td style="text-align:center">$\omicron$</td>
<td style="text-align:center">\omicron</td>
</tr>
<tr>
<td style="text-align:center">pi</td>
<td style="text-align:center">$\Pi$</td>
<td style="text-align:center">\Pi</td>
<td style="text-align:center">$\pi$</td>
<td style="text-align:center">\pi</td>
</tr>
<tr>
<td style="text-align:center">rho</td>
<td style="text-align:center">P</td>
<td style="text-align:center">P</td>
<td style="text-align:center">$\rho$</td>
<td style="text-align:center">\rho</td>
</tr>
<tr>
<td style="text-align:center">sigma</td>
<td style="text-align:center">$\Sigma$</td>
<td style="text-align:center">\Sigma</td>
<td style="text-align:center">$\sigma$</td>
<td style="text-align:center">\sigma</td>
</tr>
<tr>
<td style="text-align:center">tau</td>
<td style="text-align:center">T</td>
<td style="text-align:center">T</td>
<td style="text-align:center">$\tau$</td>
<td style="text-align:center">\tau</td>
</tr>
<tr>
<td style="text-align:center">upsilon</td>
<td style="text-align:center">$\Upsilon$</td>
<td style="text-align:center">\Upsilon</td>
<td style="text-align:center">$\upsilon$</td>
<td style="text-align:center">\upsilon</td>
</tr>
<tr>
<td style="text-align:center">phi</td>
<td style="text-align:center">$\Phi$</td>
<td style="text-align:center">\Phi</td>
<td style="text-align:center">$\phi$</td>
<td style="text-align:center">\phi</td>
</tr>
<tr>
<td style="text-align:center">chi</td>
<td style="text-align:center">X</td>
<td style="text-align:center">X</td>
<td style="text-align:center">$\chi$</td>
<td style="text-align:center">\chi</td>
</tr>
<tr>
<td style="text-align:center">psi</td>
<td style="text-align:center">$\Psi$</td>
<td style="text-align:center">\Psi</td>
<td style="text-align:center">$\psi$</td>
<td style="text-align:center">\psi</td>
</tr>
<tr>
<td style="text-align:center">omega</td>
<td style="text-align:center">$\Omega$</td>
<td style="text-align:center">\Omega</td>
<td style="text-align:center">$\omega$</td>
<td style="text-align:center">\omega</td>
</tr>
</tbody>
</table>
</div>
<h2 id="上标 -amp- 下标"><a href="# 上标 -amp- 下标" class="headerlink" title="上标 &amp; 下标"></a>上标 &amp; 下标 </h2><p> 上标和下标分别用 <code>^</code> 和<code>_</code>，如 <code>x_i^2</code>：$x<em>i^2$，但是<code>^</code> 和 `</em><code>分别只对下一个数起作用，比如 </code>10^10<code> 会得到 10101010 而不是 10101010 ，这事需要用 </code>{…}<code> 把要组合的数组合起来，也就是</code>10^{10}`。总之，加了大括号总归不会错。</p>
<h2 id="括号"><a href="# 括号" class="headerlink" title="括号"></a>括号</h2><ul>
<li>小括号和方括号，就用原始的 <code>()</code> 和<code>[]</code>:$(2+2)$ $[3+3]$；</li>
<li>大括号，由于大括号用来 <code> 分组 </code>，所以如果要在公式里面加大括号就使用<code>\{</code> 和<code>\}</code>或者用 <code>\lbrace</code> 和<code>\rbrace</code>来表示。如<code>\{a*b\}</code>:${a∗b}$，<code>\lbrace a*b \rbrace</code>: $\lbrace a*b \rbrace$ ；</li>
<li>尖括号，使用 <code>\langle</code> 和<code>\rangle</code>表示左右尖括号，<code>\langle x \rangle</code>：angle x \rangle$；</li>
<li>上取整，使用 <code>\lceil</code> 和<code>\rceil</code>表示左右尖括号，<code>\lceil x \rceil</code>：$\lceil x \rceil$；</li>
<li>下取整，使用 <code>\lfloor</code> 和<code>\rfloor</code>表示左右尖括号，<code>\lfloor x \rfloor</code>：$\lfloor x \rfloor$；</li>
<li>不可见括号，使用 <code>\left.</code> 和<code>\right.</code>表示，<code>\left. x\right.</code>：$\left. x\right.$。</li>
</ul>
<p>需要注意的是，原始的符号不会随着公式的变大而缩放，比如 <code>\frac{1}{2}</code>：$\frac{1}{2}$。可以使用<code>\left(... \right)</code> 进行缩放，如下：</p>
<p><code>\{\sum_{i=0}^{n}i^2 = \frac {(n^2+n)(2n+1)}{6} \}</code></p>
<script type="math/tex; mode=display">
\{\sum_{i=0}^{n}i^2 = \frac {(n^2+n)(2n+1)}{6} \}</script><p><code>\left\{\sum_{i=0}^{n}i^2 = \frac {(n^2+n)(2n+1)}{6} \right\}</code></p>
<script type="math/tex; mode=display">
\left\{\sum_{i=0}^{n}i^2 = \frac {(n^2+n)(2n+1)}{6} \right\}</script><h2 id="求和 -amp- 积分"><a href="# 求和 -amp- 积分" class="headerlink" title="求和 &amp; 积分"></a>求和 &amp; 积分</h2><ul>
<li><code>\sum</code>用来表示求和符号，上标表示上限，下标表示下限。如，<code>\sum_{i=1}^{n}i^2</code>：$\sum_{i=1}^{n}i^2$；</li>
<li><code>\int</code>用来表示积分符号，上标表示上限，下标表示下限。如，<code>\int_{1}^{\infty}i^2</code>：$\int_{1}^{\infty}i^2$；</li>
<li>类似的符号还有，<code>\prod</code>：$\prod$， <code>\igcup</code>：$\igcup$， <code>\bigcap</code>：$\bigcap$， <code>\iint</code>：$\iint$。</li>
</ul>
<h2 id="Fraction-amp-Radical-Expression"><a href="#Fraction-amp-Radical-Expression" class="headerlink" title="Fraction &amp; Radical Expression"></a>Fraction &amp; Radical Expression</h2><ul>
<li>分式，第一种表示为<code>\frac{a}{b}</code>:abab。第二种表示为<code>{a+1 \over b+1}</code>:a+1b+1a+1b+1;</li>
<li>根式，表示为<code>\sqrt</code>, 比如<code>\sqrt[4]{\frac {x}{y}}</code>:4√xyxy4</li>
</ul>
<h2 id="Font-Family"><a href="#Font-Family" class="headerlink" title="Font Family"></a>Font Family</h2><ul>
<li>黑板粗体字，例如 <code>\mathbb</code>或者<code>\Bbb</code>，这种字体常用来表示实数、整数、有理数、复数的大写字母，<code>\mathbb A</code>:AA, 或者<code>\mathbb {ABCDEFGHIJKLMNOPQRSTUVWXYZ}</code>:ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ;</li>
<li>黑体字，例如 <code>mathbf</code>, <code>\mathbf A</code>:AA, 或者<code>\mathbf {ABCDEFGHIJKLMNOPQRSTUVWXYZ}</code>:ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ;</li>
<li>打印机字体，例如 <code>mathtt</code>, <code>\mathtt A</code>:AA, 或者<code>\mathtt {ABCDEFGHIJKLMNOPQRSTUVWXYZ}</code>:ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ;</li>
<li>罗马字体，例如 <code>mathrm</code>, <code>\mathrm A</code>:AA, 或者<code>\mathrm {ABCDEFGHIJKLMNOPQRSTUVWXYZ}</code>:ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ;</li>
<li>手写字体，例如 <code>mathscr</code>, <code>\mathscr A</code>:AA, 或者<code>\mathscr {ABCDEFGHIJKLMNOPQRSTUVWXYZ}</code>:ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ;</li>
<li><code>Fraktur</code>一种德国字体，例如 <code>mathfrak</code>, <code>\mathfrak A</code>:AA, 或者<code>\mathfrak {ABCDEFGHIJKLMNOPQRSTUVWXYZ}</code>:ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ;</li>
</ul>
<h2 id="Special-Functions-amp-Symbols"><a href="#Special-Functions-amp-Symbols" class="headerlink" title="Special Functions &amp; Symbols"></a>Special Functions &amp; Symbols</h2><ul>
<li>常见的三角函数、求极限符号可以直接使用缩写即可，如 <code>\sin (x)</code>:sin(x)sin⁡(x), <code>\arctan (x)</code>:arctan(x)arctan⁡(x), <code>\lim_{1\rightarrow\infty}x</code>: lim1→∞xlim1→∞x ;</li>
<li>比较运算符，<code>\lt</code>:&lt;&lt;, <code>\gt</code>:&gt;&gt;, <code>\le</code>:≤≤, <code>\ge</code>:≥≥, <code>\neq</code>:≠≠, 同时如果在前面加上 <code>\not</code> 就可以是 <code>\not\lt</code>:≮≮;</li>
<li>运算符， <code>\times</code>:××,<code>\div</code>:÷÷,<code>\pm</code>:±±,<code>\mp</code>: ∓∓ ,<code>x \cdot y</code>:x⋅yx⋅y;</li>
<li>集合关系运算，<code>\cup</code>: ∪∪,<code>\cap</code>: ∩∩,<code>\setminus</code>: ∖∖,<code>\subset</code>: ⊂⊂,<code>\subseteq</code>: ⊆⊆,<code>\subsetneq</code>: ⊊⊊,<code>\supset</code>: ⊃⊃,<code>\in</code>: ∈∈,<code>\notin</code>: ∉∉,<code>\emptyset</code>: ∅∅,<code>\varnothing</code>: ∅∅;</li>
<li>表示排列使用 <code>{n+1 \choose 2k}</code> 或者 <code>\binom{n+1}{2k}</code> 表示 (n+12k)(n+12k);</li>
<li>箭头，<code>\to</code>:→→,<code>\rightarrow</code>:→→,<code>\Rightarrow</code>:⇒⇒,<code>\leftarrow</code>: ←← ,<code>\Leftarrow</code>:⇐⇐,<code>\mapsto</code>:↦↦,</li>
<li>逻辑运算符， <code>\land</code>:∧∧,<code>\lor</code>:∨∨,<code>\lnot</code>:¬¬,<code>\forall</code>: ∀∀ ,<code>\exists</code>:∃∃,<code>\top</code>:⊤⊤,<code>\bot</code>:⊥⊥,<code>\vdash</code>: ⊢⊢ ,<code>\vDash</code>:⊨⊨;</li>
<li>特殊运算符， <code>\star</code>:⋆⋆,<code>\ast</code>:∗∗,<code>\oplus</code>:⊕⊕,<code>\circ</code>: ∘∘ ,<code>\bullet</code>:∙∙;</li>
<li>等于运算符， <code>\approx</code>:≈≈,<code>\sim</code>:∼∼,<code>\cong</code>:≅≅,<code>\equiv</code>: ≡≡ ,<code>\prec</code>:≺≺;</li>
<li>特殊符号， <code>\infty</code>:∞∞,<code>\aleph_0</code>:ℵ0ℵ0,<code>\nabla</code>:∇∇,<code>\partial</code>: ∂∂ ,<code>\Im</code>:Iℑ,<code>\Re</code>:Rℜ;</li>
<li>膜运算符， <code>a \pmod b</code>:a(modb)a(modb)， 例如 <code>a\equiv b\pmod n</code>:a≡b(modn)a≡b(modn);</li>
<li>省略符， <code>\ldots</code>:…… 与 <code>\cdots</code>:⋯⋯，就是位置不一样的 3 个点;</li>
<li>一些希腊字母的变体形式， 例如 <code>\epsilon</code>:ϵϵ,<code>\varepsilon</code>:εε,<code>\phi</code>:ϕϕ,<code>\varphi</code>: φφ 。</li>
</ul>
<h2 id="Space"><a href="#Space" class="headerlink" title="Space"></a>Space</h2><p>由于 <code>MathJax</code> 的空间管理比较特殊，所以 <code>a.b</code>或者 <code>a.........b</code>（…表示空格），都会显示为 <code>a b</code>:abab, 如果要增加间隙，可以使用 <code>a\,b</code>:abab，较宽的间隙 <code>a\;b</code>:abab，或者是使用 <code>a\quad b</code>:abab 或 <code>a\qquad b</code>:abab。还有一种比较方便的方法是不使用转义字符 <code>\</code> 而是这样 <code>a~b</code>:a ba b，可以表示一个空格。</p>
<h2 id="Symbols-over-letters"><a href="#Symbols-over-letters" class="headerlink" title="Symbols over letters"></a>Symbols over letters</h2><p>对于单字符， <code>\hat x</code>:^xx^，对于多个字符使用 <code>\widehat {xy}</code>:ˆxyxy^，类似的还有 <code>\overline {xy}</code>:¯¯¯¯¯¯xyxy¯,<code>\vec {xy}</code>: →xyxy→ ,<code>\overrightarrow {xy}</code>:−→xyxy→,<code>\dot {xy}</code>:˙xyxy˙,<code>\ddot {xy}</code>:¨xyxy¨,</p>
<h1 id="Form"><a href="#Form" class="headerlink" title="Form"></a>Form</h1><hr>
<p>可以使用 $$$\begin{array}{列样式}…\end{array}$$$ 的方式来创建表格，其中 <code> 列样式 </code> 可以 <code>clr</code> 分别表示 <code> 居中 </code> <code> 左</code> <code>右 </code>，还可以用<code>|</code> 表示一条竖线。表格中每一行用 <code>\\</code> 分隔，每一列使用 <code>&amp;</code>分隔，使用 <code>\hline</code>在本行前面加一条直线，例如：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;array&#125;</span><span class="string">&#123;c|lcr&#125;</span></span></div><div class="line">n &amp; <span class="tag">\<span class="name">text</span><span class="string">&#123;Left&#125;</span></span> &amp; <span class="tag">\<span class="name">text</span><span class="string">&#123;Center&#125;</span></span> &amp; <span class="tag">\<span class="name">text</span><span class="string">&#123;Right&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line"><span class="tag">\<span class="name">hline</span></span></div><div class="line">1 &amp; 0.24 &amp; 1 &amp; 125 <span class="tag">\<span class="name">\</span></span></div><div class="line">2 &amp; -1 &amp; 189 &amp; -8 <span class="tag">\<span class="name">\</span></span></div><div class="line">3 &amp; -20 &amp; 2000 &amp; 1+10i <span class="tag">\<span class="name">\</span></span></div><div class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;array&#125;</span></span></div><div class="line">$$</div></pre></td></tr></table></figure>
<p>nLeftCenterRight10.2411252−1189−83−2020001+10inLeftCenterRight10.2411252−1189−83−2020001+10i</p>
<p>一个可以复用的例子如下：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">$$</div><div class="line">\begin&#123;array&#125;&#123;c|ccc&#125;</div><div class="line">n &amp; \text&#123;<span class="number">0</span>&#125; &amp; \text&#123;<span class="number">1</span>&#125; &amp; \text&#123;<span class="number">2</span>&#125; &amp; \text&#123;<span class="number">3</span>&#125;\\</div><div class="line">\hline</div><div class="line"><span class="number">1</span> &amp; <span class="number">1</span> &amp; <span class="number">2</span> &amp; <span class="number">3</span> &amp; <span class="number">4</span> \\</div><div class="line"><span class="number">2</span> &amp; <span class="number">2</span> &amp; <span class="number">3</span> &amp; <span class="number">4</span> &amp; <span class="number">5</span> \\</div><div class="line"><span class="number">3</span> &amp; <span class="number">3</span> &amp; <span class="number">4</span> &amp; <span class="number">5</span> &amp; <span class="number">6</span> \\</div><div class="line"><span class="number">4</span> &amp; <span class="number">4</span> &amp; <span class="number">5</span> &amp; <span class="number">6</span> &amp; <span class="number">7</span> \\</div><div class="line">\end&#123;array&#125;</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>n012311234223453345644567n012311234223453345644567</p>
<h1 id="Matrix"><a href="#Matrix" class="headerlink" title="Matrix"></a>Matrix</h1><hr>
<h2 id="Basis-Usage"><a href="#Basis-Usage" class="headerlink" title="Basis Usage"></a>Basis Usage</h2><p>使用 $$$\begin{matrix}…\end{matrix}$$$ 来表示矩阵，同样使用 <code>\\</code>作为行分隔符，<code>&amp;</code>使用列分隔符。</p>
<p>例如：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line">        <span class="tag">\<span class="name">begin</span><span class="string">&#123;matrix&#125;</span></span></div><div class="line">        1 &amp; x &amp; x^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        1 &amp; y &amp; y^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        1 &amp; z &amp; z^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        <span class="tag">\<span class="name">end</span><span class="string">&#123;matrix&#125;</span></span></div><div class="line">$$</div></pre></td></tr></table></figure>
<p>1xx21yy21zz21xx21yy21zz2</p>
<h2 id="Matrix-Brackets"><a href="#Matrix-Brackets" class="headerlink" title="Matrix Brackets"></a>Matrix Brackets</h2><p>如果要加括号，可以使用 <code>\left</code> 和<code>\right</code>，也可以使用特殊的<code>Matrix</code>，例如:</p>
<ul>
<li>单独加括号</li>
</ul>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line"><span class="tag">\<span class="name">left</span></span> (<span class="tag">\<span class="name">begin</span><span class="string">&#123;matrix&#125;</span></span></div><div class="line">1 &amp; x &amp; x^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">1 &amp; y &amp; y^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">1 &amp; z &amp; z^2 <span class="tag">\<span class="name">\</span></span></div><div class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;matrix&#125;</span></span><span class="tag">\<span class="name">right</span></span> )</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>⎛⎜⎝1xx21yy21zz2⎞⎟⎠(1xx21yy21zz2)</p>
<ul>
<li>使用<code>pmatrix</code></li>
</ul>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line">        <span class="tag">\<span class="name">begin</span><span class="string">&#123;pmatrix&#125;</span></span></div><div class="line">        1 &amp; x &amp; x^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        1 &amp; y &amp; y^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        1 &amp; z &amp; z^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        <span class="tag">\<span class="name">end</span><span class="string">&#123;pmatrix&#125;</span></span></div><div class="line">$$</div></pre></td></tr></table></figure>
<p>⎛⎜⎝1xx21yy21zz2⎞⎟⎠(1xx21yy21zz2)</p>
<ul>
<li>使用<code>bmatrix</code></li>
</ul>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line">        <span class="tag">\<span class="name">begin</span><span class="string">&#123;bmatrix&#125;</span></span></div><div class="line">        1 &amp; x &amp; x^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        1 &amp; y &amp; y^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        1 &amp; z &amp; z^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        <span class="tag">\<span class="name">end</span><span class="string">&#123;bmatrix&#125;</span></span></div><div class="line">$$</div></pre></td></tr></table></figure>
<p>⎡⎢⎣1xx21yy21zz2⎤⎥⎦[1xx21yy21zz2]</p>
<ul>
<li>使用<code>Bmatrix</code></li>
</ul>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line">        <span class="tag">\<span class="name">begin</span><span class="string">&#123;Bmatrix&#125;</span></span></div><div class="line">        1 &amp; x &amp; x^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        1 &amp; y &amp; y^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        1 &amp; z &amp; z^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        <span class="tag">\<span class="name">end</span><span class="string">&#123;Bmatrix&#125;</span></span></div><div class="line">$$</div></pre></td></tr></table></figure>
<p>⎧⎪⎨⎪⎩1xx21yy21zz2⎫⎪⎬⎪⎭{1xx21yy21zz2}</p>
<ul>
<li>使用<code>vmatrix</code></li>
</ul>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line">        <span class="tag">\<span class="name">begin</span><span class="string">&#123;vmatrix&#125;</span></span></div><div class="line">        1 &amp; x &amp; x^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        1 &amp; y &amp; y^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        1 &amp; z &amp; z^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        <span class="tag">\<span class="name">end</span><span class="string">&#123;vmatrix&#125;</span></span></div><div class="line">$$</div></pre></td></tr></table></figure>
<p>∣∣∣∣∣1xx21yy21zz2∣∣∣∣∣|1xx21yy21zz2|</p>
<ul>
<li>使用<code>Vmatrix</code></li>
</ul>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line">        <span class="tag">\<span class="name">begin</span><span class="string">&#123;Vmatrix&#125;</span></span></div><div class="line">        1 &amp; x &amp; x^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        1 &amp; y &amp; y^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        1 &amp; z &amp; z^2 <span class="tag">\<span class="name">\</span></span></div><div class="line">        <span class="tag">\<span class="name">end</span><span class="string">&#123;Vmatrix&#125;</span></span></div><div class="line">$$</div></pre></td></tr></table></figure>
<p>∥∥∥∥∥1xx21yy21zz2∥∥∥∥∥‖1xx21yy21zz2‖</p>
<h2 id="Ellipsis"><a href="#Ellipsis" class="headerlink" title="Ellipsis"></a>Ellipsis</h2><p>可以使用<code>\cdots</code> <code>\ddots</code> <code>\vdots</code> 来省略矩阵中的元素，例如：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;pmatrix&#125;</span></span></div><div class="line">1 &amp; x &amp; x^2 &amp; <span class="tag">\<span class="name">cdots</span></span> &amp; x^n<span class="tag">\<span class="name">\</span></span></div><div class="line">1 &amp; y &amp; y^2 &amp; <span class="tag">\<span class="name">cdots</span></span> &amp; y^n<span class="tag">\<span class="name">\</span></span></div><div class="line">1 &amp; z &amp; z^2 &amp; <span class="tag">\<span class="name">cdots</span></span> &amp; z^n<span class="tag">\<span class="name">\</span></span></div><div class="line"><span class="tag">\<span class="name">vdots</span></span> &amp; <span class="tag">\<span class="name">vdots</span></span> &amp; <span class="tag">\<span class="name">vdots</span></span> &amp; <span class="tag">\<span class="name">ddots</span></span> &amp; <span class="tag">\<span class="name">vdots</span></span><span class="tag">\<span class="name">\</span></span></div><div class="line">1 &amp; n &amp; n^2 &amp; <span class="tag">\<span class="name">cdots</span></span> &amp; n^n<span class="tag">\<span class="name">\</span></span></div><div class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;pmatrix&#125;</span></span></div><div class="line">$$</div></pre></td></tr></table></figure>
<p>⎛⎜⎜⎜⎜⎜⎜⎜⎝1xx2⋯xn1yy2⋯yn1zz2⋯zn⋮⋮⋮⋱⋮1nn2⋯nn⎞⎟⎟⎟⎟⎟⎟⎟⎠(1xx2⋯xn1yy2⋯yn1zz2⋯zn⋮⋮⋮⋱⋮1nn2⋯nn)</p>
<h2 id="Augmented-Matrix"><a href="#Augmented-Matrix" class="headerlink" title="Augmented Matrix"></a>Augmented Matrix</h2><p>增广矩阵需要使用前面的 <code>array</code> 来实现，例如：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$$ \left[</div><div class="line">      \begin&#123;array&#125;&#123;cc|c&#125;</div><div class="line">        <span class="number">1</span>&amp;<span class="number">2</span>&amp;<span class="number">3</span>\\</div><div class="line">        <span class="number">4</span>&amp;<span class="number">5</span>&amp;<span class="number">6</span></div><div class="line">      \end&#123;array&#125;</div><div class="line">    \right]</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>[123456][123456]</p>
<h1 id="Aligned-Formula"><a href="#Aligned-Formula" class="headerlink" title="Aligned Formula"></a>Aligned Formula</h1><hr>
<p>如果需要一系列的公式中等号对齐，可以使用<code>\begin{align}…\end{align}</code>，其中使用 <code>&amp;</code> 来对其位置，例如：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;align&#125;</span></span></div><div class="line"><span class="tag">\<span class="name">sqrt</span><span class="string">&#123;37&#125;</span></span> &amp; = <span class="tag">\<span class="name">sqrt</span><span class="string">&#123;\frac&#123;73^2-1&#125;</span><span class="string">&#123;12^2&#125;</span></span>&#125; <span class="tag">\<span class="name">\</span></span></div><div class="line"> &amp; = <span class="tag">\<span class="name">sqrt</span><span class="string">&#123;\frac&#123;73^2&#125;</span><span class="string">&#123;12^2&#125;</span></span><span class="tag">\<span class="name">cdot</span></span><span class="tag">\<span class="name">frac</span><span class="string">&#123;73^2-1&#125;</span><span class="string">&#123;73^2&#125;</span></span>&#125; <span class="tag">\<span class="name">\</span></span></div><div class="line"> &amp; = <span class="tag">\<span class="name">sqrt</span><span class="string">&#123;\frac&#123;73^2&#125;</span><span class="string">&#123;12^2&#125;</span></span>&#125;<span class="tag">\<span class="name">sqrt</span><span class="string">&#123;\frac&#123;73^2-1&#125;</span><span class="string">&#123;73^2&#125;</span></span>&#125; <span class="tag">\<span class="name">\</span></span></div><div class="line"> &amp; = <span class="tag">\<span class="name">frac</span><span class="string">&#123;73&#125;</span><span class="string">&#123;12&#125;</span></span><span class="tag">\<span class="name">sqrt</span><span class="string">&#123;1 - \frac&#123;1&#125;</span><span class="string">&#123;73^2&#125;</span></span>&#125; <span class="tag">\<span class="name">\</span></span></div><div class="line"> &amp; <span class="tag">\<span class="name">approx</span></span> <span class="tag">\<span class="name">frac</span><span class="string">&#123;73&#125;</span><span class="string">&#123;12&#125;</span></span><span class="tag">\<span class="name">left</span></span>(1 - <span class="tag">\<span class="name">frac</span><span class="string">&#123;1&#125;</span><span class="string">&#123;2\cdot73^2&#125;</span></span><span class="tag">\<span class="name">right</span></span>)</div><div class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;align&#125;</span></span></div><div class="line">$$</div></pre></td></tr></table></figure>
<p>√37=√732−1122=√732122⋅732−1732=√732122√732−1732=7312√1−1732≈7312(1−12⋅732)37=732−1122=732122⋅732−1732=732122732−1732=73121−1732≈7312(1−12⋅732)</p>
<h1 id="Piecewise-Expression"><a href="#Piecewise-Expression" class="headerlink" title="Piecewise Expression"></a>Piecewise Expression</h1><hr>
<p>分类表达式可以使用 <code>\begin{cases}…\end{cases}</code> 其中用 <code>&amp;</code> 指示对齐的位置，例如：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$$</div><div class="line">f(n) =</div><div class="line">\<span class="keyword">begin</span>&#123;cases&#125;</div><div class="line">n/<span class="number">2</span>,  &amp; \text&#123;<span class="keyword">if</span> $n$ is even&#125; \\</div><div class="line"><span class="number">3</span>n+<span class="number">1</span>, &amp; \text&#123;<span class="keyword">if</span> $n$ is odd&#125;  \\</div><div class="line">\<span class="keyword">end</span>&#123;cases&#125;</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>f(n)={n/2,if n is even3n+1,if n is oddf(n)={n/2,if n is even3n+1,if n is odd</p>
<p>上述公式的括号也可以移动到右侧，不过需要使用 <code>array</code> 来实现，如下：</p>
<figure class="highlight excel"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$$</div><div class="line">\left.</div><div class="line">\begin&#123;array&#125;&#123;l&#125;</div><div class="line">\text&#123;<span class="built_in">if</span> $<span class="built_in">n</span>$ is ev<span class="symbol">en:</span>&#125;&amp;<span class="built_in">n</span>/<span class="number">2</span>\\</div><div class="line">\text&#123;<span class="built_in">if</span> $<span class="built_in">n</span>$ is o<span class="symbol">dd:</span>&#125;&amp;<span class="number">3</span><span class="built_in">n</span>+<span class="number">1</span></div><div class="line">\end&#123;array&#125;</div><div class="line">\right\&#125;</div><div class="line">=f(<span class="built_in">n</span>)</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>if n is even:n/2if n is odd:3n+1}=f(n)if n is even:n/2if n is odd:3n+1}=f(n)</p>
<p>最后，如果想分类之间的垂直间隔变大，可以使用 <code>\[2ex]</code> 代替 <code>\</code> 来分隔不同的情况。(<code>3ex</code>,<code>4ex</code>也可以用，<code>1ex</code>相当于原始距离）。</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line">f(n) =</div><div class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;cases&#125;</span></span></div><div class="line">n/2,  &amp; <span class="tag">\<span class="name">text</span><span class="string">&#123;if $n$ is even&#125;</span></span> <span class="tag">\<span class="name">\</span><span class="string">[4ex]</span></span></div><div class="line">3n+1, &amp; <span class="tag">\<span class="name">text</span><span class="string">&#123;if $n$ is odd&#125;</span></span>  <span class="tag">\<span class="name">\</span></span></div><div class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;cases&#125;</span></span></div><div class="line">$$</div></pre></td></tr></table></figure>
<p>f(n)=⎧⎪⎨⎪⎩n/2,if n is even3n+1,if n is oddf(n)={n/2,if n is even3n+1,if n is odd</p>
<h1 id="Space-Problem"><a href="#Space-Problem" class="headerlink" title="Space Problem"></a>Space Problem</h1><hr>
<h2 id="Using-of-frac"><a href="#Using-of-frac" class="headerlink" title="Using of \frac"></a>Using of \frac</h2><p>不要在再指数或者积分中使用 <code>\frac</code>，在指数或者积分表达式中使用 <code>\frac</code>会使表达式看起来不清晰，因此在专业的数学排版中很少被使用。应该使用一个水平的 <code>/</code> 来代替，效果如下：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;array&#125;</span><span class="string">&#123;cc&#125;</span></span></div><div class="line"><span class="tag">\<span class="name">mathrm</span><span class="string">&#123;Bad&#125;</span></span> &amp; <span class="tag">\<span class="name">mathrm</span><span class="string">&#123;Better&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line"><span class="tag">\<span class="name">hline</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">e^&#123;i<span class="tag">\<span class="name">frac</span><span class="string">&#123;\pi&#125;</span></span>2&#125; <span class="tag">\<span class="name">quad</span></span> e^&#123;<span class="tag">\<span class="name">frac</span><span class="string">&#123;i\pi&#125;</span></span>2&#125;&amp; e^&#123;i<span class="tag">\<span class="name">pi</span></span>/2&#125; <span class="tag">\<span class="name">\</span></span></div><div class="line"><span class="tag">\<span class="name">int</span></span>_&#123;-<span class="tag">\<span class="name">frac</span></span><span class="tag">\<span class="name">pi</span></span>2&#125;^<span class="tag">\<span class="name">frac</span></span><span class="tag">\<span class="name">pi</span></span>2 <span class="tag">\<span class="name">sin</span></span> x<span class="tag">\<span class="name">,</span></span>dx &amp; <span class="tag">\<span class="name">int</span></span>_&#123;-<span class="tag">\<span class="name">pi</span></span>/2&#125;^&#123;<span class="tag">\<span class="name">pi</span></span>/2&#125;<span class="tag">\<span class="name">sin</span></span> x<span class="tag">\<span class="name">,</span></span>dx <span class="tag">\<span class="name">\</span></span></div><div class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;array&#125;</span></span></div><div class="line">$$</div></pre></td></tr></table></figure>
<p>BadBettereiπ2eiπ2eiπ/2∫π2−π2sinxdx∫π/2−π/2sinxdxBadBettereiπ2eiπ2eiπ/2∫−π2π2sin⁡xdx∫−π/2π/2sin⁡xdx</p>
<h2 id="Separator"><a href="#Separator" class="headerlink" title="Separator"></a>Separator</h2><p>使用 <code>\mid</code> 代替 <code>|</code> 作为分隔符，符号 <code>|</code> 作为分隔符时有排版空间大小的问题，应该使用 <code>\mid</code> 代替。效果如下：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;array&#125;</span><span class="string">&#123;cc&#125;</span></span></div><div class="line"><span class="tag">\<span class="name">mathrm</span><span class="string">&#123;Bad&#125;</span></span> &amp; <span class="tag">\<span class="name">mathrm</span><span class="string">&#123;Better&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line"><span class="tag">\<span class="name">hline</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line"><span class="tag">\<span class="name">&#123;</span></span>x|x^2<span class="tag">\<span class="name">in</span></span><span class="tag">\<span class="name">Bbb</span></span> Z<span class="tag">\<span class="name">&#125;</span></span> &amp; <span class="tag">\<span class="name">&#123;</span></span>x<span class="tag">\<span class="name">mid</span></span> x^2<span class="tag">\<span class="name">in</span></span><span class="tag">\<span class="name">Bbb</span></span> Z<span class="tag">\<span class="name">&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;array&#125;</span></span></div><div class="line">$$</div></pre></td></tr></table></figure>
<p>BadBetter{x|x2∈Z}{x∣x2∈Z}BadBetter{x|x2∈Z}{x∣x2∈Z}</p>
<h2 id="Multiple-Integration"><a href="#Multiple-Integration" class="headerlink" title="Multiple Integration"></a>Multiple Integration</h2><p>对于多重积分，不要使用 <code>\int\int</code> 此类的表达，应该使用 <code>\iint</code> <code>\iiint</code> 等特殊形式。效果如下：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;array&#125;</span><span class="string">&#123;cc&#125;</span></span></div><div class="line"><span class="tag">\<span class="name">mathrm</span><span class="string">&#123;Bad&#125;</span></span> &amp; <span class="tag">\<span class="name">mathrm</span><span class="string">&#123;Better&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line"><span class="tag">\<span class="name">hline</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line"><span class="tag">\<span class="name">int</span></span><span class="tag">\<span class="name">int</span></span>_S f(x)<span class="tag">\<span class="name">,</span></span>dy<span class="tag">\<span class="name">,</span></span>dx &amp; <span class="tag">\<span class="name">iint</span></span>_S f(x)<span class="tag">\<span class="name">,</span></span>dy<span class="tag">\<span class="name">,</span></span>dx <span class="tag">\<span class="name">\</span></span></div><div class="line"><span class="tag">\<span class="name">int</span></span><span class="tag">\<span class="name">int</span></span><span class="tag">\<span class="name">int</span></span>_V f(x)<span class="tag">\<span class="name">,</span></span>dz<span class="tag">\<span class="name">,</span></span>dy<span class="tag">\<span class="name">,</span></span>dx &amp; <span class="tag">\<span class="name">iiint</span></span>_V f(x)<span class="tag">\<span class="name">,</span></span>dz<span class="tag">\<span class="name">,</span></span>dy<span class="tag">\<span class="name">,</span></span>dx</div><div class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;array&#125;</span></span></div><div class="line">$$</div></pre></td></tr></table></figure>
<p>BadBetter∫∫Sf(x)dydx∬Sf(x)dydx∫∫∫Vf(x)dzdydx∭Vf(x)dzdydxBadBetter∫∫Sf(x)dydx∬Sf(x)dydx∫∫∫Vf(x)dzdydx∭Vf(x)dzdydx</p>
<h1 id="Continued-Fraction"><a href="#Continued-Fraction" class="headerlink" title="Continued Fraction"></a>Continued Fraction</h1><hr>
<p>书写连分数表达式时，请使用 <code>\cfrac</code> 代替 <code>\frac</code> 或者 <code>\over</code> 两者效果对比如下:</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$$</div><div class="line">x = a_<span class="number">0</span> + \cfrac&#123;<span class="number">1</span>^<span class="number">2</span>&#125;&#123;a_1</div><div class="line">          + \cfrac&#123;<span class="number">2</span>^<span class="number">2</span>&#125;&#123;a_2</div><div class="line">          + \cfrac&#123;<span class="number">3</span>^<span class="number">2</span>&#125;&#123;a_3 + \cfrac&#123;<span class="number">4</span>^<span class="number">4</span>&#125;&#123;a_4 + \cdots&#125;&#125;&#125;&#125; \tag&#123;\cfrac&#125;</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>x=a0+12a1+22a2+32a3+44a4+⋯(\cfrac)(\cfrac)x=a0+12a1+22a2+32a3+44a4+⋯</p>
<figure class="highlight gams"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">$</span><span class="symbol">$</span></div><div class="line">x = a_0 + \<span class="built-in">frac</span>&#123;<span class="number">1</span>^<span class="number">2</span>&#125;&#123;a_1</div><div class="line">          + \<span class="built-in">frac</span>&#123;<span class="number">2</span>^<span class="number">2</span>&#125;&#123;a_2</div><div class="line">          + \<span class="built-in">frac</span>&#123;<span class="number">3</span>^<span class="number">2</span>&#125;&#123;a_3 + \<span class="built-in">frac</span>&#123;<span class="number">4</span>^<span class="number">4</span>&#125;&#123;a_4 + \cdots&#125;&#125;&#125;&#125; \tag&#123;\<span class="built-in">frac</span>&#125;</div><div class="line"><span class="symbol">$</span><span class="symbol">$</span></div></pre></td></tr></table></figure>
<p>x=a0+12a1+22a2+32a3+44a4+⋯(\frac)(\frac)x=a0+12a1+22a2+32a3+44a4+⋯</p>
<h1 id="Equation-Set"><a href="#Equation-Set" class="headerlink" title="Equation Set"></a>Equation Set</h1><hr>
<p>对于方程组可以使用 <code>\begin{array} … \end{array}</code> 与<code>\left{…\right.</code>配合，表示方程组：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line">    <span class="tag">\<span class="name">left</span></span><span class="tag">\<span class="name">&#123;</span></span></div><div class="line">        <span class="tag">\<span class="name">begin</span><span class="string">&#123;array&#125;</span><span class="string">&#123;c&#125;</span></span></div><div class="line">            a_1x+b_1y+c_1z=d_1 <span class="tag">\<span class="name">\</span></span></div><div class="line">            a_2x+b_2y+c_2z=d_2 <span class="tag">\<span class="name">\</span></span></div><div class="line">            a_3x+b_3y+c_3z=d_3</div><div class="line">        <span class="tag">\<span class="name">end</span><span class="string">&#123;array&#125;</span></span></div><div class="line">    <span class="tag">\<span class="name">right</span></span>.</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>⎧⎪⎨⎪⎩a1x+b1y+c1z=d1a2x+b2y+c2z=d2a3x+b3y+c3z=d3{a1x+b1y+c1z=d1a2x+b2y+c2z=d2a3x+b3y+c3z=d3</p>
<p>同时，还可以使用 <code>\begin{cases}…\end{cases}</code> 表达同样的方程组，如：</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$$</div><div class="line">    \begin&#123;cases&#125;</div><div class="line">        a_1x+b_1y+<span class="attribute">c_1z</span>=d_1 \\</div><div class="line">        a_2x+b_2y+<span class="attribute">c_2z</span>=d_2 \\</div><div class="line">        a_3x+b_3y+<span class="attribute">c_3z</span>=d_3</div><div class="line">    \end&#123;cases&#125;</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>⎧⎨⎩a1x+b1y+c1z=d1a2x+b2y+c2z=d2a3x+b3y+c3z=d3{a1x+b1y+c1z=d1a2x+b2y+c2z=d2a3x+b3y+c3z=d3</p>
<p>对齐方程组中的 <code>=</code> 号，可以使用 <code>\being{aligned} .. \end{aligned}</code>，如：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line">    <span class="tag">\<span class="name">left</span></span><span class="tag">\<span class="name">&#123;</span></span></div><div class="line">        <span class="tag">\<span class="name">begin</span><span class="string">&#123;aligned&#125;</span></span></div><div class="line">            a_1x+b_1y+c_1z &amp;=d_1+e_1 <span class="tag">\<span class="name">\</span></span></div><div class="line">            a_2x+b_2y&amp;=d_2 <span class="tag">\<span class="name">\</span></span></div><div class="line">            a_3x+b_3y+c_3z &amp;=d_3</div><div class="line">        <span class="tag">\<span class="name">end</span><span class="string">&#123;aligned&#125;</span></span></div><div class="line">    <span class="tag">\<span class="name">right</span></span>.</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>⎧⎪⎨⎪⎩a1x+b1y+c1z=d1+e1a2x+b2y=d2a3x+b3y+c3z=d3{a1x+b1y+c1z=d1+e1a2x+b2y=d2a3x+b3y+c3z=d3</p>
<p>如果要对齐 = 号 和项，可以使用 \being{array}{列样式} … \end{array}，如：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line">    <span class="tag">\<span class="name">left</span></span><span class="tag">\<span class="name">&#123;</span></span></div><div class="line">        <span class="tag">\<span class="name">begin</span><span class="string">&#123;array&#125;</span><span class="string">&#123;ll&#125;</span></span></div><div class="line">            a_1x+b_1y+c_1z &amp;=d_1+e_1 <span class="tag">\<span class="name">\</span></span></div><div class="line">            a_2x+b_2y &amp;=d_2 <span class="tag">\<span class="name">\</span></span></div><div class="line">            a_3x+b_3y+c_3z &amp;=d_3</div><div class="line">        <span class="tag">\<span class="name">end</span><span class="string">&#123;array&#125;</span></span></div><div class="line">    <span class="tag">\<span class="name">right</span></span>.</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>⎧⎪⎨⎪⎩a1x+b1y+c1z=d1+e1a2x+b2y=d2a3x+b3y+c3z=d3{a1x+b1y+c1z=d1+e1a2x+b2y=d2a3x+b3y+c3z=d3</p>
<h1 id="Color"><a href="#Color" class="headerlink" title="Color"></a>Color</h1><hr>
<p>命名颜色是浏览器相关的，如果浏览器没有定义相关的颜色名称，则相关文本将被渲染为黑色，具体列表如下：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line">    <span class="tag">\<span class="name">begin</span><span class="string">&#123;array&#125;</span><span class="string">&#123;|rc|&#125;</span></span></div><div class="line">        <span class="tag">\<span class="name">hline</span></span></div><div class="line">            <span class="tag">\<span class="name">verb</span></span>+<span class="tag">\<span class="name">color</span><span class="string">&#123;black&#125;</span><span class="string">&#123;text&#125;</span></span>+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;black&#125;</span><span class="string">&#123;text&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">            <span class="tag">\<span class="name">verb</span></span>+<span class="tag">\<span class="name">color</span><span class="string">&#123;gray&#125;</span><span class="string">&#123;text&#125;</span></span>+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;gray&#125;</span><span class="string">&#123;text&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">            <span class="tag">\<span class="name">verb</span></span>+<span class="tag">\<span class="name">color</span><span class="string">&#123;silver&#125;</span><span class="string">&#123;text&#125;</span></span>+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;silver&#125;</span><span class="string">&#123;text&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">            <span class="tag">\<span class="name">verb</span></span>+<span class="tag">\<span class="name">color</span><span class="string">&#123;white&#125;</span><span class="string">&#123;text&#125;</span></span>+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;white&#125;</span><span class="string">&#123;text&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">        <span class="tag">\<span class="name">hline</span></span></div><div class="line">            <span class="tag">\<span class="name">verb</span></span>+<span class="tag">\<span class="name">color</span><span class="string">&#123;maroon&#125;</span><span class="string">&#123;text&#125;</span></span>+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;maroon&#125;</span><span class="string">&#123;text&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">            <span class="tag">\<span class="name">verb</span></span>+<span class="tag">\<span class="name">color</span><span class="string">&#123;red&#125;</span><span class="string">&#123;text&#125;</span></span>+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;red&#125;</span><span class="string">&#123;text&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">            <span class="tag">\<span class="name">verb</span></span>+<span class="tag">\<span class="name">color</span><span class="string">&#123;yellow&#125;</span><span class="string">&#123;text&#125;</span></span>+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;yellow&#125;</span><span class="string">&#123;text&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">            <span class="tag">\<span class="name">verb</span></span>+<span class="tag">\<span class="name">color</span><span class="string">&#123;lime&#125;</span><span class="string">&#123;text&#125;</span></span>+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;lime&#125;</span><span class="string">&#123;text&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">            <span class="tag">\<span class="name">verb</span></span>+<span class="tag">\<span class="name">color</span><span class="string">&#123;olive&#125;</span><span class="string">&#123;text&#125;</span></span>+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;olive&#125;</span><span class="string">&#123;text&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">            <span class="tag">\<span class="name">verb</span></span>+<span class="tag">\<span class="name">color</span><span class="string">&#123;green&#125;</span><span class="string">&#123;text&#125;</span></span>+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;green&#125;</span><span class="string">&#123;text&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">            <span class="tag">\<span class="name">verb</span></span>+<span class="tag">\<span class="name">color</span><span class="string">&#123;teal&#125;</span><span class="string">&#123;text&#125;</span></span>+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;teal&#125;</span><span class="string">&#123;text&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">            <span class="tag">\<span class="name">verb</span></span>+<span class="tag">\<span class="name">color</span><span class="string">&#123;aqua&#125;</span><span class="string">&#123;text&#125;</span></span>+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;aqua&#125;</span><span class="string">&#123;text&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">            <span class="tag">\<span class="name">verb</span></span>+<span class="tag">\<span class="name">color</span><span class="string">&#123;blue&#125;</span><span class="string">&#123;text&#125;</span></span>+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;blue&#125;</span><span class="string">&#123;text&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">            <span class="tag">\<span class="name">verb</span></span>+<span class="tag">\<span class="name">color</span><span class="string">&#123;navy&#125;</span><span class="string">&#123;text&#125;</span></span>+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;navy&#125;</span><span class="string">&#123;text&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">            <span class="tag">\<span class="name">verb</span></span>+<span class="tag">\<span class="name">color</span><span class="string">&#123;purple&#125;</span><span class="string">&#123;text&#125;</span></span>+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;purple&#125;</span><span class="string">&#123;text&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">            <span class="tag">\<span class="name">verb</span></span>+<span class="tag">\<span class="name">color</span><span class="string">&#123;fuchsia&#125;</span><span class="string">&#123;text&#125;</span></span>+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;magenta&#125;</span><span class="string">&#123;text&#125;</span></span> <span class="tag">\<span class="name">\</span></span></div><div class="line">        <span class="tag">\<span class="name">hline</span></span></div><div class="line">    <span class="tag">\<span class="name">end</span><span class="string">&#123;array&#125;</span></span></div><div class="line">$$</div></pre></td></tr></table></figure>
<p>\color{black}{text}text\color{gray}{text}text\color{silver}{text}text\color{white}{text}text\color{maroon}{text}text\color{red}{text}text\color{yellow}{text}text\color{lime}{text}text\color{olive}{text}text\color{green}{text}text\color{teal}{text}text\color{aqua}{text}text\color{blue}{text}text\color{navy}{text}text\color{purple}{text}text\color{fuchsia}{text}text\color{black}{text}text\color{gray}{text}text\color{silver}{text}text\color{white}{text}text\color{maroon}{text}text\color{red}{text}text\color{yellow}{text}text\color{lime}{text}text\color{olive}{text}text\color{green}{text}text\color{teal}{text}text\color{aqua}{text}text\color{blue}{text}text\color{navy}{text}text\color{purple}{text}text\color{fuchsia}{text}text</p>
<p>此外，<code>HTML5</code>与 <code>CSS3</code> 也定义了一些颜色名称。 同时，颜色也可以使用 <code>#rgb</code> 的形式来表示，<code>r</code>、<code>g</code>、<code>b</code>分别表示代表颜色值得 16 进制数，如:</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="formula">$$</span></div><div class="line">    <span class="tag">\<span class="name">begin</span><span class="string">&#123;array&#125;</span><span class="string">&#123;|rrrrrrrr|&#125;</span></span><span class="tag">\<span class="name">hline</span></span></div><div class="line">        <span class="tag">\<span class="name">verb</span></span>+#000+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;#000&#125;</span><span class="string">&#123;text&#125;</span></span> &amp; &amp; &amp;</div><div class="line">        <span class="tag">\<span class="name">verb</span></span>+#00F+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;#00F&#125;</span><span class="string">&#123;text&#125;</span></span> &amp; &amp; <span class="tag">\<span class="name">\</span></span></div><div class="line">        &amp; &amp; <span class="tag">\<span class="name">verb</span></span>+#0F0+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;#0F0&#125;</span><span class="string">&#123;text&#125;</span></span> &amp;</div><div class="line">        &amp; &amp; <span class="tag">\<span class="name">verb</span></span>+#0FF+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;#0FF&#125;</span><span class="string">&#123;text&#125;</span></span><span class="tag">\<span class="name">\</span></span></div><div class="line">        <span class="tag">\<span class="name">verb</span></span>+#F00+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;#F00&#125;</span><span class="string">&#123;text&#125;</span></span> &amp; &amp; &amp;</div><div class="line">        <span class="tag">\<span class="name">verb</span></span>+#F0F+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;#F0F&#125;</span><span class="string">&#123;text&#125;</span></span> &amp; &amp; <span class="tag">\<span class="name">\</span></span></div><div class="line">        &amp; &amp; <span class="tag">\<span class="name">verb</span></span>+#FF0+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;#FF0&#125;</span><span class="string">&#123;text&#125;</span></span> &amp;</div><div class="line">        &amp; &amp; <span class="tag">\<span class="name">verb</span></span>+#FFF+ &amp; <span class="tag">\<span class="name">color</span><span class="string">&#123;#FFF&#125;</span><span class="string">&#123;text&#125;</span></span><span class="tag">\<span class="name">\</span></span></div><div class="line">        <span class="tag">\<span class="name">hline</span></span></div><div class="line">    <span class="tag">\<span class="name">end</span><span class="string">&#123;array&#125;</span></span></div><div class="line">$$</div></pre></td></tr></table></figure>
<p>#000text#00Ftext#0F0text#0FFtext#F00text#F0Ftext#FF0text#FFFtext#000text#00Ftext#0F0text#0FFtext#F00text#F0Ftext#FF0text#FFFtext</p>
<h1 id="Formula-Mark-amp-Quote"><a href="#Formula-Mark-amp-Quote" class="headerlink" title="Formula Mark &amp; Quote"></a>Formula Mark &amp; Quote</h1><hr>
<p>使用 <code>\tag{yourtag}</code> 来标记公式，如果想在之后引用该公式，则还需要加上 <code>\label{yourlabel}</code> 在<code>\tag</code>之后，如:</p>
<p>a:=x2−y3(1-1)(1-1)a:=x2−y3</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="tag">\<span class="name">[</span></span></div><div class="line">a := x^2-y^3 <span class="tag">\<span class="name">tag</span><span class="string">&#123;1-1&#125;</span></span><span class="tag">\<span class="name">label</span><span class="string">&#123; 1-1&#125;</span></span></div><div class="line"><span class="tag">\<span class="name">]</span></span></div></pre></td></tr></table></figure>
<p>为了引用公式，可以使用<code>\eqref{rlabel}</code>，如：</p>
<p>a+y3<a href="https://melissanotes.github.io/HTML/Docs/MathJax-Quick-Memo.md.html#" target="_blank" rel="external">(???)</a>=x2a+y3=(???)x2</p>
<figure class="highlight gams"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">$</span><span class="symbol">$</span></div><div class="line">a+y^<span class="number">3</span> \stackrel&#123;\eqref&#123;<span class="number">1</span><span class="number">-1</span>&#125;&#125;= x^<span class="number">2</span></div><div class="line"><span class="symbol">$</span><span class="symbol">$</span></div></pre></td></tr></table></figure>
<h1 id="HTML-Color-Quick-Reference"><a href="#HTML-Color-Quick-Reference" class="headerlink" title="HTML Color Quick Reference"></a>HTML Color Quick Reference</h1><hr>
<p><a href="http://html-color-codes.info/chinese/" target="_blank" rel="external">HTML 色彩代码提取</a></p>
<p><a href="http://www.w3schools.com/colors/colors_picker.asp" target="_blank" rel="external">HTML Color Picker</a></p>
<h1 id="Mathematics-Symbols-Quick-Reference"><a href="#Mathematics-Symbols-Quick-Reference" class="headerlink" title="Mathematics Symbols Quick Reference"></a>Mathematics Symbols Quick Reference</h1><hr>
<p><code>Detexify</code>, 一个数学符号查询手写识别系统，如图：</p>
<p><img src="https://melissanotes.github.io/Docs/assets/MathJax-Quick-Memo-51ba3.png?v=1475994320721" alt="detexify-demo"></p>
<p><code>Detexify</code> : <a href="http://detexify.kirelabs.org/classify.html" target="_blank" rel="external">Detexify2</a></p>
]]></content>
    
    <summary type="html">
    
      本文是关于 MathJax 的一些写法。
    
    </summary>
    
      <category term="Tools" scheme="http://randolph.pro/categories/Tools/"/>
    
    
      <category term="Tools" scheme="http://randolph.pro/tags/Tools/"/>
    
  </entry>
  
  <entry>
    <title>「Computer Vision」 JPEG Compression</title>
    <link href="http://randolph.pro/2016/04/15/%E3%80%8CComputer%20Vision%E3%80%8D%20JPEG%20Compression/"/>
    <id>http://randolph.pro/2016/04/15/「Computer Vision」 JPEG Compression/</id>
    <published>2016-04-14T16:00:00.000Z</published>
    <updated>2017-08-09T14:21:51.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4432/35506833264_65d70af92c_o.jpg" alt=""></p>
<p>有关「Computer Vision」的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/Computer-Vision/">「Computer Vision」</a></p>
<h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><ul>
<li><strong>颜色模式转换：RBG to YUV</strong></li>
<li><strong>采样 &amp; 分块</strong></li>
<li><strong>DCT 离散余弦变换</strong></li>
<li><strong>量化</strong></li>
<li><strong>DPCM 差分编码（对 DC 系数）</strong></li>
<li><strong>RLE 游程编码（对 AC 系数）：Zig-Zag 扫描</strong></li>
<li><strong>熵编码：Huffman 编码</strong></li>
</ul>
<h1 id="颜色模式转换"><a href="# 颜色模式转换" class="headerlink" title="颜色模式转换"></a>颜色模式转换</h1><p>JPEG 采用的是 YCrCb 颜色空间，而 BMP 采用的是 RGB 颜色空间，要想对 BMP 图片进行压缩，首先需要进行颜色空间的转换。YCrCb 颜色空间中，$Y$ 代表亮度，$Cr$，$Cb$ 则代表色度和饱和度（也有人将 $Cb$，$Cr$ 两者统称为色度），三者通常以 $Y$，$U$，$V$ 来表示，即用 $U$ 代表 $Cb$，用 $V$ 代表 $Cr$。RGB 和 YCrCb 之间的转换关系如下所示：</p>
<script type="math/tex; mode=display">
\begin{align}
Y &= &0.299R &+ 0.587G + 0.114B \cr
Cb &= &-0.168R &- 0.331G + 0.500B  + 128 \cr
Cr &= &0.500R &- 0.418G - 0.081B  + 128 
\end{align}</script><h1 id="采样"><a href="# 采样" class="headerlink" title="采样"></a>采样 </h1><p> 研究发现，<strong>人眼对亮度变换的敏感度要比对色彩变换的敏感度高出很多</strong>。</p>
<p>因此，我们可以认为 Y 分量要比 Cb，Cr 分量重要的多。在 BMP 图片中，RGB 三个分量各采用一个字节进行采样，也就是我们常听到的 RGB-888 的模式；而 JPEG 图片中，通常采用两种采样方式：YUV-411 和 YUV-422，它们所代表的意义是 Y，Cb，Cr 三个分量的数据取样比例一般是 4：1：1 或者 4：2：2（4：1：1 含义就是：在 $2\times 2$ 的单元中，本应分别有 4 个 Y，4 个 U，4 个 V 值，用 12 个字节进行存储。经过 4:1:1 采样处理后，每个单元中的值分别有 4 个 Y、1 个 U、1 个 V，只要用 6 个字节就可以存储了）</p>
<p>这样的采样方式，虽然损失了一定的精度但也在人眼不太察觉到的范围内减小了数据的存储量。当然，JPEG 格式里面也允许将每个点的 U，V 值都记录下来。</p>
<h1 id="分块"><a href="# 分块" class="headerlink" title="分块"></a>分块 </h1><p> 由于后面的 DCT 变换是是对 $8 \times 8$ 的子块进行处理的，因此，在进行 DCT 变换之前必须把源图象数据进行分块。源图象中每点的 3 个分量是交替出现的，先要把这 3 个分量分开，存放到 3 张表中去。然后由左及右，由上到下依次读取 $8 \times 8$ 的子块，存放在长度为 64 的表中，即可以进行 DCT 变换。</p>
<p>注意，编码时，程序从源数据中读取一个 $8 \times 8$ 的数据块后，进行 DCT 变换，量化，编码，然后再读取、处理下一个 $8 \times 8$ 的数据块。</p>
<p><strong>JPEG 编码是以每 $8 \times 8$ 个点为一个单位进行处理的。所以如果原始图片的长宽不是 8 的倍数， 都需要先补成 8 的倍数， 使其可以进行一块块的处理。将原始图像数据分为 $8 \times 8$ 的数据单元矩阵之后，还必须将每个数值减去 128，然后一一带入 DCT 变换公式，即可达到 DCT 变换的目的。图像的数据值必须减去 128，是因为 DCT 公式所接受的数字范围是 -128 到 127 之间。</strong></p>
<h2 id="DCT- 离散余弦变化"><a href="#DCT- 离散余弦变化" class="headerlink" title="DCT 离散余弦变化"></a>DCT 离散余弦变化</h2><p>DCT（Discrete Cosine Transform，离散余弦变换），是码率压缩中常用的一种变换编码方法。任何连续的实对称函数的傅里叶变换中只含有余弦项，因此，余弦变换同傅里叶变换一样具有明确的物理意义。DCT 是先将整体图像分成 $N \times N$ 的像素块，然后针对 $N \times N$ 的像素块逐一进行 DCT 操作。需要提醒的是，JPEG 的编码过程需要进行正向离散余弦变换，而解码过程则需要反向离散余弦变换。</p>
<h3 id="二维 -DCT- 变换公式："><a href="# 二维 -DCT- 变换公式：" class="headerlink" title="二维 DCT 变换公式："></a>二维 DCT 变换公式：</h3><script type="math/tex; mode=display">
F(u,v)=c(u)c(v)\sum_{i=0}^{N-1}\sum_{j=0}^{N-1}f(i,j)\cos \left[\frac{(i+0.5)\pi}{N}u\right]\cos \left[\frac{(j+0.5)\pi}{N}v\right]</script><script type="math/tex; mode=display">
c(u)=
\begin{cases}
\sqrt{\frac 1N},& u = 0 \cr
\sqrt{2 \over N},& u \neq 0 \cr
\end{cases}</script><h3 id="二维 -DCT- 逆变换公式："><a href="# 二维 -DCT- 逆变换公式：" class="headerlink" title="二维 DCT 逆变换公式："></a>二维 DCT 逆变换公式：</h3><script type="math/tex; mode=display">
f(i,j)=\sum_{u=0}^{N-1}\sum_{v=0}^{N-1}c(u)c(v)F(u,v)\cos \left[\frac{(i+0.5)\pi}{N}u\right]\cos \left[\frac{(j+0.5)\pi}{N}v\right]</script><script type="math/tex; mode=display">
c(u)=
\begin{cases}
\sqrt{\frac 1N},& u = 0 \cr
\sqrt{2 \over N},& u \neq 0 \cr
\end{cases}</script><p>这里的 N 是水平、垂直方向的像素数目，一般取值为 8。$8 \times 8$ 的二维像素块经过 DCT 操作之后，就得到了 $8 \times 8$ 的变换系数矩阵。这些系数，都有具体的物理含义。</p>
<p>例如，$U=0$，$V=0$ 时的 $F(0,0)$ 是原来的 64 个数据的均值，相当于直流分量，也有人称之为 DC 系数或者直流系数。随着 $U$，$V$ 的增加，相另外的 63 个系数则代表了水平空间频率和垂直空间频率分量（高频分量）的大小，多半是一些接近于 0 的正负浮点数，我们称之为交流系数 AC。</p>
<p>DCT 变换后的 $8 \times 8$ 的系数矩阵中，低频分量集中在矩阵的左上角。高频成分则集中在右下角。这里，我们暂时先只考虑水平方向上一行数据（8 个像素）的情况时的 DCT 变换，从而来说明其物理意义。如下图所示：</p>
<p><img src="https://farm6.staticflickr.com/5342/31519596181_839559fe8c_o.png" alt=""></p>
<p>原始的图像信号（最左边的波形）经过 DCT 变换之后变成了 8 个波，其中第一个波为直流成分，其余 7 个为交流成分。可见图像信号被分解为直流成分和一些从低频到高频的各种余弦成分。而 DCT 系数只表示了该种成分所占原图像信号的份额大小。显然，恢复图像信息可以表示为下面的式子：</p>
<script type="math/tex; mode=display">
F(n) = C(n) \cdot E(n)</script><p>这里，$E(n)$ 是一个基底，$C(n)$ 是 DCT 系数，$F(n)$ 则是图像信号；如果考虑垂直方向的变化，那就需要一个二维的基底。大学里面的信号处理，傅里叶变换等课程上也讲过，任何信号都可以被分解为基波和不同幅度的谐波的组合，而 DCT 变换的物理意义也正是如此。</p>
<p><strong>由于大多数图像的高频分量比较小，相应的图像高频分量的 DCT 系数经常接近于 0，再加上高频分量中只包含了图像的细微的细节变化信息，而人眼对这种高频成分的失真不太敏感，所以，可以考虑将这一些高频成分予以抛弃，从而降低需要传输的数据量。这样一来，传送 DCT 变换系数的所需要的编码长度要远远小于传送图像像素的编码长度。到达接收端之后通过反离散余弦变换就可以得到原来的数据，虽然这么做存在一定的失真，但人眼是可接受的，而且对这种微小的变换是不敏感的。所谓 JPEG 的有损压缩，损的是量化过程中的高频部分，romove 50% 的高频信息可能对于编码信息只损失了 5%。</strong></p>
<h1 id="量化"><a href="# 量化" class="headerlink" title="量化"></a>量化 </h1><p> 图像数据转换为 DCT 频率系数之后，还要进行量化阶段，才能进入编码过程。量化阶段需要两个 $8 \times 8$ 量化矩阵数据，将频率系数除以量化矩阵的值之后取整，即完成了量化过程。</p>
<p>简而言之，所谓量化就是用像素值除以量化表对应值所得的结果。</p>
<p>由于量化表左上角的值较小，右下角的值较大，这样就起到了保持低频分量，抑制高频分量的目的。JPEG 使用的颜色是 YUV 格式。我们提到过，$Y$ 分量代表了亮度信息，$UV$ 分量代表了色差信息。相比而言，$Y$ 分量更重要一些。我们可以对  $Y$ 采用细量化，对 $UV$ 采用粗量化，可进一步提高压缩比。所以上面所说的量化表通常有两张，一个是专门处理亮度的频率系数，另一个则是针对色度的频率系数，即一张是针对 $Y$ 的，而另一张是针对 $UV$ 的。<strong>当频率系数经过量化之后，将频率系数由浮点数转变为整数，这才便于执行最后的编码。不难发现，经过量化阶段之后，所有的数据只保留了整数近似值，也就再度损失了一些数据内容。在 JPEG 算法中，由于对亮度和色度的精度要求不同，分别对亮度和色度采用不同的量化表。</strong></p>
<p>下图给出 JPEG 的亮度量化表和色度量化表，该量化表是从广泛的实验中得出的。当然，你也可以自定义量化表。</p>
<h2 id="JPEG- 亮度量化表"><a href="#JPEG- 亮度量化表" class="headerlink" title="JPEG 亮度量化表"></a>JPEG 亮度量化表</h2><script type="math/tex; mode=display">
\begin{array}{|c|c|c|c|c|c|c|c|c|}
\hline
16 & 11 & 10 & 16 & 24 & 40 & 51 & 61 \cr
\hline
12 & 12 & 14 & 19 & 26 & 58 & 60 & 55 \cr
\hline
14 & 13 & 16 & 24 & 40 & 57 & 69 & 56 \cr
\hline
14 & 17 & 22 & 29 & 51 & 87 & 80 & 62 \cr
\hline
18 & 22 & 37 & 56 & 68 & 109 & 103 & 77 \cr
\hline
24 & 35 & 55 & 64 & 81 & 104 & 113 & 92 \cr
\hline
49 & 64 & 78 & 87 & 103 & 121 & 120 & 101 \cr
\hline
72 & 92 & 95 & 98 & 112 & 100 & 103 & 99 \cr
\hline
\end{array}</script><h2 id="JPEG- 色度量化表"><a href="#JPEG- 色度量化表" class="headerlink" title="JPEG 色度量化表"></a>JPEG 色度量化表</h2><script type="math/tex; mode=display">
\begin{array}{|c|c|c|c|c|c|c|c|c|}
\hline
17 & 18 & 24 & 47 & 99 & 99 & 99 & 99 \cr
\hline
18 & 21 & 26 & 66 & 99 & 99 & 99 & 99 \cr
\hline
24 & 26 & 56 & 99 & 99 & 99 & 99 & 99 \cr
\hline
47 & 66 & 99 & 99 & 99 & 99 & 99 & 99 \cr
\hline
99 & 99 & 99 & 99 & 99 & 99 & 99 & 99 \cr
\hline
99 & 99 & 99 & 99 & 99 & 99 & 99 & 99 \cr
\hline
99 & 99 & 99 & 99 & 99 & 99 & 99 & 99 \cr
\hline
99 & 99 & 99 & 99 & 99 & 99 & 99 & 99 \cr
\hline
\end{array}</script><p>这两张表依据心理视觉阀制作，对 8 bit 的亮度和色度的图象的处理效果不错。</p>
<p><strong>量化表是控制 JPEG 压缩比的关键，这个步骤除掉了一些高频量，损失了很多细节信息。但事实上人眼对高频信号的敏感度远没有低频信号那么敏感。</strong></p>
<p>所以处理后的视觉损失很小，从上面的量化表也可以看出，低频部分采用了相对较短的量化步长，而高频部分则采用了相对较长的量化步长，这样做，也是为了在一定程度上得到相对清晰的图像和更高的压缩率。另一个重要原因是所有的图片的点与点之间会有一个色彩过渡的过程，而大量的图象信息被包含在低频率空间中，经过 DCT 处理后，在高频率部分，将出现大量连续的零。</p>
<h1 id="DPCM- 差分编码（对 -DC- 系数）-amp-RLE- 游程编码（对 -AC- 系数）"><a href="#DPCM- 差分编码（对 -DC- 系数）-amp-RLE- 游程编码（对 -AC- 系数）" class="headerlink" title="DPCM 差分编码（对 DC 系数）&amp; RLE 游程编码（对 AC 系数）"></a>DPCM 差分编码（对 DC 系数）&amp; RLE 游程编码（对 AC 系数）</h1><p>DCT 将一个 $8 \times 8$ 的数组变换成另一个 $8 \times 8$ 的数组。编码信息分两类，一类是每个 $8 \times 8$ 格子 $F(0,0)$ 位置上元素，这是 DC（直流分量），代表 $8 \times 8$ 个子块的平均值，JPEG 中对 $F(0,0)$ 单独编码，由于两个相邻的 $8 \times 8$ 子块的 DC 系数相差很小，所以对它们采用 <strong> 差分编码 DPCM （Difference Pulse Code Modulation）</strong>，可以提高压缩比，也就是说 <strong> 对相邻的子块 DC 系数的差值进行编码。</strong>另一类是 $8 \times 8$ 块的其它 63 个子块，即交流（AC）系数，采用 <strong> 行程编码 RLE （Run-length encode）</strong>。</p>
<p>那么，现在问题来了：这 63 个系数应该按照怎么样的顺序排列？为了保证低频分量先出现，高频分量后出现，以增加行程中连续 “0” 的个数，这 63 个元素采用了 “之” 字型（Zig-Zag）的排列方法，如下图所示：</p>
<p><img src="https://farm1.staticflickr.com/42/31635280065_4468d99a85_o.png" alt=""></p>
<p>举个例子，现在在一个 $ 8\times 8$ 的 block 中：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{cccccccc}
0 & 1 & 5 & 6 & 14 & 15 & 27 & 28 \cr
2 & 4 & 7 & 13 & 16 & 26 & 29 & 42 \cr
3 & 8 & 12 & 17 & 25 & 30 & 41 & 43 \cr
9 & 11 & 18 & 24 & 31 & 40 & 44 & 53 \cr
10 & 19 & 23 & 32 & 39 & 45 & 52 & 54 \cr
20 & 22 & 33 & 38 & 46 & 51 & 55 & 60 \cr
21 & 34 & 37 & 47 & 50 & 56 & 59 & 61 \cr
35 & 36 & 48 & 49 & 57 & 58 & 62 & 63 \cr
\end{array}
\right]</script><p>进行 DCT 余弦离散变换之后，对其进行 Zig-Zag 扫描排序的过程：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{cccccccc}
-26 & -3 & -6 & 2 & 2 & -1 & 0 & 0 \cr
0 & -3 & -4 & 1 & 1 & 0 & 0 & 0 \cr
-3 & 1 & 5 & -1 & -1 & 0 & 0 & 0 \cr
-4 & 1 & 2 & -1 & 0 & 0 & 0 & 0 \cr
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
\end{array}
\right]</script><h2 id="DC- 系数的差分脉冲调制编码"><a href="#DC- 系数的差分脉冲调制编码" class="headerlink" title="DC 系数的差分脉冲调制编码"></a>DC 系数的差分脉冲调制编码</h2><p>$8\times 8$ 的图像块经过 DCT 变换之后得到的 DC 系数有两个特点：</p>
<ol>
<li>系数的数值比较大；</li>
<li>相邻的 $8\times 8$ 图像块的 DC 系数值变化不大；</li>
</ol>
<p>根据这两个特点，DC 系数一般采用差分脉冲调制编码 DPCM（Difference Pulse Code Modulation），即：取同一个图像分量中每个 DC 值与前一个 DC 值的差值来进行编码。对差值进行编码所需要的位数会比对原值进行编码所需要的位数少了很多。假设某一个 $8\times 8$ 图像块的 DC 系数值为 15，而上一个 $8\times 8$ 图像块的 DC 系数为 12，则两者之间的差值为 3。</p>
<h2 id="DC- 系数的中间格式计算"><a href="#DC- 系数的中间格式计算" class="headerlink" title="DC 系数的中间格式计算"></a>DC 系数的中间格式计算</h2><p>JPEG 中为了更进一步节约空间，并不直接保存数据的具体数值，而是将数据按照位数分为 16 组，保存在表里面。这也就是所谓的变长整数编码 VLI。即，第 0 组中保存的编码位数为 0，其编码所代表的数字为 0；第 1 组中保存的编码位数为 1，编码所代表的数字为 -1 或者 1 ……，如下面的表格所示，暂且称其为 VLI 编码表：</p>
<script type="math/tex; mode=display">
\begin{array}{|c|c|c|}
\hline
\text{数值} & \text{组} & \text{实际保存值} \cr
\hline
0 & 0 & - \cr
\hline
-1, 1 & 1 & 0,1 \cr
\hline
-3,-2,2,3 & 2 & 00,01,10,11 \cr
\hline
-7,-6,-5,-4,4,5,6,7 & 3 & 000,001,010,011,100,101,110,111 \cr
\hline
-15,..,-8,8,..,15 & 4 & 0000,..,0111,1000,..,1111 \cr
\hline
-31,..,-16,16,..,31 & 5 & 00000,..,01111,10000,..,11111 - \cr
\hline
-63,..,-32,32,..,63 & 6 & . \cr
\hline
-127,..,-64,64,..,127 & 7 & . \cr
\hline
-255,..,-128,128,..,255 & 8 & . \cr
\hline
-511,..,-256,256,..,511 & 9 & . \cr
\hline
-1023,..,-512,512,..,1023 & 10 & . \cr
\hline
-2047,..,-1024,1024,..,2047 & 11 & . \cr
\hline
-4095,..,-2048,2048,..,4095 & 12 & . \cr
\hline
-8191,..,-4096,4096,..,8191 & 13 & . \cr
\hline
-16383,..,-8192,8192,..,16383 & 14 & . \cr
\hline
-32767,..,-16384,16384,..,32767 & 15 & . \cr
\hline
\end{array}</script><p>前面提到的那个 DC 差值为 3 的数据，通过查找 VLI 可以发现，整数 3 位于 VLI 表格的第 2 组，因此，可以写成 $(2)(3)$ 的形式，该形式，称之为 DC 系数的中间格式。</p>
<h2 id="AC- 系数的行程长度编码（RLC）"><a href="#AC- 系数的行程长度编码（RLC）" class="headerlink" title="AC 系数的行程长度编码（RLC）"></a>AC 系数的行程长度编码（RLC）</h2><p>量化之后的 AC 系数的特点是，63 个系数中含有很多值为 0 的系数。因此，可以采用行程编码 RLC（Run Length Coding）来更进一步降低数据的传输量。利用该编码方式，可以将一个字符串中重复出现的连续字符用两个字节来代替，其中，第一个字节代表重复的次数，第二个字节代表被重复的字符串。例如，$(4, 6)$ 就代表字符串 “6666”。</p>
<p>但是，在 JPEG 编码中，RLC 的含义就同其原有的意义略有不同。在 JPEG 编码中，假设 RLC 编码之后得到了一个 $(M,N)$ 的数据对，其中 M 是两个非零 AC 系数之间连续的 0 的个数（即，行程长度），N 是下一个非零的 AC 系数的值。采用这样的方式进行表示，是因为 AC 系数当中有大量的 0，而采用 Zigzag 扫描也会使得 AC 系数中有很多连续的 0 的存在，如此一来，便非常适合于用 RLC 进行编码。</p>
<p>例如，现有一个字符串，如下所示：</p>
<script type="math/tex; mode=display">
57, 45, 0, 0, 0, 0, 23, 0, -30, -8, 0, 0, 1, 0 0 0 .....</script><p>经过 RLC 之后，将呈现出以下的形式：</p>
<script type="math/tex; mode=display">
 (0, 57) ; (0, 45) ; (4, 23) ; (1, -30) ; (0, -8) ; (2, 1) ; (0, 0)</script><p><strong>注意，如果 AC 系数之间连续 0 的个数超过 16，则用一个扩展字节 (15, 0) 来表示 16 连续的 0。</strong></p>
<h2 id="AC 系数的中间格式"><a href="#AC 系数的中间格式" class="headerlink" title="AC 系数的中间格式"></a>AC 系数的中间格式 </h2><p> 根据前面提到的 VLI 表格，对于前面的字符串：</p>
<script type="math/tex; mode=display">
 (0, 57) ; (0, 45) ; (4, 23) ; (1, -30) ; (0, -8) ; (2, 1) ; (0, 0)</script><p>只处理每对数右边的那个数据，对其进行 VLI 编码：查找上面的 VLI 编码表格，可以发现，57 在第 6 组当中，因此，可以将其写成 $(0,6),57$ 的形式，该形式，称之为 AC 系数的中间格式。<br>同样的 $(0, 45)$ 的中间格式为：$(0, 6), 45$；$(1, -30)$ 的中间格式为：$(1, 5), -30$；</p>
<h1 id="熵编码：Huffman- 编码"><a href="# 熵编码：Huffman- 编码" class="headerlink" title="熵编码：Huffman 编码"></a>熵编码：Huffman 编码 </h1><p> 在得到 DC 系数的中间格式和 AC 系数的中间格式之后，为进一步压缩图象数据，有必要对两者进行熵编码。<strong>JPEG 标准具体规定了两种熵编码方式：Huffman 编码和算术编码。</strong>JPEG 基本系统规定采用 Huffman 编码（因为不存在专利问题），但 JPEG 标准并没有限制 JPEG 算法必须用 Huffman 编码方式或者算术编码方式。 </p>
<h2 id="Huffman- 编码"><a href="#Huffman- 编码" class="headerlink" title="Huffman 编码"></a>Huffman 编码 </h2><p> 对出现概率大的字符分配字符长度较短的二进制编码，对出现概率小的字符分配字符长度较长的二进制编码，从而使得字符的平均编码长度最短。Huffman 编码的原理请参考数据结构中的 Huffman 树或者最优二叉树。</p>
<p><strong>Huffman 编码时 DC 系数与 AC 系数分别采用不同的 Huffman 编码表，对于亮度和色度也采用不同的 Huffman 编码表。</strong>因此，需要 4 张 Huffman 编码表才能完成熵编码的工作。具体的 Huffman 编码采用查表的方式来高效地完成。然而，在 JPEG 标准中没有定义缺省的 Huffman 表，用户可以根据实际应用自由选择，也可以使用 JPEG 标准推荐的 Huffman 表。或者预先定义一个通用的 Huffman 表，也可以针对一副特定的图像，在压缩编码前通过搜集其统计特征来计算 Huffman 表的值。</p>
<p>下面我们举例来说明 $8\times 8$ 图像子块经过 DCT 及量化之后的处理过程：<br>假设一个图像块经过量化以后得到以下的系数矩阵：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{cccccccc}
15 & 0 & -1 & 0 & 0 & 0 & 0 & 0 \cr
-2 & -1 & 0 & 0 & 0 & 0 & 0 & 0 \cr
-1 & -1 & 0 & 0 & 0 & 0 & 0 & 0 \cr
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
\end{array}
\right]</script><p>显然，DC 系数为 15，假设前一个 $8\times 8$ 的图像块的 DC 系数量化值为 12，则当前 DC 系统同上一个 DC 系数之间的差值为 3，通过查找 VLI 编码表，可以得到 DC 系数的中间格式为 $(2)(3)$，这里的 2 代表后面的数字 $(3)$ 的编码长度为 2 位；之后，通过 Zigzag 扫描之后，遇到第一个非 0 的 AC 系数为 -2，遇到 0 的个数为 1，AC 系数经过 RLC 编码后可表示为 $(1, -2)$，通过查找 VLI 表发现，-2 在第 2 组，因此，该 AC 系数的中间格式为 $(1, 2) , -2$；其余的点类似，可以求得这个 $ 8\times 8$ 子块熵编码的中间格式为：</p>
<script type="math/tex; mode=display">
\begin{array}{c|c}
DC & (2)(3) \cr
\hline
AC & (1,2)(-2),(0,1)(-1),(0,1)(-1),(0,1)(-1),(2,1)(-1),(EOB)(0,0)
\end{array}</script><ul>
<li>对于 DC 系数的中间格式 $(2)(3)$ 而言，数字 2 查 DC 亮度 Huffman 表得到 011，数字 3 通过查找 VLI 编码表得到其被编码为 11；</li>
<li>对于 AC 系数的中间格式 $(1, 2)(-2)$ 而言，$(1, 2)$ 查 AC 亮度 Huffman 表得到 11011，-2 通过查找 VLI 编码表得到其被编码为 01；</li>
<li>对于 AC 系数的中间格式 $(0, 1)(-1)$ 而言，$(0, 1)$ 查 AC 亮度 Huffman 表得到 00，数字 -1 通过查找 VLI 编码表得到其被编码为 0；</li>
<li>对于 AC 系数的中间格式 $(2, 1)(1)$ 而言，$(2, 1)$ 查 AC 亮度 Huffman 表得到 11100，数字 -1 通过查找 VLI 编码表得到其被编码为 0；</li>
<li>对于 AC 系数的中间格式 $(0, 0)$ 而言，查 AC 亮度 Huffman 表得到 1010；</li>
</ul>
<p>因此，最后这个 $8\times 8$ 子块亮度信息压缩后的数据流为 01111，1101101，000，000，000，111000，1010。总共 31 比特，其压缩比是 $ \frac{64 \times 8}{31} =16.5$，大约每个像素用半个比特。</p>
]]></content>
    
    <summary type="html">
    
      本文介绍了关于信息隐藏中的图像 JPEG 压缩标准方法，JPEG 是一种针对照片视频而广泛使用的一种「有损压缩」标准方法。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
      <category term="Computer Vision" scheme="http://randolph.pro/categories/Machine-Learning/Computer-Vision/"/>
    
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
      <category term="Computer Vision" scheme="http://randolph.pro/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>♞「Programming Collective Intelligence」 Chapter 3</title>
    <link href="http://randolph.pro/2016/03/19/%E2%99%9E%E3%80%8CProgramming%20Collective%20Intelligence%E3%80%8D%20Chapter%203/"/>
    <id>http://randolph.pro/2016/03/19/♞「Programming Collective Intelligence」 Chapter 3/</id>
    <published>2016-03-18T16:00:00.000Z</published>
    <updated>2017-08-06T13:18:18.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4306/36216231065_8a7e508f9e_o.jpg" alt=""></p>
<p>有关该书的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/Book-「Programming-Collective-Intelligence」/">Book:「Programming Collective Intelligence」</a></p>
<h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><ul>
<li><strong>皮尔逊相关系数</strong></li>
<li><strong>分级聚类</strong></li>
<li><strong>K- 均值聚类</strong></li>
<li><strong>针对偏好的聚类</strong></li>
<li><strong>多维缩放</strong></li>
<li><strong>EM 聚类</strong></li>
</ul>
<hr>
<h1 id="Key"><a href="#Key" class="headerlink" title="Key:"></a>Key:</h1><h2 id="皮尔逊相关系数："><a href="# 皮尔逊相关系数：" class="headerlink" title="皮尔逊相关系数："></a>皮尔逊相关系数：</h2><p>定义：两个变量之间的皮尔逊相关系数定义为 <strong>（两个变量之间的协方差）</strong> 和<strong>（两个变量标准差的积）</strong>的商。</p>
<script type="math/tex; mode=display">
\rho_{x,y} = \frac{cov(X,Y)}{\sigma_{X}\sigma_{Y}}=\frac{E[(X-E(X))\cdot (Y-E(Y))]}{\sigma_{X}\sigma_{Y}}=\frac{E(XY)-E(X)E(Y)}{\sigma_{X}\sigma_{Y}}</script><p>公式进一步推导：</p>
<script type="math/tex; mode=display">
\rho_{x,y} = \frac{\sum XY - \frac{\sum X \sum Y}{N}}{\sqrt{(\sum X-\frac{(\sum X)^2}{N})(\sum Y-\frac{(\sum Y)^2}{N})}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> math <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">pearson</span><span class="params">(v1, v2)</span>:</span></div><div class="line">    sum1 = sum(v1)</div><div class="line">    sum2 = sum(v2)</div><div class="line"></div><div class="line">    <span class="comment"># 求平方和</span></div><div class="line">    sum1_Sq = sum([pow(v, <span class="number">2</span>) <span class="keyword">for</span> v <span class="keyword">in</span> v1])</div><div class="line">    sum2_Sq = sum([pow(v, <span class="number">2</span>) <span class="keyword">for</span> v <span class="keyword">in</span> v2])</div><div class="line"></div><div class="line">    <span class="comment"># 求乘积之和</span></div><div class="line">    pSum = sum([v1[i]*v2[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(v1))])</div><div class="line"></div><div class="line">    num = pSum-(sum1*sum2/len(v1))</div><div class="line">    den = sqrt((sum1_Sq-pow(sum1, <span class="number">2</span>)/len(v1))*(sum2_Sq-pow(sum2, <span class="number">2</span>)/len(v2)))</div><div class="line"></div><div class="line">    <span class="keyword">if</span> den == <span class="number">0</span>:</div><div class="line">        <span class="keyword">return</span> <span class="number">0</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1.0</span> - num/den</div></pre></td></tr></table></figure>
<ul>
<li>皮尔逊相关度的计算结果在两者完全匹配的情况下为 1.0，而在两者毫无关系的情况下则为 0.0。此段代码的最后一行，返回的是以 1.0 减去皮尔逊相关度之后的结果，这样的目的是为了让相似度越大的两个元素之间的距离变得更小（做了一个小处理）。</li>
</ul>
<hr>
<h2 id="分级聚类："><a href="# 分级聚类：" class="headerlink" title="分级聚类："></a>分级聚类：</h2><p>分级聚类通过连续不断地将最为相似的群组两两合并，来构造出一个群组的层级结构。其中的每个群组都是从单一元素开始的。在每次迭代的过程，分级聚类算法会计算每两个群组间的距离，并将距离最近的两个群组合并成一个新的群组。这一过程会一直重复下去，直到只剩一个群组为止。<br><img src="https://farm1.staticflickr.com/591/30827330683_718ddf881c_o.png" alt=""></p>
<p>直接使用 <strong>blogdata.txt</strong> 博客数据集，不要用书上前面的代码来下载一系列博客的订阅源，从而构造这样一个数据集，因为书上的代码无法使用，一些库的模块功能已经过时，运行会出错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">readfile</span><span class="params">(filename)</span>:</span></div><div class="line">    lines = [line <span class="keyword">for</span> line <span class="keyword">in</span> file(filename)]</div><div class="line"></div><div class="line">    colnames = lines[<span class="number">0</span>].strip().split(<span class="string">'\t'</span>)[<span class="number">1</span>:]</div><div class="line"></div><div class="line">    rownames = []</div><div class="line">    data = []</div><div class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines[<span class="number">1</span>:]:</div><div class="line">        p = line.strip().split(<span class="string">'\t'</span>)</div><div class="line">        rownames.append(p[<span class="number">0</span>])</div><div class="line">        data.append([float(x) <span class="keyword">for</span> x <span class="keyword">in</span> p[<span class="number">1</span>:]])</div><div class="line"></div><div class="line">    <span class="keyword">return</span> colnames, rownames, data</div></pre></td></tr></table></figure>
<ul>
<li>colnames = [‘china’, ‘kids’, …, ‘book’]为所有博客中出现的单词的单词表。</li>
<li>rownames = [‘$blog_1$’, ‘$blog_2$’, …, ‘$blog_n$’]为所有博客的名字表。</li>
<li>data = [[0,0, 1.0, 3.0, …],[2.0, 3.0, 0.0, …], …,[4.0, 5.0, 3.0, …]]为各个博客在单词表中各个单词出现的次数的表。</li>
</ul>
<p>很容易想到用树结构来表示聚类这样的结构关系。分级聚类算法中的每一个聚类，可以是树中的枝节点，也可以是与数据集中实际数据行相对应的叶节点。没一个聚类还包含了指示器位置的信息，这一信息可以是来自叶节点的行数据，也可以是来自枝节点的经合并后的数据。我们可以新建一个 <strong><code>bicluster</code></strong> 类，将所有这些属性存放其中，并以此来描述这棵层级树。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">bicluster</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vec, left = None, right = None, distance = <span class="number">0.0</span>, id = None)</span>:</span></div><div class="line">        self.left = left</div><div class="line">        self.right = right</div><div class="line">        self.vec = vec</div><div class="line">        self.id = id</div><div class="line">        self.distance = distance</div></pre></td></tr></table></figure>
<p>分级聚类算法以一组对应于原始数据项的聚类开始。函数的主循环部分会尝试每一组可能的配对并计算它们的相关度，以此来找出最佳配对。最佳配对的两个聚类会被合并成一个新的聚类。新生成的聚类中所包含的数据，等于将两个旧聚类的数据求均值之后得到的结果。这一过程会一直重复下去，知道只剩下一个聚类为止。由于整个计算过程可能会非常耗时，我们需要将每个配对的相关度计算结果保存起来以便优化执行速度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">hcluster</span><span class="params">(rows, distance = pearson)</span>:</span></div><div class="line">    distances = &#123;&#125;</div><div class="line">    currentclustid = <span class="number">-1</span></div><div class="line">    <span class="comment"># 最开始的聚类就是数据集中的行</span></div><div class="line">    clust = [bicluster(rows[i], id = i) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rows))]</div><div class="line"></div><div class="line">    <span class="keyword">while</span> len(clust) &gt; <span class="number">1</span>:</div><div class="line">        lowestpair = (<span class="number">0</span>, <span class="number">1</span>)</div><div class="line">        closest = distance(clust[<span class="number">0</span>].vec, clust[<span class="number">1</span>].vec)</div><div class="line">        <span class="comment"># 遍历每一个配对，寻找最小距离</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(clust)):</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, len(clust)):</div><div class="line">                <span class="comment"># 用 distances 来缓存距离的计算值</span></div><div class="line">                <span class="keyword">if</span> (clust[i].id, clust[j].id) <span class="keyword">not</span> <span class="keyword">in</span> distances:</div><div class="line">                    distances[(clust[i].id, clust[j].id)] = distance(clust[i].vec, clust[j].vec)</div><div class="line">                d = distances[(clust[i].id, clust[j].id)]</div><div class="line">                <span class="keyword">if</span> d &lt; closest:</div><div class="line">                    closest = d</div><div class="line">                    lowestpair = (i, j)</div><div class="line">        <span class="comment"># 计算两个聚类的平均值</span></div><div class="line">        mergevec = [(clust[lowestpair[<span class="number">0</span>]].vec[i] + clust[lowestpair[<span class="number">1</span>]].vec[i])/<span class="number">2.0</span></div><div class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(clust[<span class="number">0</span>].vec))]</div><div class="line">        <span class="comment"># 建立新的聚类，聚类后的新聚类的 vec 更新为两个原先聚类的均值</span></div><div class="line">        newcluster = bicluster(mergevec, left = clust[lowestpair[<span class="number">0</span>]],</div><div class="line">                               right = clust[lowestpair[<span class="number">1</span>]],</div><div class="line">                               distance = closest, id = currentclustid)</div><div class="line">        <span class="comment"># 去除原先的两个初始聚类，将新聚类添加至 clust，然后递归重复过程</span></div><div class="line">        currentclustid -= <span class="number">1</span></div><div class="line">        <span class="keyword">del</span> clust[lowestpair[<span class="number">1</span>]]</div><div class="line">        <span class="keyword">del</span> clust[lowestpair[<span class="number">0</span>]]</div><div class="line">        clust.append(newcluster)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> clust[<span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>其中 <strong><code>distance = pearson</code></strong> 表示的是采用皮尔逊相关系数来度量变量之间的距离，也可以构造其他距离度量函数（比如曼哈顿距离或者欧式距离）。</p>
<p>接下来就是直观地展现最终的聚类结果，就是绘制树状图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageDraw</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getheight</span><span class="params">(clust)</span>:</span></div><div class="line">    <span class="keyword">if</span> clust.left == <span class="keyword">None</span> <span class="keyword">and</span> clust.right == <span class="keyword">None</span>:  <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">return</span>  getheight(clust.left) + getheight(clust.right)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getdepth</span><span class="params">(clust)</span>:</span></div><div class="line">    <span class="keyword">if</span> clust.left == <span class="keyword">None</span> <span class="keyword">and</span> clust.right == <span class="keyword">None</span>:  <span class="keyword">return</span> <span class="number">0</span></div><div class="line">    <span class="keyword">return</span> max(getdepth(clust.left), getdepth(clust.right)) + clust.distance</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">drawdendrogram</span><span class="params">(clust, labels, jpeg = <span class="string">'clusters.jpg'</span>)</span>:</span></div><div class="line">    h = getheight(clust)*<span class="number">20</span></div><div class="line">    w = <span class="number">1200</span></div><div class="line">    depth = getdepth(clust)</div><div class="line"></div><div class="line">    scaling = float(w - <span class="number">150</span>)/depth</div><div class="line"></div><div class="line">    img = Image.new(<span class="string">'RGB'</span>, (w, h), (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>))</div><div class="line">    draw = ImageDraw.Draw(img)</div><div class="line"></div><div class="line">    draw.line((<span class="number">0</span>, h/<span class="number">2</span>, <span class="number">10</span>, h/<span class="number">2</span>), fill = (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>))</div><div class="line"></div><div class="line">    drawnode(draw, clust, <span class="number">10</span>, (h/<span class="number">2</span>), scaling, labels)</div><div class="line">    img.save(jpeg, <span class="string">'JPEG'</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">drawnode</span><span class="params">(draw, clust, x, y, scaling, labels)</span>:</span></div><div class="line">    <span class="keyword">if</span> clust.id &lt; <span class="number">0</span>:</div><div class="line">        h1 = getheight(clust.left)*<span class="number">20</span></div><div class="line">        h2 = getheight(clust.right)*<span class="number">20</span></div><div class="line">        top = y - (h1 + h2)/<span class="number">2</span></div><div class="line">        bottom = y + (h1 + h2)/<span class="number">2</span></div><div class="line"></div><div class="line">        l1 = clust.distance * scaling</div><div class="line"></div><div class="line">        draw.line((x, top + h1/<span class="number">2</span>, x, bottom - h2/<span class="number">2</span>), fill = (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>))</div><div class="line">        draw.line((x, top + h1/<span class="number">2</span>, x + l1, top + h1/<span class="number">2</span>), fill = (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>))</div><div class="line">        draw.line((x, bottom - h2/<span class="number">2</span>, x + l1, bottom - h2/<span class="number">2</span>), fill = (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>))</div><div class="line"></div><div class="line">        drawnode(draw, clust.left, x + l1, top + h1/<span class="number">2</span>, scaling, labels)</div><div class="line">        drawnode(draw, clust.right, x + l1, bottom - h2/<span class="number">2</span>, scaling, labels)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        draw.text((x + <span class="number">5</span>, y - <span class="number">7</span>), labels[clust.id], (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>))</div></pre></td></tr></table></figure>
<p>输入以下命令，就可以看到最终的博客聚类情况的树状图：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import clusters</div><div class="line">&gt;&gt;&gt; words, blognames, data = clusters.readfile(<span class="string">'blogdata.txt'</span>)</div><div class="line">&gt;&gt;&gt; clust = clusters.hcluster(data)</div><div class="line">&gt;&gt;&gt; clusters.drawdendrogram(clust, blogname, jpeg = <span class="string">'blogclust'</span>.jpg)</div></pre></td></tr></table></figure></p>
<hr>
<h2 id="K- 均值聚类："><a href="#K- 均值聚类：" class="headerlink" title="$K$- 均值聚类："></a>$K$- 均值聚类：</h2><p>分级聚类有个缺点就是，我们必须计算每两个配对项之间的关系，并且在合并项之后，这些关系还需要重新计算，所以在处理很大规模的数据集时，分级聚类算法的运行速度会非常缓慢。</p>
<p>而 $K$- 均值聚类，完全不同于分级聚类，因为我们会预先告诉算法希望生成的聚类数量，然后算法会根据数据的结构状况来确定聚类的大小。</p>
<p>$K$- 均值聚类算法首先会随机确定 $k$ 个中心位置（位于空间中代表聚类中心的点），然后将各个数据项分配给最临近的中心点。待分配完成之后，聚类中心就会移到分配给该聚类的所有节点的平均位置处，然后整个分配过程重新开始。这一过程会一直重复下去，直到分配过程不再产生变化为止。</p>
<p><img src="https://farm1.staticflickr.com/380/31521742461_dcb281b151_o.png" alt=""></p>
<p>实现 $K$- 均值聚类算法的函数与分级聚类算法的一样，接受相同的数据行作为输入，此外它还接受一个调用者期望返回的聚类数（$k$）作为参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">kcluster</span><span class="params">(rows, distance = pearson, k = <span class="number">4</span>)</span>:</span></div><div class="line">    <span class="comment"># 确定每个点的最小值和最大值</span></div><div class="line">    ranges = [(min([row[i] <span class="keyword">for</span> row <span class="keyword">in</span> rows]), max([row[i] <span class="keyword">for</span> row <span class="keyword">in</span> rows]))</div><div class="line">              <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rows[<span class="number">0</span>]))]</div><div class="line">    </div><div class="line">    <span class="comment"># 随机创建 k 个中心点</span></div><div class="line">    clusters = [[random.random()*(ranges[i][<span class="number">1</span>] - ranges[i][<span class="number">0</span>]) + ranges[i][<span class="number">0</span>]</div><div class="line">                 <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rows[<span class="number">0</span>]))] <span class="keyword">for</span> j <span class="keyword">in</span> range(k)]</div><div class="line">    lastmatches = <span class="keyword">None</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</div><div class="line">        print(<span class="string">'Iteration %d'</span> % t)</div><div class="line">        bestmatches = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(k)]</div><div class="line"></div><div class="line">        <span class="comment"># 在每一行中寻找距离最近的中心点</span></div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(rows)):</div><div class="line">            row = rows[j]</div><div class="line">            bestmatch = <span class="number">0</span></div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</div><div class="line">                d = distance(clusters[i], row)</div><div class="line">                <span class="keyword">if</span> d &lt; distance(clusters[bestmatch], row):</div><div class="line">                    bestmatch = i</div><div class="line">            bestmatches[bestmatch].append(j)</div><div class="line"></div><div class="line">        <span class="comment"># 如果结果与上一次相同，则整个过程结束</span></div><div class="line">        <span class="keyword">if</span> bestmatches == lastmatches:  <span class="keyword">break</span></div><div class="line">        lastmatches = bestmatches</div><div class="line"> </div><div class="line">        <span class="comment"># 把中心点移到其所有成员的平均位置处</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</div><div class="line">            avgs = [<span class="number">0.0</span>]*len(rows[<span class="number">0</span>])</div><div class="line">            <span class="keyword">if</span> len(bestmatches[i]) &gt; <span class="number">0</span>:</div><div class="line">                <span class="keyword">for</span> rowid <span class="keyword">in</span> bestmatches[i]:</div><div class="line">                    <span class="keyword">for</span> m <span class="keyword">in</span> range(len(rows[rowid])):</div><div class="line">                        avgs[m] += rows[rowid][m]</div><div class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(len(avgs)):</div><div class="line">                    avgs[j] /= len(bestmatches[i])</div><div class="line">                clusters[i] = avgs</div><div class="line"></div><div class="line">    <span class="keyword">return</span> bestmatches</div></pre></td></tr></table></figure>
<p>上述代码在每个变量的值域范围内随机构造了一组聚类。当每次迭代进行的时候，算法会将每一行数据分配给某个中心点，然后再将中心点的数据更新为分配给它的所有项的平均位置。当分配情况与前一次相同的时候，迭代过程就结束了，同时算法会返回 k 组序列，其中每个序列代表一个聚类。与分级聚类相比，该算法为产生最终结果所须迭代的次数是非常少的。</p>
<p><strong>由于函数选用随机的中心点作为开始，所以返回结果的顺序几乎总是不同的。根据中心点初始位置的不同，最终聚类中所包含的内容也可能会有所不同。</strong></p>
<p>我们可以针对博客数据集试验一下该函数。算法的执行速度应该会比分级聚类更快一些：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> clusters</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>words, blognames, data = clusters.readfile(<span class="string">'blogdata.txt'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>kclust = clusters.kcluster(data, k = <span class="number">10</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[blognames[r] <span class="keyword">for</span> r <span class="keyword">in</span> kclust[<span class="number">0</span>]]</div><div class="line">[<span class="string">'Online Marketing Report'</span>, <span class="string">"Sifry's Alerts"</span>, <span class="string">'Treehugger'</span>, <span class="string">'Oilman'</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[blognames[r] <span class="keyword">for</span> r <span class="keyword">in</span> kclust[<span class="number">1</span>]]</div><div class="line">[<span class="string">'Mashable!'</span>, <span class="string">'Signum sine tinnitu--by Guy Kawasaki'</span>, <span class="string">'TechCrunch'</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[blognames[r] <span class="keyword">for</span> r <span class="keyword">in</span> kclust[<span class="number">2</span>]]</div><div class="line">[<span class="string">"The Superficial - Because You're Ugly"</span>, <span class="string">'Wonkette'</span>, <span class="string">'Eschaton'</span>, <span class="string">'we make money not art'</span>, <span class="string">'Joho the Blog'</span>, <span class="string">"Neil Gaiman's Journal"</span>, <span class="string">'Signal vs. Noise'</span>, <span class="string">'lifehack.org'</span>, <span class="string">'Kotaku'</span>, <span class="string">'Daily Kos'</span>, <span class="string">'Deadspin'</span>, <span class="string">'Go Fug Yourself'</span>, <span class="string">'Gizmodo'</span>, <span class="string">'Gothamist'</span>, <span class="string">'The Viral Garden'</span>, <span class="string">'SpikedHumor'</span>, <span class="string">'flagrantdisregard'</span>, <span class="string">'Techdirt'</span>, <span class="string">'Schneier on Security'</span>, <span class="string">'Scobleizer - Tech Geek Blogger'</span>, <span class="string">'Little Green Footballs'</span>, <span class="string">"Dave Shea's mezzoblue"</span>, <span class="string">'kottke.org'</span>, <span class="string">'MetaFilter'</span>, <span class="string">'ongoing'</span>, <span class="string">'Instapundit.com'</span>, <span class="string">"Joi Ito's Web"</span>, <span class="string">'Joel on Software'</span>, <span class="string">'PerezHilton.com'</span>, <span class="string">'Derek Powazek'</span>, <span class="string">"Jeremy Zawodny's blog"</span>, <span class="string">'plasticbag.org'</span>, <span class="string">'Gawker'</span>, <span class="string">'WWdN: In Exile'</span>, <span class="string">"Seth's Blog"</span>, <span class="string">'The Huffington Post | Raw Feed'</span>]</div></pre></td></tr></table></figure></p>
<p>现在，kclust 中应该包含了一组代表聚类的 ID 序列。<br>kclust 应该是一个这样的东西：</p>
<script type="math/tex; mode=display">
[[rowid_1,rowid_2,…,rowid_n]_{(聚类_1)} ,[rowid_1,rowid_2,...,rowid_n]_{(聚类_2)} ,...,[rowid_1,rowid_2,...,rowid_n]_{(聚类_k)}]</script><p>$K$- 均值聚类的最佳特质就是各簇在本质上呈紧致的球状分布，且总会收敛到某个解：<br><img src="https://farm1.staticflickr.com/732/31600482456_67bececb29_o.png" alt=""></p>
<p>$K$- 均值聚类中的距离度量方式除了上述的皮尔逊相关系数，针对具体任务（不同的数据集）可以采用不同的距离度量方式。<strong>但是 $K$- 均值聚类算法的缺陷是各簇之间必须存在一个“硬”边界，这意味着每个数据点只能属于一个簇，无法跨越两个簇之间的界限。此外，$K$- 均值聚类算法适合于呈球状分布的数据，因为大多数情况下人们采用的都是欧式距离。在像上述图中这样的数据分布（位于中间的那些点实际上可以属于两个簇的任意一个），这些缺陷非常明显。</strong></p>
<hr>
<h2 id="针对偏好的聚类："><a href="# 针对偏好的聚类：" class="headerlink" title="针对偏好的聚类："></a>针对偏好的聚类：</h2><p>直接使用 <strong>zebo.txt</strong> 数据集，虽然书上的网站无法访问导致我们无法构建数据集。但是其中使用 BeautifulSoup 这一函数库，以及其中对于网页内容的处理，我们可以学习一下代码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> BeautifulSoup <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> urllib2</div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fuck</span><span class="params">()</span>:</span></div><div class="line">    chare = re.compile(<span class="string">r'[!-\.&amp;]'</span>)</div><div class="line">    itemowners = &#123;&#125;</div><div class="line"></div><div class="line">    <span class="comment"># 想要去除的单词</span></div><div class="line">    dropwords = [<span class="string">'a'</span>, <span class="string">'new'</span>, <span class="string">'some'</span>, <span class="string">'more'</span>, <span class="string">'my'</span>, <span class="string">'own'</span>, <span class="string">'the'</span>, <span class="string">'many'</span>, <span class="string">'other'</span>, <span class="string">'another'</span>]</div><div class="line"></div><div class="line">    currentuser = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">51</span>):</div><div class="line">        <span class="comment"># 搜索想要的对应 URL</span></div><div class="line">        c = urllib2.urlopen(</div><div class="line">            <span class="string">'http://member.zebo.com/Main?event_key=USERSEARCH&amp;wiowiw=wiw&amp;keyword=car&amp;page=%d'</span></div><div class="line">            % (i))</div><div class="line">        soup = BeautifulSoup(c.read())</div><div class="line">        <span class="keyword">for</span> td <span class="keyword">in</span> soup(<span class="string">'td'</span>):</div><div class="line">            <span class="comment"># 寻找带有 bgverdanasmall 类的表格单元格</span></div><div class="line">            <span class="keyword">if</span> (<span class="string">'class'</span> <span class="keyword">in</span> dict(td.attrs) <span class="keyword">and</span> td[<span class="string">'class'</span>] == <span class="string">'bgverdanasmall'</span>):</div><div class="line">                items = [re.sub(chare, <span class="string">''</span>, str(a.contents[<span class="number">0</span>]).lower()).strip() <span class="keyword">for</span> a <span class="keyword">in</span> td(<span class="string">'a'</span>)]</div><div class="line">                <span class="keyword">for</span> item <span class="keyword">in</span> items:</div><div class="line">                    <span class="comment"># 去除多余的单词</span></div><div class="line">                    txt = <span class="string">' '</span>.join([t <span class="keyword">for</span> t <span class="keyword">in</span> item.split(<span class="string">' '</span>) <span class="keyword">if</span> t <span class="keyword">not</span> <span class="keyword">in</span> dropwords])</div><div class="line">                    <span class="keyword">if</span> len(txt) &lt; <span class="number">2</span>: <span class="keyword">continue</span></div><div class="line">                    itemowners.setdefault(txt, &#123;&#125;)</div><div class="line">                    itemowners[txt][currentuser] = <span class="number">1</span></div><div class="line">                currentuser += <span class="number">1</span></div><div class="line"></div><div class="line">    out = file(<span class="string">'zebo.txt'</span>, <span class="string">'w'</span>)</div><div class="line">    out.write(<span class="string">'Item'</span>)</div><div class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> range(<span class="number">0</span>, currentuser): out.write(<span class="string">'\tU%d'</span> % user)</div><div class="line">    out.write(<span class="string">'\n'</span>)</div><div class="line">    <span class="keyword">for</span> item, owners <span class="keyword">in</span> itemowners.items():</div><div class="line">        <span class="comment"># 寻找超过 10 个人都希望拥有的物品</span></div><div class="line">        <span class="keyword">if</span> len(owners) &gt; <span class="number">10</span>:</div><div class="line">            out.write(item)</div><div class="line">            <span class="keyword">for</span> user <span class="keyword">in</span> range(<span class="number">0</span>, currentuser):</div><div class="line">                <span class="keyword">if</span> user <span class="keyword">in</span> owners:</div><div class="line">                    out.write(<span class="string">'\t1'</span>)</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    out.write(<span class="string">'\t0'</span>)</div><div class="line">            out.write(<span class="string">'\n'</span>)</div><div class="line">            </div><div class="line">fuck()</div></pre></td></tr></table></figure></p>
<ul>
<li>其中 <strong><code>range(1, 51)</code></strong> 表示我们会处理其中的前五十个页面，当然我们也可以自定义。</li>
<li>由于所有的物品的文字都是随意输入的，所以需要进行大量的处理工作，其中包括去除像 <strong>dropwords</strong> 中“a”、“some”、“new”等这样的单词，去除 <strong>chare</strong> 标点符号，以及将所有文本转换成小写。</li>
</ul>
<p>在 <strong>zebo.txt</strong> 文件当中，如果一个人希望拥有某件物品，那么我们将其标记为 1，否则就标记为 0。皮尔逊相关度很适合于博客数据集，改数据集中所包含的是单词的实际统计值。而在此处，数据集只有 1 和 0 两种取值，分别代表着有或无。并且，假如我们对同时希望拥有两件物品的人在物品方面互有重叠的情况进行度量，那或许是一件更有意义上的事情。</p>
<p>为此，我们采用一种被称为 <strong>Tanimoto 系数</strong> 的度量方法，它代表的是交集（只包含那些在两个集合中都出现的项）与并集（包含所有出现于任一集合中的项）的比率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanimoto</span><span class="params">(v1, v2)</span>:</span></div><div class="line">    c1, c2, shr = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(v1)):</div><div class="line">        <span class="keyword">if</span> v1[i] != <span class="number">0</span>:  c1 += <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> v2[i] != <span class="number">0</span>:  c2 += <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> v1[i] != <span class="number">0</span> <span class="keyword">and</span> v2[i] != <span class="number">0</span>:   shr += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> <span class="number">1.0</span> - (float(shr)/(c1 + c2 - shr))</div></pre></td></tr></table></figure>
<p>上述代码将返回一个介于 1.0 和 0.0 之间的值。其中 1.0 代表不存在同时喜欢两件物品的人，而 0.0 则代表所有人都同时喜欢两个向量中的物品。</p>
<p>因为数据的格式与先前所用的相同，所以我们可以利用同样的函数来生成和绘制分级聚类。利用上面的函数并相应传入两个向量，我们很容易就可以实现聚类的功能。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import clusters</div><div class="line">&gt;&gt;&gt; users, items, data = clusters.readfile(<span class="string">'zebo.txt'</span>)</div><div class="line">&gt;&gt;&gt; clust = clusters.hcluster(data)</div><div class="line">&gt;&gt;&gt; clust = clusters.hcluster(data, distance = clusters.tanimoto)</div><div class="line">&gt;&gt;&gt; clusters.drawdendrogram(clust, items, jpeg = <span class="string">'itemsclusters.jpg'</span>)</div></pre></td></tr></table></figure></p>
<hr>
<h2 id="多维缩放："><a href="# 多维缩放：" class="headerlink" title="多维缩放："></a>多维缩放：</h2><p>由于在大多数真是生活的例子中，我们所要聚类的内容都不只包含两个数值，所以我们不可能按照前面的方法来采集数据并以二维的形式将其绘制出来。但是为了要弄明白物品之间的关系，将它们绘制在一个二维的平面上，两两之间的距离远近表达的是两者之间的相似程度。而 <strong> 多维缩放 </strong> 目的就是根据每对数据项之间的相似情况，将其表现在一个二维平面上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">scaledown</span><span class="params">(data, distance = pearson, rate = <span class="number">0.01</span>)</span> :</span></div><div class="line">    n = len(data)</div><div class="line"></div><div class="line">    <span class="comment"># 每一对数据项之间的真实距离</span></div><div class="line">    realdist = [[distance(data[i], data[j]) <span class="keyword">for</span> j <span class="keyword">in</span> range(n)]</div><div class="line">                 <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, n)]</div><div class="line"></div><div class="line">    <span class="comment"># 随机初始化节点在二维空间中的初始位置</span></div><div class="line">    loc = [[random.random(), random.random()] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</div><div class="line">    fakedist = [[<span class="number">0.0</span> <span class="keyword">for</span> j <span class="keyword">in</span> range(n)] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</div><div class="line"></div><div class="line">    lasterror = <span class="keyword">None</span></div><div class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">1000</span>) :</div><div class="line">        <span class="comment"># 寻找投影后的距离</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</div><div class="line">                fakedist[i][j] = sqrt(sum([pow(loc[i][x] - loc[j][x], <span class="number">2</span>)</div><div class="line">                                                <span class="keyword">for</span> x <span class="keyword">in</span> range(len(loc[i]))]))</div><div class="line"></div><div class="line">        <span class="comment"># 移动节点</span></div><div class="line">        grad = [[<span class="number">0.0</span>, <span class="number">0.0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</div><div class="line"></div><div class="line">        totalerror = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(n):</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</div><div class="line">                <span class="keyword">if</span> j == k: <span class="keyword">continue</span></div><div class="line">                <span class="comment"># 误差值等于目标距离与当前距离之间差值的百分比</span></div><div class="line">                errorterm = (fakedist[j][k] - realdist[j][k]) / realdist[j][k]</div><div class="line"></div><div class="line">                <span class="comment"># 每个节点都需要根据误差的多少，按照比例移离或者移向其他节点</span></div><div class="line">                <span class="comment"># point in proportion to how much error it has</span></div><div class="line">                grad[k][<span class="number">0</span>] += ((loc[k][<span class="number">0</span>] - loc[j][<span class="number">0</span>]) / fakedist[j][k]) * errorterm</div><div class="line">                grad[k][<span class="number">1</span>] += ((loc[k][<span class="number">1</span>] - loc[j][<span class="number">1</span>]) / fakedist[j][k]) * errorterm</div><div class="line"></div><div class="line">                <span class="comment"># 记录总的误差值</span></div><div class="line">                totalerror += abs(errorterm)</div><div class="line">        <span class="keyword">print</span> totalerror</div><div class="line"></div><div class="line">        <span class="comment"># 如果节点移动之后的情况变得更糟，则 break 程序结束</span></div><div class="line">        <span class="keyword">if</span> lasterror <span class="keyword">and</span> lasterror &lt; totalerror: <span class="keyword">break</span></div><div class="line">        lasterror = totalerror</div><div class="line"></div><div class="line">        <span class="comment"># 根据 rate 参数与 grad 值相乘的结果，移动每一个节点</span></div><div class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(n):</div><div class="line">            loc[k][<span class="number">0</span>] -= rate * grad[k][<span class="number">0</span>]</div><div class="line">            loc[k][<span class="number">1</span>] -= rate * grad[k][<span class="number">1</span>]</div><div class="line"></div><div class="line">    <span class="keyword">return</span> loc</div></pre></td></tr></table></figure>
<p>根据 <strong><code>scaledown()</code></strong> 函数得到的<strong><code>loc</code></strong>，我们可以利用 PIL 在生成一张二维图，根据新的坐标值<strong><code>loc</code></strong>，在图上标出所有数据项的位置以及对应的标签。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw2d</span><span class="params">(data, labels, jpeg=<span class="string">'mds2d.jpg'</span>)</span>:</span></div><div class="line">    img = Image.new(<span class="string">'RGB'</span>, (<span class="number">2000</span>, <span class="number">2000</span>), (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>))</div><div class="line">    draw = ImageDraw.Draw(img)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</div><div class="line">        x = (data[i][<span class="number">0</span>] + <span class="number">0.5</span>)*<span class="number">1000</span></div><div class="line">        y = (data[i][<span class="number">1</span>] + <span class="number">0.5</span>)*<span class="number">1000</span></div><div class="line">        draw.text((x, y), labels[i], (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>))</div><div class="line">    img.save(jpeg, <span class="string">'JPEG'</span>)</div></pre></td></tr></table></figure></p>
<p>在命令行中输入：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import clusters</div><div class="line">&gt;&gt;&gt; words, blognames, data = clusters.readfile(<span class="string">'blogname.txt'</span>)</div><div class="line">&gt;&gt;&gt; coords = clusters.scaledown(data)</div><div class="line">&gt;&gt;&gt; clusters.draw2d(coords, blognames, jpeg = <span class="string">'blogs2d.jpg'</span>)</div></pre></td></tr></table></figure></p>
<p>得到的 <strong>blog2d.jpg</strong> 反映就是博客数据集中 blog 之间的相似关系，当然我们也可以通过 <strong><code>rotatematrix()</code></strong> 函数来转置矩阵 <strong><code>data</code></strong>，得到<strong><code>rdata</code></strong>，再根据<strong><code>rdata</code></strong> 画出单词表中的单词的相似度（误差有点大）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import clusters</div><div class="line">&gt;&gt;&gt; words, blognames, data = clusters.readfile(<span class="string">'blogname.txt'</span>)</div><div class="line">&gt;&gt;&gt; rdata = clusters.rotatematrix(data)</div><div class="line">&gt;&gt;&gt; coords = clusters.scaledown(rdata)</div><div class="line">&gt;&gt;&gt; clusters.draw2d(coords, words, jpeg = <span class="string">'words2d.jpg'</span>)</div></pre></td></tr></table></figure></p>
<p>即得到单词之间的相似关系<strong>words2d.jpg</strong>。</p>
<p>同样，我们可以将其用于我们的欲望物品数据集，距离度量使用 <strong>tanimoto 系数</strong>，但是要修改<strong><code>tanimoto()</code></strong> 与<strong><code>scaledown()</code></strong>几处代码，即需要判断分母为 0 的时候的返回值。修改完之后，调用 <strong><code>scaledown()</code></strong> 与<strong><code>draw2d()</code></strong>，画出欲望物品之间的相似关系图 <strong>items2d.jpg</strong>。（同样也可以通过转置<strong><code>data</code></strong> 矩阵，画出用户之间的相似关系图<strong>users2d.jpg</strong>）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import clusters</div><div class="line">&gt;&gt;&gt; users, items, data = clusters.readfile(<span class="string">'zebo.txt'</span>)</div><div class="line">&gt;&gt;&gt; coords = clusters.scaledown(rdata, distance = tanimoto)</div><div class="line">&gt;&gt;&gt; clusters.draw2d(coords, items, jpeg = <span class="string">'items2d.jpg'</span>)</div></pre></td></tr></table></figure></p>
<p>当然，我们还可以根据需要修改 <strong><code>rate</code></strong> 等诸多参数的值来调整算法，在此不再赘述。</p>
<hr>
<h2 id="EM 聚类："><a href="#EM 聚类：" class="headerlink" title="EM 聚类："></a>EM 聚类：</h2><p>EM 聚类算法的重点不是像 K- 均值聚类那样找到一个质心，然后找到与其相关的数据点，而是求解另一个不同的问题。比如我们希望一个数据集分为两部分：簇 1 与簇 2。<strong>EM 聚类算法的目的是，我们希望得到一个关于数据是否存在某个簇中的良好估计，但并不用关心其中是否存在模糊性。我们真正希望获得的是一个数据点属于各簇的概率值，而非分配结果。</strong></p>
<p>与专注于确定各簇之间边界的 $K$- 均值聚类算法不同，EM 聚类对于可能同属于多个簇的数据点具有一定的稳健性。EM 聚类算法非常适用于对不存在明确边界的数据进行分类。</p>
<hr>
<h1 id="Correct-errors-in-printing"><a href="#Correct-errors-in-printing" class="headerlink" title="Correct errors in printing:"></a>Correct errors in printing:</h1><ul>
<li>P46</li>
</ul>
<blockquote>
<p>…代码首先会构造一个列表，其中包含的是超过 5 个人都希望拥有的物品….</p>
</blockquote>
<p>需要更正为：</p>
<blockquote>
<p>…代码首先会构造一个列表，其中包含的是超过 10 个人都希望拥有的物品….</p>
</blockquote>
<hr>
<h1 id="Practice"><a href="#Practice" class="headerlink" title="Practice:"></a>Practice:</h1><ul>
<li><strong>暂无</strong></li>
</ul>
<hr>
]]></content>
    
    <summary type="html">
    
      本文是关于「Programming Collective Intelligence」这本书的 Chapter 3的学习笔记。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
      <category term="Book:「Programming Collective Intelligence」" scheme="http://randolph.pro/categories/Machine-Learning/Book-%E3%80%8CProgramming-Collective-Intelligence%E3%80%8D/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>♞「Programming Collective Intelligence」 Chapter 1 &amp; 2</title>
    <link href="http://randolph.pro/2016/03/12/%E2%99%9E%E3%80%8CProgramming%20Collective%20Intelligence%E3%80%8D%20Chapter%201%20&amp;%202/"/>
    <id>http://randolph.pro/2016/03/12/♞「Programming Collective Intelligence」 Chapter 1 &amp; 2/</id>
    <published>2016-03-11T16:00:00.000Z</published>
    <updated>2017-08-06T13:18:21.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4306/36216231065_8a7e508f9e_o.jpg" alt=""></p>
<p>有关该书的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/Book-「Programming-Collective-Intelligence」/">Book:「Programming Collective Intelligence」</a></p>
<h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><ul>
<li><strong>欧几里得距离评价</strong></li>
<li><strong>皮尔逊相关度评价</strong></li>
</ul>
<blockquote>
<ul>
<li>The formula for this is more complicated than the Euclidean distance score, but it tends to give better results in situations where the data isn’t well normalized—for example, if critics’ movie rankings are routinely more harsh than average. 它相比于欧几里德距离评价更加复杂，但其在数据不是很规范的时候（比如，影评者对影片的评价总是相对于平均水平偏离很大的时候），会给出更好的结果。      </li>
<li>If one critic is inclined to give higher scores than the other, there can still be perfect correlation if the difference between their scores is consistent. The Euclidean distance score described earlier will say that two critics are dissimilar because one is consistently harsher than the other, even if their tastes are very similar. 如果某人总是倾向于给出比另一个人更高的分值，而两者的分值之差又始终保持一致，则他们依然可能会存在很好的相关性。而欧几里德距离评价会因为一个人的评价之中比另外一个人的更为“严格”（从而导致评价始终相对偏低），从而得出两者不相近的结论，即使他们的品位很相似也是如此。</li>
</ul>
</blockquote>
<ul>
<li><strong>其他相似度计算函数（Minkowski 距离、Mahalanobis 距离等）</strong></li>
</ul>
<hr>
<h1 id="Key"><a href="#Key" class="headerlink" title="Key:"></a>Key:</h1><p>在「推荐物品」的模块中，提到了一种方法：</p>
<ol>
<li>通过函数找出与自己有相似品味的影评者，并按相似度从大到小排序。</li>
<li>对于自己未看过的影片，建立一张表，表的内容包括：品位相似的影评者以及其对应的相似度，对于自己未看过影片的评分（影评者可以看过也可以没有看过）。</li>
<li>对于某一个未看过的电影，影评者的相似度（可理解为权值，不同评论者的权值不同，相似度越高，权值越高）乘以其对该电影的评分，其他影评者也得到一个值（如果没有看过，则为零），然后累加，记为其他影评者对于该电影者的评价总和，之后总和需要除以所有对该电影评过分的影评者的相似度之和。</li>
<li>最后得到的结果，表示为自己对于没有看过的电影，通过自己品位相似的影评者得到的预测评分，根据预测评分，来给出决策。</li>
</ol>
<hr>
<h1 id="Need-to-know"><a href="#Need-to-know" class="headerlink" title="Need to know:"></a>Need to know:</h1><p>在「构建一个基于 del.icio.us 的链接推荐系统」的模块中：<br>首先我们需要下载 <strong><code>pydelicious</code></strong> 这一个 package。<strong>［这个 package 不支持 python3.x］</strong></p>
<p>我的尝试：</p>
<ol>
<li>通过 Pycharm 自带的“便利”package 下载。出错，原因：无法找到对应的版本。</li>
<li>通过命令行输入<strong><code>sudo pip install pydelicious</code></strong>。出错，原因：Could not find a version that satisfies the requirement pydelicious (from versions:)No matching distribution found for pydelicious。</li>
<li>通过命令行输入<strong><code>sudo pip install pydelicious --allow-external pydelicious --allow-unverified pydelicious</code></strong>。出错，原因：Could not find a version that satisfies the requirement pydelicious (from versions:)No matching distribution found for pydelicious。</li>
</ol>
<hr>
<p>初次尝试失败之后，在 Stackoverflow 寻找解决办法：</p>
<ol>
<li>首先，按照书本提供的下载地址：<a href="https://code.google.com/archive/p/pydelicious/downloads" target="_blank" rel="external">the pydelicious download page</a>［需要翻墙］下载 <strong>pydelicious-0.5.0.zip</strong> 文件。解压之后得到文件夹。</li>
<li>命令行 <strong><code>cd</code></strong> 到解压后的文件夹，然后输入 <strong><code>sudo python setup.py install</code></strong>，错误提示：Feedparser not available, no RSS parsing。<strong> 意思是缺少 <code>feedparser</code> 这一 package 依赖库，需要安装 feedparser</strong>。</li>
<li>安装 feedparser，下载地址：<a href="http://download.csdn.net/download/dixin28/5271130" target="_blank" rel="external">the feedparser download page</a>［需要积分］，或者<a href="https://github.com/kurtmckee/feedparser" target="_blank" rel="external">the feedparser download page</a>［需要翻墙］，下载文件夹。</li>
<li>命令行 <strong><code>cd</code></strong> 到 feedparser 的文件夹，然后输入<strong><code>sudo python setup.py install</code></strong>，feedparser 安装完成。</li>
<li>命令行 <strong><code>cd</code></strong> 回到 pydelicious 文件夹，再次输入<strong><code>sudo python setup.py install</code></strong>，此时会发现 pydelicious 安装成功。</li>
<li>测试 pydelicious 此 package 是否能够导入，命令行输入 <strong><code>python</code></strong> 之后，再输入<strong><code>import pydelicious</code></strong>，如果没有报错，这说明 pydelicious 安装成功。</li>
</ol>
<ul>
<li>2016.11.28 补充：如果按照书本上下载的 pydelicious-0.5.0 版本，是可以正常运行书本上的代码的而不报错的，但是会出现无论我如何修改 tag 的值，返回的内容都是一样的，原因在后面解释了。但是如果我们下载的是 <a href="https://github.com/dotmpe/python-delicious" target="_blank" rel="external">github 上更新后的 pydelicious 版本</a>，会遇到如下问题，解决办法是需要修改<strong><code>__init__.py</code></strong> 文件中的几处代码，但是仍然会出现 tag 值的问题。</li>
</ul>
<hr>
<p>本以为问题得到了解决，可以按照书上的代码继续进行:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ python</div><div class="line">&gt;&gt;&gt; import pydelicious</div><div class="line">&gt;&gt;&gt; pydelicious.get_popular(tag=<span class="string">'python'</span>)</div></pre></td></tr></table></figure>
<p>此时会报错，无论是否翻墙，显示获取失败。</p>
<hr>
<p>我在 stackoverflow.com 上找到了原因：<a href="http://stackoverflow.com/questions/29543799/pydelicious-get-popularprogramming-doesnt-return-any-valid-url" target="_blank" rel="external">the answer</a><strong>［需要翻墙］</strong><br>仔细看提问者的问题，重点是后面提出解决办法的几个回答。</p>
<blockquote>
<p><strong>You should modify the __init__.py to:rss =http_request(‘<a href="http://feeds.delicious.com/v2/rss" target="_blank" rel="external">http://feeds.delicious.com/v2/rss</a>‘) .read()</strong></p>
</blockquote>
<p>所以解决的办法是：<br>打开 pydelicious 的文件夹，找到子文件夹 pydelicious 下的 <strong><code>__init__.py</code></strong> 文件，修改三处地方：</p>
<ul>
<li><strong>DLCS_RSS = ‘<a href="http://feeds.delicious.com/v2/rss/" target="_blank" rel="external">http://feeds.delicious.com/v2/rss/</a>‘</strong></li>
<li><strong>rss = http_request(‘<a href="http://feeds.delicious.com/v2/rss" target="_blank" rel="external">http://feeds.delicious.com/v2/rss</a>‘). read()</strong></li>
<li><strong>def get_popular(tag =””):return getrss(tag = tag, popular =0)</strong></li>
</ul>
<p>命令行 <strong><code>cd</code></strong> 到 pydelicious 安装总文件夹，重新输入<strong><code>sudo python setup.py install</code></strong>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ python</div><div class="line">&gt;&gt;&gt; import pydelicious</div><div class="line">&gt;&gt;&gt; pydelicious.get_popular(tag=<span class="string">'python'</span>)</div></pre></td></tr></table></figure>
<p>此时会发现成功获取到了内容（注意检查网络，如果仍然无法获取，记得翻墙）。</p>
<hr>
<p>本以为到此终于告一段落，但是实际上：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import pydelicious</div><div class="line">&gt;&gt;&gt; pydelicious.get_popular(tag=<span class="string">'python'</span>)</div><div class="line">&gt;&gt;&gt; pydelicious.get_popular(tag=<span class="string">'xxx'</span>)</div></pre></td></tr></table></figure>
<p>意思是无论我如何更改参数 <strong><code>tag</code></strong> 的值，返回的内容会发现是一样。这个问题，stackoverflow 老外也同样遇到了：</p>
<blockquote>
<p><strong>I see the resource code again. Maybe it is wrong. Because If you edit the code, the procedural answer always remain unchanged…I’m studing…</strong></p>
</blockquote>
<p>我个人觉得可能是 DLCS_RSS 的网址还需要更改一下（因为这本书在刚出来的时候，pydelicious 还是支持原 del.icio.us 的网站，是不需要去更改 <strong><code>__init__.py</code></strong> 的文件等，后来是 unspported，所以需要更改 <strong><code>__init__.py</code></strong> 文件中的 RSS 订阅源，也许可能这个订阅源还不是最新的，反正是坑…），或者说是 <strong><code>get_popular()</code></strong> 这个 function 有误（这个不太可能），总而言之，折腾了一下晚上，感觉是遇到了坑，不过好歹也算是解决出来了。</p>
<p>貌似有 <strong><code>deliciousapi</code></strong> 这个 package 作为替代，我也尝试过，但运行说明文档中的几个函数，发现会报错，希望如果有人知道如何用 <strong>deliciousapi</strong> 替代 pydelicious 完成第二章后续的几个模块，请务必告诉我！</p>
<p>注意：新手实践这本书的时候，完全可以跳过这个坑，因为没有必要，只需要 get 第二章几个重要的算法或者是思想就可以了。</p>
<hr>
<h1 id="Correct-errors-in-printing"><a href="#Correct-errors-in-printing" class="headerlink" title="Correct errors in printing:"></a>Correct errors in printing:</h1><ul>
<li>P13</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 如果两者没有共同之处，则返回 1</span></div><div class="line"><span class="keyword">if</span> n==<span class="number">0</span>: <span class="keyword">return</span> <span class="number">1</span></div></pre></td></tr></table></figure>
<p>需要更正为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 如果两者没有共同之处，则返回 0</span></div><div class="line"><span class="keyword">if</span> n==<span class="number">0</span>: <span class="keyword">return</span> <span class="number">0</span></div></pre></td></tr></table></figure>
<hr>
<h1 id="Practice"><a href="#Practice" class="headerlink" title="Practice:"></a>Practice:</h1><ul>
<li><strong>暂无</strong></li>
</ul>
<hr>
]]></content>
    
    <summary type="html">
    
      本文是关于「Programming Collective Intelligence」这本书的 Chapter 1 &amp; 2 的学习笔记。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
      <category term="Book:「Programming Collective Intelligence」" scheme="http://randolph.pro/categories/Machine-Learning/Book-%E3%80%8CProgramming-Collective-Intelligence%E3%80%8D/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>♞「Web Scraping with Python」smtplib &amp; email module</title>
    <link href="http://randolph.pro/2015/12/13/%E2%99%9E%E3%80%8CWeb%20Scraping%20with%20Python%E3%80%8Dsmtplib%20&amp;%20email%20module/"/>
    <id>http://randolph.pro/2015/12/13/♞「Web Scraping with Python」smtplib &amp; email module/</id>
    <published>2015-12-12T16:00:00.000Z</published>
    <updated>2017-08-06T13:17:54.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4316/36049777032_f975ed0941_o.jpg" alt=""></p>
<p>有关该书的其他学习笔记系列：<a href="http://randolph.pro/categories/Web-Scraping/Book-「Web-Scraping-with-Python」/">Book:「Web Scraping with Python」</a></p>
<h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><ul>
<li><strong>smtplib module</strong></li>
<li><strong>email module</strong></li>
</ul>
<hr>
<h1 id="smtplib-module"><a href="#smtplib-module" class="headerlink" title="smtplib module"></a>smtplib module</h1><p>使用 python 脚本发邮件，一般会用到 <strong><code>smtplib</code></strong> 和 <strong><code>email</code></strong> 这两个模块。<strong><code>smtplib</code></strong> 模块定义了一个简单的 SMTP 客户端，可以用来在互联网上发送邮件。参考下面的程序：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> email.header <span class="keyword">import</span> Header</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> smtplib</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line">mail_host = <span class="string">'smtp.gmail.com'</span>            <span class="comment"># 设置服务器</span></div><div class="line">mail_port = <span class="number">587</span>                         <span class="comment"># 服务器端口号</span></div><div class="line">mail_user = <span class="string">'your_username@gmail.com'</span>   <span class="comment"># 用户名</span></div><div class="line">mail_pass = <span class="string">'your_password'</span>             <span class="comment"># 口令</span></div><div class="line"></div><div class="line">sender = <span class="string">'your_username@gmail.com'</span></div><div class="line">receivers = <span class="string">'your_other_username@hotmail.com'</span></div><div class="line"></div><div class="line"><span class="comment"># fill content with MIMEText's object</span></div><div class="line">msg = MIMEText(<span class="string">'Hi, I am Randolph.'</span>)</div><div class="line">msg[<span class="string">'From'</span>] = sender</div><div class="line">msg[<span class="string">'To'</span>] = receivers</div><div class="line">msg[<span class="string">'Subject'</span>] = <span class="string">'Hello, today is a special day.'</span></div><div class="line">print(msg.as_string())</div><div class="line"></div><div class="line"><span class="comment"># connect</span></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    print(<span class="string">"Connecting ..."</span>)</div><div class="line">    smtpObj = smtplib.SMTP(mail_host, mail_port)</div><div class="line"><span class="keyword">except</span>:</div><div class="line">    print(<span class="string">"CONNECT ERROR ****"</span>)</div><div class="line"></div><div class="line"><span class="comment"># show the debug log</span></div><div class="line">smtpObj.set_debuglevel(<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># gmail uses ssl</span></div><div class="line">smtpObj.ehlo()</div><div class="line">smtpObj.starttls()</div><div class="line">smtpObj.ehlo()</div><div class="line"></div><div class="line"><span class="comment"># login with username &amp; password</span></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    print(<span class="string">"Loginning ..."</span>)</div><div class="line">    smtpObj.login(mail_user, mail_pass)</div><div class="line"><span class="keyword">except</span>:</div><div class="line">    print(<span class="string">"LOGIN ERROR ****"</span>)</div><div class="line"></div><div class="line">smtpObj.sendmail(sender, receivers, msg.as_string())</div><div class="line">smtpObj.quit()</div></pre></td></tr></table></figure>
<p>smtp 实例封装一个 smtp 连接，它支持所有的 SMTP 和 ESMTP 操作指令，如果 host 和 port 参数被定义，则 smtp 会在初始化期间自动调用 <strong><code>connect()</code></strong> 方法，如果 <strong><code>connect()</code></strong> 方法失败，则会触发 <strong><code>SMTPConnectError</code></strong> 异常，<strong><code>timeout</code></strong> 参数设置了超时时间。在一般的调用过程中，应该遵 <strong><code>connect()</code></strong>、<strong><code>sendmail()</code></strong>、<strong><code>quit()</code></strong>步骤。</p>
<hr>
<h2 id="SMTP 模块主要方法"><a href="#SMTP 模块主要方法" class="headerlink" title="SMTP 模块主要方法"></a>SMTP 模块主要方法 </h2><p> 下面我们来看看该类的方法：</p>
<ul>
<li><strong><code>smtp.set_debuglevel(level)</code></strong><br>设置输出 debug 调试信息，默认不输出调试信息。</li>
<li><strong><code>smtp.docmd(cmd, argstring)</code></strong><br>发送一个 command 到 smtp 服务器，</li>
<li><strong><code>smtp.connect(host, port)</code></strong><br>连接到指定的 smtp 服务器，默认是本机的 25 端口。也可以写成 hostname:port 的形式。</li>
<li><strong><code>smtp.helo(hostname)</code></strong><br>使用 helo 指令向 smtp 服务器确认你的身份。</li>
<li><strong><code>smtp.ehlo(hostname)</code></strong><br>使用 ehlo 指令向 esmtp 服务器确认你的身份。</li>
<li><strong><code>smtp.ehlo_or_helo_if_needed()</code></strong><br>如果在以前的会话连接中没有提供 ehlo 或者 helo 指令，这个方法调用 ehlo() 或者 helo()。</li>
<li><strong><code>smtp.has_extn(name)</code></strong><br>判断指定的名称是否在 smtp 服务器上。</li>
<li><strong><code>smtp.verify(address)</code></strong><br>判断邮件地址是否在 smtp 服务器上存在。</li>
<li><strong><code>smtp.login(user, password)</code></strong><br>登陆需要验证的 smtp 服务器，如果之前没有提供 ehlo 或者 helo 指令，则会先尝试 ESMTP 的 ehlo 指令。</li>
<li><strong><code>smtp.starttls(keyfile, certfile)</code></strong><br>使 smtp 连接运行在 TLS 模式，所有的 smtp 指令都会被加密。</li>
<li><strong><code>smtp.sendmail(from_addr, to_addrs, msg, mail_options, rcpt_options)</code></strong><br>发送邮件，该方法需要一些邮件地址和消息。</li>
<li><strong><code>smtp.quit()</code></strong><br>终止 smtp 会话并且关闭连接。</li>
</ul>
<hr>
<h2 id="email-module"><a href="#email-module" class="headerlink" title="email module"></a>email module</h2><p>如果想在邮件中携带附件，使用 html 书写邮件，附带图片等等，就需要使用 <strong><code>email</code></strong> 模块及其子模块。下面来看看 email 包，email 包是用来管理 email 信息的，它包括 MIME 和其他基于 RFC 2822 的消息格式。email 包的主要特征是在它内部解析和生成 email 信息是分开的模块来实现的。</p>
<p>MIME 消息由消息头和消息体两大部分组成，在邮件里就是邮件头和邮件体。邮件头与邮件体之间以空行进行分隔。</p>
<p>邮件头包含了发件人、收件人、主题、时间、MIME 版本、邮件内容的类型等重要信息。每条信息称为一个域，由域名后加 “ : ” 和信息内容构成，可以是一行，较长的也可以占用多行。域的首行必须“顶头”写，即左边不能有空白字符（空格和制表符）；续行则必须以空白字符打头，且第一个空白字符不是信息本身固有的。</p>
<p>邮件体包含邮件的内容，它的类型由邮件头的 “Content-Type” 域指出。最常见的类型有 text/plain（纯文本） 和 text/html（超文本）。邮件体被分为多个段，每个段又包含段头和段体两部分，这两部分之间也以空行分隔。常见的 multipart 类型有三种：multipart/mixed，multipart/related 和 multipart/alternative。</p>
<p>在 email 的包里面包含了很多模块：</p>
<ul>
<li><strong><code>email.message</code></strong></li>
<li><strong><code>email.parser</code></strong></li>
<li><strong><code>email.generator</code></strong></li>
<li><strong><code>email.mime</code> （创建 email 和 MIME 对象）</strong></li>
<li><strong><code>email.header</code></strong></li>
<li><strong><code>email.charset</code></strong></li>
<li><strong><code>email.encoders</code></strong></li>
<li><strong><code>email.errors</code></strong></li>
<li><strong><code>email.utils</code></strong></li>
<li><strong><code>email.iterators</code></strong></li>
</ul>
<p>主要来看看 <strong><code>email.mime</code></strong>，在邮件中携带附件、图片、音频时，主要使用的是该模块。一般情况下，你通过解析一个文件或者一段 text 来生成一个消息对象结构，你也可以从头开始建立一个消息结构，实际上，你可以给一个已经存在的消息结构追加一个新的消息对象。你可以通过创建 message 实例来创建一个对象结构，然后给该结构追加附件和头部信息。email 包提供了一些子类使得该操作变得很容易。</p>
<p>模拟在邮件内容中携带图片，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> email.mime.text <span class="keyword">import</span> MIMEText</div><div class="line"><span class="keyword">from</span> email.mime.multipart <span class="keyword">import</span> MIMEMultipart</div><div class="line"><span class="keyword">from</span> email.mime.image <span class="keyword">import</span> MIMEImage</div><div class="line"><span class="keyword">from</span> email.message <span class="keyword">import</span> Message</div><div class="line"><span class="keyword">import</span> email.utils</div><div class="line"><span class="keyword">import</span> smtplib</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> smtplib</div><div class="line"><span class="keyword">import</span> base64</div><div class="line"></div><div class="line">mail_host = <span class="string">'smtp.gmail.com'</span>            <span class="comment"># 设置服务器</span></div><div class="line">mail_port = <span class="number">587</span>                         <span class="comment"># 服务器端口号</span></div><div class="line">mail_user = <span class="string">'your_username@gmail.com'</span>   <span class="comment"># 用户名</span></div><div class="line">mail_pass = <span class="string">'your_password'</span>             <span class="comment"># 口令</span></div><div class="line"></div><div class="line">sender = <span class="string">'your_username@gmail.com'</span></div><div class="line">receivers = <span class="string">'your_other_username@hotmail.com'</span></div><div class="line"></div><div class="line"><span class="comment"># send email with images use MIMEMultipart's object</span></div><div class="line">msg = MIMEMultipart()</div><div class="line">msg[<span class="string">'From'</span>] = sender</div><div class="line">msg[<span class="string">'To'</span>] = receivers</div><div class="line">msg[<span class="string">'Subject'</span>] = <span class="string">'An email with a image.'</span></div><div class="line">body = <span class="string">'Test image send.'</span></div><div class="line">con = MIMEText(<span class="string">'&lt;b&gt;%s&lt;/b&gt;![](cid:/Users/xxx/xxx/xxx.jpg)'</span> % body, <span class="string">'html'</span>)</div><div class="line">msg.attach(con)</div><div class="line"></div><div class="line">img = MIMEImage(open(<span class="string">'/Users/xxx/xxx/xxx.jpg'</span>, <span class="string">'rb'</span>).read())</div><div class="line">img.add_header(<span class="string">'Content-ID'</span>, <span class="string">'/Users/xxx/xxx/xxx.jpg'</span>)</div><div class="line">msg.attach(img)</div><div class="line"></div><div class="line"><span class="comment"># connect</span></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    print(<span class="string">"Connecting ..."</span>)</div><div class="line">    smtpObj = smtplib.SMTP(mail_host, mail_port)</div><div class="line"><span class="keyword">except</span>:</div><div class="line">    print(<span class="string">"CONNECT ERROR ****"</span>)</div><div class="line"></div><div class="line"><span class="comment"># show the debug log</span></div><div class="line">smtpObj.set_debuglevel(<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># gmail uses ssl</span></div><div class="line">smtpObj.ehlo()</div><div class="line">smtpObj.starttls()</div><div class="line">smtpObj.ehlo()</div><div class="line"></div><div class="line"><span class="comment"># login with username &amp; password</span></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    print(<span class="string">"Loginning ..."</span>)</div><div class="line">    smtpObj.login(mail_user, mail_pass)</div><div class="line"><span class="keyword">except</span>:</div><div class="line">    print(<span class="string">"LOGIN ERROR ****"</span>)</div><div class="line"></div><div class="line">smtpObj.sendmail(sender, receivers, msg.as_string())</div><div class="line">smtpObj.quit()</div></pre></td></tr></table></figure>
<hr>
<h2 id="Send-email-with-attachment"><a href="#Send-email-with-attachment" class="headerlink" title="Send email with attachment"></a>Send email with attachment</h2><p>发送带附件的邮件，首先要创建 <strong><code>MIMEMultipart()</code></strong> 实例，然后构造附件，如果有多个附件，可依次构造，最后利用 <strong><code>smtplib.smtp</code></strong> 发送。</p>
<p>模拟在邮件中携带附件，代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> email.mime.text <span class="keyword">import</span> MIMEText</div><div class="line"><span class="keyword">from</span> email.mime.multipart <span class="keyword">import</span> MIMEMultipart</div><div class="line"><span class="keyword">from</span> email.mime.image <span class="keyword">import</span> MIMEImage</div><div class="line"><span class="keyword">from</span> email.message <span class="keyword">import</span> Message</div><div class="line"><span class="keyword">import</span> email.utils</div><div class="line"><span class="keyword">import</span> smtplib</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> smtplib</div><div class="line"><span class="keyword">import</span> base64</div><div class="line"></div><div class="line">mail_host = <span class="string">'smtp.gmail.com'</span>            <span class="comment"># 设置服务器</span></div><div class="line">mail_port = <span class="number">587</span>                         <span class="comment"># 服务器端口号</span></div><div class="line">mail_user = <span class="string">'your_username@gmail.com'</span>   <span class="comment"># 用户名</span></div><div class="line">mail_pass = <span class="string">'your_password'</span>             <span class="comment"># 口令</span></div><div class="line"></div><div class="line">sender = <span class="string">'your_username@gmail.com'</span></div><div class="line">receivers = <span class="string">'your_other_username@hotmail.com'</span></div><div class="line"></div><div class="line"><span class="comment"># send email with attachment</span></div><div class="line">msg = MIMEMultipart()</div><div class="line">txt = MIMEText(<span class="string">"我这半世未算赶，何妨迷途看风光."</span>,<span class="string">'plain'</span>,<span class="string">'gb2312'</span>)</div><div class="line">msg.attach(txt)</div><div class="line"></div><div class="line"><span class="comment"># 构造附件 1</span></div><div class="line">att1 = MIMEText(open(<span class="string">'/Users/xxx/xxx/xxx.jpg'</span>, <span class="string">'rb'</span>).read(), <span class="string">'base64'</span>, <span class="string">'gb2312'</span>)</div><div class="line">att1[<span class="string">"Content-Type"</span>] = <span class="string">'application/octet-stream'</span></div><div class="line">att1[<span class="string">"Content-Disposition"</span>] = <span class="string">'attachment; filename="xxx.jpg"'</span></div><div class="line"><span class="comment"># 这里的 filename 可以任意写，写什么名字，邮件中显示什么名字</span></div><div class="line">msg.attach(att1)</div><div class="line"></div><div class="line"><span class="comment"># 构造附件 2</span></div><div class="line">att2 = MIMEText(open(<span class="string">'/Users/xxx/xxx/xxx.doc'</span>, <span class="string">'rb'</span>).read(), <span class="string">'base64'</span>, <span class="string">'gb2312'</span>)</div><div class="line">att2[<span class="string">"Content-Type"</span>] = <span class="string">'application/octet-stream'</span></div><div class="line">att2[<span class="string">"Content-Disposition"</span>] = <span class="string">'attachment; filename="xxx.doc"'</span></div><div class="line">msg.attach(att2)</div><div class="line"></div><div class="line"><span class="comment"># 加邮件头</span></div><div class="line">msg[<span class="string">'to'</span>] = sender</div><div class="line">msg[<span class="string">'from'</span>] = receivers</div><div class="line">msg[<span class="string">'subject'</span>] = <span class="string">'Test.'</span></div><div class="line"></div><div class="line"><span class="comment"># connect</span></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    print(<span class="string">"Connecting ..."</span>)</div><div class="line">    smtpObj = smtplib.SMTP(mail_host, mail_port)</div><div class="line"><span class="keyword">except</span>:</div><div class="line">    print(<span class="string">"CONNECT ERROR ****"</span>)</div><div class="line"></div><div class="line"><span class="comment"># show the debug log</span></div><div class="line">smtpObj.set_debuglevel(<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># gmail uses ssl</span></div><div class="line">smtpObj.ehlo()</div><div class="line">smtpObj.starttls()</div><div class="line">smtpObj.ehlo()</div><div class="line"></div><div class="line"><span class="comment"># login with username &amp; password</span></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    print(<span class="string">"Loginning ..."</span>)</div><div class="line">    smtpObj.login(mail_user, mail_pass)</div><div class="line"><span class="keyword">except</span>:</div><div class="line">    print(<span class="string">"LOGIN ERROR ****"</span>)</div><div class="line"></div><div class="line">smtpObj.sendmail(sender, receivers, msg.as_string())</div><div class="line">smtpObj.quit()</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      本文介绍了关于 Web Scraping 网络爬虫时可能会用到的 smtplib &amp; email 模块。
    
    </summary>
    
      <category term="Web Scraping" scheme="http://randolph.pro/categories/Web-Scraping/"/>
    
      <category term="Book:「Web Scraping with Python」" scheme="http://randolph.pro/categories/Web-Scraping/Book-%E3%80%8CWeb-Scraping-with-Python%E3%80%8D/"/>
    
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
      <category term="Web Scraping" scheme="http://randolph.pro/tags/Web-Scraping/"/>
    
  </entry>
  
  <entry>
    <title>♞「Web Scraping with Python」 Chapter 4 &amp; 5</title>
    <link href="http://randolph.pro/2015/11/27/%E2%99%9E%E3%80%8CWeb%20Scraping%20with%20Python%E3%80%8D%20Chapter%204%20&amp;%205/"/>
    <id>http://randolph.pro/2015/11/27/♞「Web Scraping with Python」 Chapter 4 &amp; 5/</id>
    <published>2015-11-26T16:00:00.000Z</published>
    <updated>2017-08-06T13:18:04.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4316/36049777032_f975ed0941_o.jpg" alt=""></p>
<p> 有关该书的其他学习笔记系列：<a href="http://randolph.pro/categories/Web-Scraping/Book-「Web-Scraping-with-Python」/">Book:「Web Scraping with Python」</a></p>
<h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><ul>
<li><strong>Parsing JSON</strong></li>
<li><strong>Storing Data to CSV</strong></li>
<li><strong>Integrating with Python &amp; MySQL</strong></li>
</ul>
<hr>
<h1 id="Key："><a href="#Key：" class="headerlink" title="Key："></a>Key：</h1><h2 id="Parsing-JSON"><a href="#Parsing-JSON" class="headerlink" title="Parsing JSON?"></a>Parsing JSON?</h2><blockquote>
<p>Python uses a more flexible approach and turns JSON objects into dictionaries, JSON arrays into lists, JSON strings into strings, and so forth. In this way, it makes it extremely easy to access and manipulate values stored in JSON.</p>
<p>Python 使用了一种更加灵活的方式来处理 JSON，把 JSON 转换成字典，JSON 数组转换成列表，JSON 字符串转换成 Python 字符串。通过这种方式，就可以让 JSON 的获取和操作变得更加简单。<br> 下面的程序对维基百科的编辑历史页面里面的 IP 地址找出来，并查询 IP 地址所属的国家和地区：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</div><div class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> HTTPError</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> datetime</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line">random.seed(datetime.datetime.now())</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getLinks</span><span class="params">(article_url)</span>:</span></div><div class="line">    html = urlopen(<span class="string">"http://en.wikipedia.org"</span>+article_url)</div><div class="line">    bsObj = BeautifulSoup(html.read(),<span class="string">"html5lib"</span>)</div><div class="line">    <span class="keyword">return</span> bsObj.find(<span class="string">"div"</span>,&#123;<span class="string">"id"</span>:<span class="string">"bodyContent"</span>&#125;).findAll(<span class="string">"a"</span>,href=re.compile(<span class="string">"^(/wiki/)((?!:).)*$"</span>))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHistoryIPs</span><span class="params">(page_url)</span>:</span></div><div class="line">    <span class="comment"># http://en.wikipedia.org/w/index.php?title=Title_in_URL&amp;action=history</span></div><div class="line">    page_url = page_url.replace(<span class="string">"/wiki/"</span>,<span class="string">""</span>)</div><div class="line">    history_url = <span class="string">"http://en.wikipedia.org/w/index.php?title="</span> + page_url + <span class="string">"&amp;action=history"</span></div><div class="line">    print(<span class="string">"history url is: "</span> + history_url)</div><div class="line">    html = urlopen(history_url)</div><div class="line">    bsObj = BeautifulSoup(html.read(),<span class="string">"html5lib"</span>)</div><div class="line">    <span class="comment"># finds only the links with class"mw-anonuserlink"which has IP addresses instead of usernames</span></div><div class="line">    ipAddresses = bsObj.findAll(<span class="string">"a"</span>, &#123;<span class="string">"class"</span>:<span class="string">"mw-anonuserlink"</span>&#125;)</div><div class="line">    addressList = set()</div><div class="line">    <span class="keyword">for</span> ipAddresses <span class="keyword">in</span> ipAddresses:</div><div class="line">        addressList.add(ipAddresses.get_text())</div><div class="line">    <span class="keyword">return</span> addressList</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getCountry</span><span class="params">(ipAddress)</span>:</span></div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        response = urlopen(<span class="string">"http://freegeoip.net/json/"</span> + ipAddress).read().decode(<span class="string">'utf-8'</span>)</div><div class="line">    <span class="keyword">except</span> HTTPError:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">None</span></div><div class="line">    responseJson = json.loads(response)</div><div class="line">    <span class="keyword">return</span> responseJson.get(<span class="string">"country_code"</span>)</div><div class="line"></div><div class="line">links = getLinks(<span class="string">"/wiki/Python_(programming_language)"</span>)</div><div class="line"></div><div class="line"><span class="keyword">while</span>(len(links) &gt; <span class="number">0</span>):</div><div class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> links:</div><div class="line">        print(<span class="string">"----------------"</span>)</div><div class="line">        historyIPs = getHistoryIPs(link.attrs[<span class="string">"href"</span>])</div><div class="line">        <span class="keyword">for</span> historyIP <span class="keyword">in</span> historyIPs:</div><div class="line">            country = getCountry(historyIP)</div><div class="line">            <span class="keyword">if</span> country <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">                print(historyIP + <span class="string">"is from "</span> + country)</div><div class="line"></div><div class="line">    newLink = links[random.randint(<span class="number">0</span>,len(links)<span class="number">-1</span>)].attrs[<span class="string">"href"</span>]</div><div class="line">    links = getLinks(newLink)</div></pre></td></tr></table></figure>
<hr>
<h2 id="Download-Page-Source"><a href="#Download-Page-Source" class="headerlink" title="Download Page Source"></a>Download Page Source</h2><p> 下面的程序将 <a href="http://pythonscraping.com" target="_blank" rel="external">http://pythonscraping.com</a> 主页上所有 src 属性的文件都下载下来，然后对 URL 链接进行清理和标准化，获得文件对绝对路径（而且去掉了外链）。最后，每个文件都会下载到程序所在文件夹到 downloaded 文件里：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlretrieve</div><div class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line">download_directory = <span class="string">"downloaded"</span></div><div class="line">base_url = <span class="string">"http://pythonscraping.com"</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAbsolute_url</span><span class="params">(base_url,source)</span>:</span></div><div class="line">    <span class="keyword">if</span> source.startswith(<span class="string">"http://www."</span>):</div><div class="line">        url = <span class="string">"http://"</span> + source[<span class="number">11</span>:]</div><div class="line">    <span class="keyword">elif</span> source.startswith(<span class="string">"http://"</span>):</div><div class="line">        url = source</div><div class="line">    <span class="keyword">elif</span> source.startswith(<span class="string">"www."</span>):</div><div class="line">        url = source[<span class="number">4</span>:]</div><div class="line">        url = <span class="string">"http://"</span> + source</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        url = base_url + <span class="string">"/"</span> + source</div><div class="line">    <span class="keyword">if</span> base_url <span class="keyword">not</span> <span class="keyword">in</span> url:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">None</span></div><div class="line">    <span class="keyword">return</span> url</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getDownloadPath</span><span class="params">(base_url, absolute_url, download_directory)</span>:</span></div><div class="line">    path = absolute_url.replace(<span class="string">"www"</span>,<span class="string">""</span>)</div><div class="line">    path = path.replace(base_url,<span class="string">""</span>)</div><div class="line">    path = download_directory + path</div><div class="line">    directory = os.path.dirname(path)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(directory):</div><div class="line">        os.makedirs(directory)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> path</div><div class="line"></div><div class="line">html = urlopen(<span class="string">"http://www.pythonscraping.com"</span>)</div><div class="line">bsObj = BeautifulSoup(html.read(),<span class="string">"html5lib"</span>)</div><div class="line">downloadList = bsObj.findAll(src=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> download <span class="keyword">in</span> downloadList:</div><div class="line">    file_url = getAbsolute_url(base_url, download[<span class="string">"src"</span>])</div><div class="line">    <span class="keyword">if</span> file_url <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        print(file_url)</div><div class="line"></div><div class="line">urlretrieve(file_url,getDownloadPath(base_url,file_url,download_directory))</div></pre></td></tr></table></figure>
<hr>
<h2 id="Storing-Data-to-CSV"><a href="#Storing-Data-to-CSV" class="headerlink" title="Storing Data to CSV"></a>Storing Data to CSV</h2><blockquote>
<p>CSV, or comma-separated values, is one of the most popular file formats in which to store spreadsheet data. It is supported by Microsoft Excel and many other applica‐ tions because of its simplicity. The following is an example of a perfectly valid CSV file: </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">fruit,cost</div><div class="line">apple,1.00</div><div class="line">banana,0.30</div><div class="line">pear,1.25</div></pre></td></tr></table></figure>
<p> 网络数据采集的一个常用功能就是获取 HTML 表格并写入 CSV 文件。</p>
<hr>
<h1 id="Need-to-Know"><a href="#Need-to-Know" class="headerlink" title="Need to Know:"></a>Need to Know:</h1><h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><p><a href="http://dev.mysql.com/downloads/mysql/" target="_blank" rel="external"><strong>The download page</strong></a></p>
<p> 下载 .dmg 安装包，在 MySQL5.7.x 版本之后，安装的时候会随机分配一个初始密码！这非常重要，例如 root@localhost: <strong>;,aLs&amp;%%4ziE</strong> 密码很复杂，最好先复制下来，等会更改密码的时候需要用到。</p>
<p> 安装完成之后，可以在系统偏好设置中看到多出了一个 MySQL，我们可以通过其来开关 MySQL 服务器，当然我们可以通过命令行输入来控制。<br> 打开服务器，在命令行输入：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">alias</span> mysql=/usr/<span class="built_in">local</span>/mysql/bin/mysql</span></div><div class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">alias</span> mysqladmin=/usr/<span class="built_in">local</span>/mysql/bin/mysqladmin</span></div></pre></td></tr></table></figure>
<p>ps: 注意，这上面 alias 别名的方法，只是一次性的，意味着我们关闭了终端之后再开，命令行直接输入 mysql 或者 mysqladmin 就无效了。如果需要长期有效，需要修改文件，让终端启动的时候加载。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> ~</span></div><div class="line"><span class="meta">$</span><span class="bash"> vim ./bash_profile</span></div></pre></td></tr></table></figure>
<p><strong> 注意：如果你安装了 oh-my-zsh，需要去更改 .zshrc 文件。</strong></p>
<p> 然后更改密码，命令行输入：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span><span class="bash"> mysqladmin - u root -p password xxx(我们需要的新密码)</span></div></pre></td></tr></table></figure>
<p> 确保 MySQL 服务器打开，然后命令输入：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span><span class="bash"> mysql -u root -p</span></div></pre></td></tr></table></figure>
<p> 若未显示错误，则表示连接上数据库了 </p>
<hr>
<h2 id="Integrating-with-Python"><a href="#Integrating-with-Python" class="headerlink" title="Integrating with Python"></a>Integrating with Python</h2><p>Python 没有内置的 MySQL 支持工具。不过，有很多开源的库可以用来与 MySQL 做交互，Python2.x 和 Python3.x 版本都支持。最有名的一个库就是 PyMySQL。</p>
<p> 我是在 PyCharm 直接安装 PyMySQL，安装完成之后，如果我们的 MySQL 的服务器处于运行状态，应该就可以使用 PyMySQL 包。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pymysql.conn = pymysql.connect(host=<span class="string">'127.0.0.1'</span>, unix_socket=<span class="string">'/tmp/mysql.sock'</span>, </div><div class="line"> 			user=<span class="string">'root'</span>, passwd=<span class="string">'xxxx'</span>, db=<span class="string">'mysql'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>cur = conn.cursor()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>cur.execute(<span class="string">"USE scraping"</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>cur.execute(<span class="string">"SELECT * FROM pages WHERE id=1"</span>) </div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(cur.fetchone())cur.close().conn.close()</div></pre></td></tr></table></figure>
<ol>
<li> 程序中有两个对象：连接对象 <strong><code>conn</code></strong> 和光标对象 <strong><code>cur</code></strong>。</li>
<li> 连接 / 光标模式是数据库编程中常见的模式。连接模式除了要连接数据库之外，还要发送数据库信息，处理回滚操作（当一个查询或一组查询被中断时，数据库需要回到初始状态，一般用事务控制手段实现状态会滚），创建新的光标对象，等等。</li>
<li><strong> 而一个 <code>conn</code> 可以有很多个 <code>cur</code></strong>。一个光标跟踪一种状态信息，比如跟踪数据库的使用状态。如果你有多个数据库，且需要向所有数据库写内容，就需要多个光标来处理。光标还包含最后一次查询执行的结果。通过调用光标函数，比如 <strong><code>cur.fetchone()</code></strong>，可以获取查询结果。</li>
<li> 用完光标和链接之后，千万记得要把它们关闭。如果不关闭就会导致连接泄漏（<strong>connection leak</strong>），造成一种未关闭连接的现象，即连接已经不在使用，但是数据库却不能关闭，因为数据库不能确定你还要不要继续使用它。这种现象会一直耗费数据库的资源，所以用完数据库之后记得关闭连接！</li>
<li> 进行网络数据采集的时候，处理 Unicode 字符串是很痛苦的事情。默认情况下，MySQL 也不支持 Unicode 字符处理。不过我们可以设置这个功能，因为采集的时候，我们难免会遇到各种各样的字符，所以最好一开始就让我们的数据库支持 Unicode：</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">ALTER</span> <span class="keyword">DATABASE</span> scraping <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> = utf8mb4 <span class="keyword">COLLATE</span> = utf8mb4_unicode_ci; </div><div class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> pages <span class="keyword">CONVERT</span> <span class="keyword">TO</span> <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8mb4 <span class="keyword">COLLATE</span> utf8mb4_unicode_ci; </div><div class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> pages <span class="keyword">CHANGE</span> title title <span class="built_in">VARCHAR</span>(<span class="number">200</span>) <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8mb4 <span class="keyword">COLLATE</span> utf8mb4_unicode_ci; </div><div class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> pages <span class="keyword">CHANGE</span> <span class="keyword">content</span> <span class="keyword">content</span> <span class="built_in">VARCHAR</span>(<span class="number">10000</span>) <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8mb4 <span class="keyword">COLLATE</span> utf8mb4_unicode_ci;</div></pre></td></tr></table></figure>
<p> 我们尝试用下面的程序来存储数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> datetime</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> pymysql</div><div class="line"></div><div class="line">conn = pymysql.connect(host=<span class="string">'127.0.0.1'</span>, unix_socket=<span class="string">'/tmp/mysql.sock'</span>,</div><div class="line">                       user=<span class="string">'root'</span>,passwd=<span class="string">'randolph'</span>,db=<span class="string">'mysql'</span>,charset=<span class="string">'utf8'</span>)</div><div class="line"></div><div class="line">cur = conn.cursor()</div><div class="line">cur.execute(<span class="string">"USE scraping"</span>)</div><div class="line"></div><div class="line">random.seed(datetime.datetime.now())</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">store</span><span class="params">(title, content)</span>:</span></div><div class="line">    cur.execute(<span class="string">"INSERT INTO pages(title, content) VALUE (\"%s\",\"%s\")"</span>,(title,content))</div><div class="line">    cur.connection.commit()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getLinks</span><span class="params">(article_url)</span>:</span></div><div class="line">    html = urlopen(<span class="string">"http://en.wikipedia.org"</span>+article_url)</div><div class="line">    bsObj = BeautifulSoup(html.read(),<span class="string">"html5lib"</span>)</div><div class="line">    title = bsObj.find(<span class="string">"h1"</span>).get_text()</div><div class="line">    content = bsObj.find(<span class="string">"div"</span>, &#123;<span class="string">"id"</span>:<span class="string">"mw-content-text"</span>&#125;).find(<span class="string">"p"</span>).get_text()</div><div class="line">    store(title,content)</div><div class="line">    <span class="keyword">return</span> bsObj.find(<span class="string">"div"</span>,&#123;<span class="string">"id"</span>:<span class="string">"bodyContent"</span>&#125;).findAll(<span class="string">"a"</span>,href=re.compile(<span class="string">"^(/wiki/)(?!:).)*$"</span>))</div><div class="line"></div><div class="line">links = getLinks(<span class="string">"/wiki/Kevin_Bacon"</span>)</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    <span class="keyword">while</span> len(links) &gt; <span class="number">0</span>:</div><div class="line">        newArticle = links[random.randint(<span class="number">0</span>, len(links)<span class="number">-1</span>)].attrs[<span class="string">"href"</span>]</div><div class="line">        print(newArticle)</div><div class="line">        links = getLinks(newArticle)</div><div class="line"></div><div class="line"><span class="keyword">finally</span>:</div><div class="line">    cur.close()</div><div class="line">    conn.close()</div></pre></td></tr></table></figure>
<ul>
<li> 需要注意的是 <strong><code>store()</code></strong> 函数，它有两个参数：<strong><code>title</code></strong> 和 <strong><code>content</code></strong>，并把这两个参数加到了一个 INSERT 语句中并用光标执行，然后用光标进行连接确认。这是一个让光标与连接操作分离的好例子；当光标里存储了一些数据库与数据库上下文的信息时，需要通过连接的确认操作先将信息传进数据库，再将信息插入数据库。</li>
<li> 最后需要注意的是 finally 语句是在程序主循环的外面，代码的最底下。这样做可以保证，无论程序执行过程中如何发生中断或抛出异常（当然，因为网络很复杂，我们需要随时准备遭遇异常），光标和连接都会在程序结束前立即关闭。无论我们是在采集网络还是在处理一个打开连接的数据库，用 <strong><code>try...finally</code></strong> 都是一个好主意。</li>
</ul>
<hr>
<h1 id="Correct-errors-in-printing"><a href="#Correct-errors-in-printing" class="headerlink" title="Correct errors in printing:"></a>Correct errors in printing:</h1><ul>
<li><strong> 暂无 </strong></li>
</ul>
<hr>
<h1 id="Practice"><a href="#Practice" class="headerlink" title="Practice:"></a>Practice:</h1><ul>
<li><strong> 暂无 </strong></li>
</ul>
]]></content>
    
    <summary type="html">
    
      本文是关于「Web Scraping with Python」这本书的 Chapter 4 &amp; 5 的学习笔记。
    
    </summary>
    
      <category term="Web Scraping" scheme="http://randolph.pro/categories/Web-Scraping/"/>
    
      <category term="Book:「Web Scraping with Python」" scheme="http://randolph.pro/categories/Web-Scraping/Book-%E3%80%8CWeb-Scraping-with-Python%E3%80%8D/"/>
    
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
      <category term="Web Scraping" scheme="http://randolph.pro/tags/Web-Scraping/"/>
    
  </entry>
  
  <entry>
    <title>♞「Web Scraping with Python」 Chapter 3</title>
    <link href="http://randolph.pro/2015/11/13/%E2%99%9E%E3%80%8CWeb%20Scraping%20with%20Python%E3%80%8D%20Chapter%203/"/>
    <id>http://randolph.pro/2015/11/13/♞「Web Scraping with Python」 Chapter 3/</id>
    <published>2015-11-12T16:00:00.000Z</published>
    <updated>2017-08-06T13:18:07.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4316/36049777032_f975ed0941_o.jpg" alt=""></p>
<p> 有关该书的其他学习笔记系列：<a href="http://randolph.pro/categories/Web-Scraping/Book-「Web-Scraping-with-Python」/">Book:「Web Scraping with Python」</a></p>
<h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><ul>
<li><strong>urlparse module</strong></li>
</ul>
<hr>
<h1 id="Key"><a href="#Key" class="headerlink" title="Key:"></a>Key:</h1><h2 id="Think-First"><a href="#Think-First" class="headerlink" title="Think First"></a>Think First</h2><blockquote>
<p>What data am I trying to gather? Can this be accomplished by scraping just a few predefined websites (almost always the easier option), or does my crawler need to be able to discover new websites I might not know about?<br> 我需要收集哪些数据？这些数据可以通过采集几个已经确定的网站（永远是最简单的做法）完成吗？或者我需要通过爬虫发现那些我可能不知道的网站从而获取我想要的信息吗？</p>
<p>When my crawler reaches a particular website, will it immediately follow the next outbound link to a new website, or will it stick around for a while and drill down into the current website?<br> 当我的爬虫到了某一个网站，它是立即顺着下一个出站链接跳转到下一个新网站，还是在网站上呆一会，深入采集网站的内容？</p>
<p>Are there any conditions under which I would not want to scrape a particular site? Am I interested in non-English content?<br> 有没有我不想采集的一些网站？我对非英文网站的内容感兴趣么？</p>
<p>How am I protecting myself against legal action if my web crawler catches the attention of a webmaster on one of the sites it runs across?<br> 如果我的爬虫引起了某个网站网管的怀疑，我该如何避免法律责任？</p>
</blockquote>
<hr>
<h2 id="urlparse-module"><a href="#urlparse-module" class="headerlink" title="urlparse module"></a>urlparse module</h2><p><strong>urlparse</strong> 模块主要是把 url 拆分为六个部分，并返回元组 tuple。并且可以把拆分后的部分再组成一个 url。主要函数有 <strong><code>urljoin</code></strong>、<strong><code>urlsplit</code></strong>、<strong><code>urlunsplit</code></strong>、<strong><code>urlparse</code></strong> 等。</p>
<h3 id="urlparse-function"><a href="#urlparse-function" class="headerlink" title="urlparse function"></a>urlparse function</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; from urlparse import urlparse</div><div class="line">&gt;&gt;&gt; o = urlparse(<span class="string">'http://www.cwi.nl:80/%7Eguido/Python.html'</span>)</div><div class="line">&gt;&gt;&gt; o    </div><div class="line">    ParseResult(scheme=<span class="string">'http'</span>, netloc=<span class="string">'www.cwi.nl:80'</span>, path=<span class="string">'/%7Eguido/Python.html'</span>, params=<span class="string">''</span>, query=<span class="string">''</span>, fragment=<span class="string">''</span>)</div><div class="line">&gt;&gt;&gt; o.scheme  </div><div class="line">    <span class="string">'http'</span></div><div class="line">&gt;&gt;&gt; o.port  </div><div class="line">    80</div><div class="line">&gt;&gt;&gt; o.geturl()  </div><div class="line">    <span class="string">'http://www.cwi.nl:80/%7Eguido/Python.html'</span></div></pre></td></tr></table></figure>
<p> 其将 url 解析成六个部分 <strong>（scheme, netloc, path, parameters, query, fragment）</strong>。</p>
<hr>
<h2 id="scrapy"><a href="#scrapy" class="headerlink" title="scrapy"></a>scrapy</h2><blockquote>
<p>Scrapy uses the Item objects to determine which pieces of information it should save from the pages it visits. This information can be saved by Scrapy in a variety of ways, such as a CSV, JSON, or XML files, using the following commands:<br>Scrapy 用 Item 对象决定要从它浏览的页面中提取哪些信息。Scrapy 支持用不同的输出格式来保存这些信息，比如 CSV、JSON、XML 文件格式，对应命令如下：</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ scrapy crawl article -o articles.csv -t csv</div><div class="line">$ scrapy crawl article -o articles.json -t json</div><div class="line">$ scrapy crawl article -o articles.xml -t xml</div></pre></td></tr></table></figure>
<p> 当然我们也可以自己定义 Item 对象，把结果写入我们需要的一个文件或者数据库中，只要在爬虫的 parse 部分增加相应的代码即可。<br>Scrapy 是处理网络数据采集相关问题的利器。它可以自动收集所有 URL，然后和指定的规则进行比较；确保所有的 URL 是唯一的；根据需求对相关的 URL 进行标准化；以及到更深层的页面中递归查询。</p>
<hr>
<h1 id="Need-to-know"><a href="#Need-to-know" class="headerlink" title="Need to know:"></a>Need to know:</h1><p> 在「用 Scrapy 采集」的模块中：</p>
<p> 我们需要下载 <strong><code>scrapy</code></strong> 这一个 package。<strong>「 这个 package 不支持 python3.x 和 python2.6，只能使用 python2.7 。」</strong></p>
<p> 我的尝试：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo pip install scrapy</div><div class="line">Could not find <span class="keyword">function</span> xmlCheckVersion <span class="keyword">in</span> library libxml2. Is libxml2 installed? </div><div class="line">Perhaps try: xcode-select --install</div></pre></td></tr></table></figure>
<p> 意思是缺少 <strong><code>libxml2</code> </strong>，通过命令行输入:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ xcode-select --install</div></pre></td></tr></table></figure>
<p> 接着会弹出 Xcode command line tools 下载，里面包含了 <strong><code>libxml2</code></strong>。安装完成之后，再次尝试 <strong><code>sudo pip install scrapy</code></strong>，报错，内容为:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; from six.moves import xmlrpc_client as xmlrpclib</div><div class="line">ImportError: cannot import name xmlrpc_client</div></pre></td></tr></table></figure>
<p> 在 stackoverflow 上寻找原因:</p>
<blockquote>
<ul>
<li><p><strong>six.moves</strong> is a virtual namespace. It provides access to packages that were renamed between Python 2 and 3. As such, you shouldn’t be installing anything.</p>
</li>
<li><p>By importing from six.moves.xmlrpc_client the developer doesn’t have to handle the case where it is located at xmlrpclib in Python 2, and at xmlrpc.client in Python 3. Note that these are part of the standard library.</p>
</li>
<li><p>The mapping was added to six version 1.5.0; make sure you have that version or newer.</p>
</li>
<li><p>Mac comes with six version 1.4.1 pre-installed in the path:  </p>
<p><strong>/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python</strong> </p>
<p>and this will interfere with any version you install in site-packages (which is listed last in the sys.path).</p>
<p>The best work-around is to use a virtualenv and install your own version of six into that, together with whatever else you need for this project. Create a new virtualenv for new projects.</p>
</li>
<li><p>If you absolutely have to install this at the system level, then for this specific project you’ll have to remove the /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python path:</p>
</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import sys</div><div class="line">&gt;&gt;&gt; sys.path.remove(<span class="string">'/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python'</span>)</div></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>This will remove various OS X-provided packages from your path for just that run of Python; Apple installs these for their own needs.</li>
</ul>
</blockquote>
<p>Mac 自带的 <strong><code>six</code></strong> 版本过低，<strong><code>scrapy</code></strong> 需要 <strong><code>six</code></strong> 的版本在 1.5.0 以上，建议是采用 Python 虚拟环境，如果真的需要在 system level 上进行更改的话，需要重新安装 <strong><code>six</code></strong>。<br> 于是，我先尝试了其中的一个解决办法：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ sudo rm -rf /Library/Python/2.7/site-packages/six*</div><div class="line">$ sudo rm -rf </div><div class="line">/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six*</div><div class="line">$ sudo pip install six</div></pre></td></tr></table></figure>
<p> 但很不幸的是，<strong><code>sudo rm -rf</code></strong> 尝试删除文件的时候失败报错，Operation not Permitted。<br> 继续查找原因：</p>
<blockquote>
<ul>
<li>This is because OS X El Capitan ships with six 1.4.1 installed already and when it attempts to uninstall it (because scrapy depends on six &gt;= 1.5) it doesn’t have permission to do so because <strong>System Integrity Protection</strong> doesn’t allow even root to modify those directories.</li>
<li>Ideally, pip should just skip uninstalling those items since they aren’t installed to site-packages they are installed to a special Apple directory. However, even if pip skips uninstalling those items and installs six into site-packages we’ll hit another bug where Apple puts their pre-installed stuff earlier in the sys.path than site-packages. I’ve talked to Apple about this and I’m not sure if they’re going to do anything about it or not.</li>
</ul>
</blockquote>
<p> 我的 Mac OS X 系统版本为 10.11.4，Mac 自版本 10.11 之后，由于新的 SIP 机制，即使是 root 用户也无法对 /System 中的内容进行修改删除（在系统恢复中可以办到）。</p>
<p> 于是，我采用另外一种方法继续尝试：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo pip uninstall six</div><div class="line">$ easy_install six</div></pre></td></tr></table></figure>
<p> 同样得到的是 Operation not Permitted（此方法在 10.11 之前的版本应该都可以行得通）。</p>
<p> 后来尝试了通过 Python 虚拟环境进行解决，能力不够失败。<br> 还尝试了通过下载 Python 官网的 2.7.11，不使用 Mac 系统默认自带的 2.7.10（有人提到使用自己安装的 Python2.7 可以解决问题），折腾了半天，还是失败告终，还差点弄的 pip 无法安装 package。挽救办法为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ brew link python </div><div class="line">$ bre unlink python</div></pre></td></tr></table></figure>
<p> 到最后，本来想着要放弃的，Stackoverflow 上的另一个办法让事情有了转机：</p>
<blockquote>
<p>This is a known issue on Mac OSX for Scrapy. You can refer to <a href="https://github.com/pypa/pip/issues/3165" target="_blank" rel="external">this link</a>.<br>Basically the issue is with the <strong>PYTHONPATH</strong> in your system. To solve the issue change the current PYTHONPATH to point to the newer or none Mac OSX version of Python. Before running Scrapy, try:</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">export</span> PYTHONPATH=/Library/Python/2.7/site-packages:<span class="variable">$PYTHONPATH</span></div></pre></td></tr></table></figure>
<blockquote>
<p>If that worked you can change the .bashrc file permanently:</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">echo</span> <span class="string">"export PYTHONPATH=/Library/Python/2.7/site-packages:<span class="variable">$PYTHONPATH</span>"</span> &gt;&gt; ~/.bashrc</div></pre></td></tr></table></figure>
<blockquote>
<p>If none of this works, take a look at the link above.</p>
</blockquote>
<p> 此时命令行输入 python，之后输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import scrapy</div></pre></td></tr></table></figure>
<p> 没有报错，说明可以导入 scrapy。</p>
<p> 尝试书上的命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ scrapy startproject wikiSpider</div></pre></td></tr></table></figure>
<p> 得到信息：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">New Scrapy project <span class="string">'wikiSpider'</span> created <span class="keyword">in</span>:</div><div class="line">	/Users/randolph/PycharmProjects/Scraping/wikiSpider</div><div class="line">You can start your first spider with:</div><div class="line">	<span class="built_in">cd</span> wikiSpider</div><div class="line">	scrapy genspider example example.com</div></pre></td></tr></table></figure>
<p> 成功！scrapy is ready to go!</p>
<hr>
<h1 id="Correct-errors-in-printing"><a href="#Correct-errors-in-printing" class="headerlink" title="Correct errors in printing:"></a>Correct errors in printing:</h1><ul>
<li><strong> 暂无 </strong></li>
</ul>
<hr>
<h1 id="Practice"><a href="#Practice" class="headerlink" title="Practice:"></a>Practice:</h1><ul>
<li><strong> 暂无 </strong></li>
</ul>
<hr>
]]></content>
    
    <summary type="html">
    
      本文是关于「Web Scraping with Python」这本书的 Chapter 3 的学习笔记。
    
    </summary>
    
      <category term="Web Scraping" scheme="http://randolph.pro/categories/Web-Scraping/"/>
    
      <category term="Book:「Web Scraping with Python」" scheme="http://randolph.pro/categories/Web-Scraping/Book-%E3%80%8CWeb-Scraping-with-Python%E3%80%8D/"/>
    
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
      <category term="Web Scraping" scheme="http://randolph.pro/tags/Web-Scraping/"/>
    
  </entry>
  
  <entry>
    <title>♞「Web Scraping with Python」 Chapter 1 &amp; 2</title>
    <link href="http://randolph.pro/2015/11/02/%E2%99%9E%E3%80%8CWeb%20Scraping%20with%20Python%E3%80%8D%20Chapter%201%20&amp;%202/"/>
    <id>http://randolph.pro/2015/11/02/♞「Web Scraping with Python」 Chapter 1 &amp; 2/</id>
    <published>2015-11-01T16:00:00.000Z</published>
    <updated>2017-08-06T13:18:11.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4316/36049777032_f975ed0941_o.jpg" alt=""></p>
<p>有关该书的其他学习笔记系列：<a href="http://randolph.pro/categories/Web-Scraping/Book-「Web-Scraping-with-Python」/">Book:「Web Scraping with Python」</a></p>
<h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><ul>
<li><strong>BeautifulSoup package</strong></li>
<li><strong>Navigating Trees</strong> </li>
<li><strong>Regular Expression</strong></li>
</ul>
<hr>
<h1 id="Key"><a href="#Key" class="headerlink" title="Key:"></a>Key:</h1><h2 id="urlib-or-urlib2"><a href="#urlib-or-urlib2" class="headerlink" title="urlib or urlib2?"></a>urlib or urlib2?</h2><blockquote>
<p>If you’ve used the urllib2 library in Python 2.x, you might have noticed that things have changed somewhat between urllib2 and urllib. In Python 3.x, urllib2 was renamed urllib and was split into several submodules: urllib.request, urllib.parse, and url lib.error. Although function names mostly remain the same, you might want to note which functions have moved to submodules when using the new urllib. </p>
</blockquote>
<p>在学习这本书之前，使用过此 package（我一开始学习 Python 就用的是 3.x，Mac 自带 Python2.x），当时出错了，上 Stackoverflow 找到了答案，现在这本书提到了这点，重新回顾一下:</p>
<p>如果你用过 Python 2.x 里的 <strong><code>urllib2</code></strong> 库，可能会发现 <strong><code>urllib2</code></strong> 与 <strong><code>urllib</code></strong> 有些不同。在 Python 3.x 里，<strong><code>urllib2</code></strong> 改名为 <strong><code>urllib</code></strong>，被分成一些子模块：<strong><code>urllib.request</code></strong>、<strong><code>urllib.parse</code></strong> 和 <strong><code>urllib.error</code></strong>。尽管函数名称大多和原来一样，但是在用新的 <strong><code>urllib</code></strong> 库时需要注意哪些函数被移动到子模块里了。</p>
<hr>
<h2 id="When-to-get-text-and-When-to-Preserve-Tags"><a href="#When-to-get-text-and-When-to-Preserve-Tags" class="headerlink" title="When to get_text() and When to Preserve Tags?"></a>When to get_text() and When to Preserve Tags?</h2><blockquote>
<p>.get_text() strips all tags from the document you are working with and returns a string containing the text only. For example, if you are working with a large block of text that contains many hyperlinks, paragraphs, and other tags, all those will be stripped away and you’ll be left with a tagless block of text.</p>
<p>Keep in mind that it’s much easier to find what you’re looking for in a BeautifulSoup object than in a block of text. Calling .get_text() should always be the last thing you do, immediately before you print, store, or manipulate your final data. In general, you should try to preserve the tag structure of a document as long as possible.</p>
</blockquote>
<p>简而言之，通常在我们准备打印、存储和操作数据的时候，即最后的时候才使用 <strong><code>.get_text()</code></strong>。一般情况下，我们应该尽可能地保留 HTML 文档的标签结构。</p>
<hr>
<h2 id="find-and-findAll-with-BeautifulSoup"><a href="#find-and-findAll-with-BeautifulSoup" class="headerlink" title="find() and findAll() with BeautifulSoup?"></a>find() and findAll() with BeautifulSoup?</h2><ul>
<li><strong>findAll(tag, attributes, recursive, text, limit, keywords)</strong></li>
<li><strong>find(tag, attributes, recursive, text, keywords)</strong></li>
</ul>
<p>先说结论，再仔细说说参数的用法。<br><strong><code>find()</code></strong> is equivalent to the same <strong><code>findAll()</code></strong> call, with a <strong>limit</strong> of  1.<br><strong><code>find()</code></strong> 其实等价于 <strong><code>findAll()</code></strong> 的 limit 等于 1 时的特殊情况。 </p>
<ul>
<li><strong><code>tag</code></strong>: 我们可以传一个标签的名称或多个标签名称组成的 Python 列表做标签参数。例如：<strong><code>(”span”, “h1” , {“span”, “h1”}, {“h1”, “h2”, “h3”})</code></strong>。其实就是一个「或」关系的过滤器（即我们可以选择带有 <strong><code>span</code></strong> 或 <strong><code>h1</code></strong> 或 <strong><code>h2</code></strong> 等的一列标签）。</li>
<li><strong><code>attributes</code></strong>: 这是一个用 Python 字典封装某一标签的若干属性和对应的属性值。例如：<strong><code>{“class”: {“green”, “red”}}</code></strong></li>
<li><strong><code>recursive</code></strong>: 一般情况下，这个参数不需要设置，除非我们真正了解自己需要哪些信息，而且抓取速度非常重要，因为这个参数会根据我们的要求去查找标签参数的所有子标签，以及子标签的子标签。</li>
<li><strong><code>limit</code></strong>: 只适用于 <strong><code>findAll()</code></strong> 方法，如果我们只对网页中获取的前 <em>x</em> 项结果感兴趣，我们就可以通过设置 <strong><code>limit</code></strong> 来获取。<strong>但是需要注意的是：获得的前几项结果是按照网页上的顺序排序的，未必是我们想要的前几项，所以我们还需要额外做一些自己的排序。</strong></li>
<li><strong><code>keyword</code></strong>: 使我们选择那些具有制定属性的标签成为可能。</li>
</ul>
<blockquote>
<p><strong>keyword 关键词参数的主意事项：</strong><br>使用 <strong><code>keyword</code></strong> 偶尔会出现问题，尤其是在用 <strong>class</strong> 属性查找标签的时候，因为 <strong>class</strong> 是 Python 中受保护的关键字。也就是说，<strong>class</strong> 是 Python 语言的保留字，在 Python 程序中是不能充当变量或者参数名使用的。假如我们运行下面的代码，Python 就会因为我们误用 <strong>class</strong> 保留字而产生一个语法错误：</p>
<p>   <strong><code>bsObj.findAll(class=&quot;green&quot;)</code></strong></p>
<p>不过 BeautifulSoup 提供了一个解决方案，就是在 class 后面增加一个下划线：<br>   <strong><code>bsObj.findAll(class_=&quot;green&quot;)</code></strong><br>我们也可以使用属性参数来将 <strong>class</strong> 用引号包起来：<br>   <strong><code>bsObj.findAll(&quot;&quot;,{&quot;class&quot;: &quot;green&quot;})</code></strong></p>
</blockquote>
<p><strong>另外，如果说 <code>tag</code> 参数是相当于一个「或」关系的过滤器，那么 <code>keyword</code> 参数就可以为我们构造一个「与」关系的过滤器来提高我们的工作效率，简化我们的工作。</strong></p>
<hr>
<h2 id="Navigating-Trees"><a href="#Navigating-Trees" class="headerlink" title="Navigating Trees"></a>Navigating Trees</h2><p>如果说 <strong><code>find()</code></strong> 和 <strong><code>findAll()</code> </strong> 函数是通过标签的名称和属性来查找标签，那么 Navigating Trees 就是通过标签在文档中的位置来查找标签。</p>
<h3 id="Make-Selections-Specific"><a href="#Make-Selections-Specific" class="headerlink" title="Make Selections Specific"></a>Make Selections Specific</h3><blockquote>
<p>To make your scrapers more robust, it’s best to be as specific as pos‐ sible when making tag selections. Take advantage of tag attributes when they are available. </p>
</blockquote>
<p>如果想让我们的爬虫更加稳定，最好还是让标签的选择更加具体。如果有属性，就利用标签的属性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">bsObj.tr</div><div class="line">bsObj.table.tr</div><div class="line">bsObj.find(<span class="string">"table"</span>,&#123;<span class="string">"id"</span>:<span class="string">"giftList"</span>&#125;).tr</div></pre></td></tr></table></figure>
<p>上述三行目的都是获取书本上的网站中的表格的第一行。但是我们应该采用最后一条，用更具体的形式来获取，原因很简单，即使页面上只有一个表格（或者其他的目标标签），只用标签也很容易丢失细节。另外，页面的布局总是不断变化的，一个标签这次是在表格中的第一行的位置，没准哪天就在第二行或者第三行了。</p>
<hr>
<h2 id="Regular-Expression"><a href="#Regular-Expression" class="headerlink" title="Regular Expression"></a>Regular Expression</h2><p>学好正则表达式，走遍天下都不怕。正则表达式其实就是一个过滤器，如果你给我的字符串符合我写的规则，那么我就返回它。    </p>
<p>让我们来看看用正则表达式来表示邮箱地址：</p>
<p><strong>[A-Za-z0-9\._+]+@[A-Za-z]+\.(com|org|edu|net)</strong></p>
<p>让我们把它分解开来看：</p>
<ol>
<li><strong>[A-Za-z0-9\._+]+</strong> : 这个表达式把所有可能的序列和符号放在中括号（而不是小括号）里面，表示“括号中的符号里任何一个”。另外注意，后面的加号表示“这些符号都可以出现多次，而且至少出现一次”。</li>
<li><strong>@</strong>：这个符号很直接，出现在中间位置，有且仅有一次。</li>
<li><strong>[A-Za-z]+</strong> ：可能出现在域名的前半部分、符号 @后面用字母。而且，至少有一个字母。</li>
<li><strong>.</strong> : 域名前必须有一个点号。</li>
<li><strong>(com|org|edu|net)</strong> : 顶级域名可能有很多种，但是作为参考，这是个后缀够用了。</li>
</ol>
<hr>
<h1 id="Correct-errors-in-printing"><a href="#Correct-errors-in-printing" class="headerlink" title="Correct errors in printing:"></a>Correct errors in printing:</h1><ul>
<li>P17:</li>
</ul>
<blockquote>
<p>例如，tr 标签是 <strong>tabel</strong> 标签的子标签，而……</p>
</blockquote>
<p>需要更正为：</p>
<blockquote>
<p>例如，tr 标签是 <strong>table</strong> 标签的子标签，而……</p>
</blockquote>
<hr>
<h1 id="Practice"><a href="#Practice" class="headerlink" title="Practice:"></a>Practice:</h1><ul>
<li><strong>暂无</strong></li>
</ul>
<hr>
]]></content>
    
    <summary type="html">
    
      本文是关于「Web Scraping with Python」这本书的 Chapter 1 &amp; 2的学习笔记。
    
    </summary>
    
      <category term="Web Scraping" scheme="http://randolph.pro/categories/Web-Scraping/"/>
    
      <category term="Book:「Web Scraping with Python」" scheme="http://randolph.pro/categories/Web-Scraping/Book-%E3%80%8CWeb-Scraping-with-Python%E3%80%8D/"/>
    
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
      <category term="Web Scraping" scheme="http://randolph.pro/tags/Web-Scraping/"/>
    
  </entry>
  
  <entry>
    <title>♞「NLP with Python」 Chapter 3</title>
    <link href="http://randolph.pro/2015/09/17/%E2%99%9E%E3%80%8CNLP%20with%20Python%E3%80%8D%20Chapter%203/"/>
    <id>http://randolph.pro/2015/09/17/♞「NLP with Python」 Chapter 3/</id>
    <published>2015-09-16T16:00:00.000Z</published>
    <updated>2017-08-14T12:57:35.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4423/36425940061_fe957aaf15_o.jpg" alt=""></p>
<p>有关该书的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/Natural-Language-Processing/Book-「NLP-with-Python」/">Book:「NLP with Python」</a></p>
<h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><ul>
<li><strong>The NLP Pipeline</strong></li>
<li><strong>Basic Operations with Strings</strong></li>
<li><strong>Regular Expressions for Detecting Word Patterns</strong></li>
<li><strong>Finding Word Stems</strong></li>
<li><strong>Searching Tokenized Text</strong></li>
<li><strong>Normalizing Text</strong></li>
<li><strong>Word Segmentation</strong></li>
<li><strong>Formatting: From Lists to Strings</strong></li>
</ul>
<hr>
<h1 id="Key"><a href="#Key" class="headerlink" title="Key:"></a>Key:</h1><h2 id="The-NLP-Pipeline"><a href="#The-NLP-Pipeline" class="headerlink" title="The NLP Pipeline"></a>The NLP Pipeline</h2><p>NLP 的处理流程：我们打开一个 URL 代码读取里面 HTML 格式的内容，去除标记，并选择字符的切片，然后分词，是否转换为 <strong><code>nltk.Text</code></strong> 对象是可选择的。我们也可以将所有词汇小写并提取成词汇表（Vocab）。</p>
<p><img src="https://farm1.staticflickr.com/645/30825223223_8abc614f13_o.png" alt=""></p>
<hr>
<h2 id="Basic-Operations-with-Strings"><a href="#Basic-Operations-with-Strings" class="headerlink" title="Basic Operations with Strings"></a>Basic Operations with Strings</h2><p>有时候字符串跨好几行。Python 提供了多种方式表示它们。在下面的例子中，一个包含两个字符串的序列被连接为一个字符串。我们需要使用 <strong> 反斜杠 </strong> 或者 <strong> 括号</strong>，这样解释器就知道第一行的表达式不完整了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; couplet = &quot;Shall I compare thee to a Summer&apos;s day?&quot;\</div><div class="line">...           &quot;Thou are more lovely and more temperate:&quot;</div><div class="line">&gt;&gt;&gt; print couplet</div><div class="line">Shall I compare thee to a Summer&apos;s day?Thou are more lovely and more temperate:</div><div class="line">&gt;&gt;&gt; couplet = (&quot;Rough winds do shake the darling buds of May,&quot;</div><div class="line">...           &quot;And Summer&apos;s lease hath all too short a date:&quot;)</div><div class="line">&gt;&gt;&gt; print couplet</div><div class="line">Rough winds do shake the darling buds of May,And Summer&apos;s lease hath all too short a date:</div></pre></td></tr></table></figure>
<p>不幸的是，这些方法并没有展现给我们十四行诗中两行之间的换行。为此，我们可以使用如下所示的三重引号的字符串。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; couplet = &quot;&quot;&quot;Shall I compare thee to a Summer&apos;s day?</div><div class="line">...           Thou are more lovely and more temperate:&quot;&quot;&quot;</div><div class="line">&gt;&gt;&gt; print couplet</div><div class="line">Shall I compare thee to a Summer&apos;s day?</div><div class="line">Thou are more lovely and more temperate:</div><div class="line">&gt;&gt;&gt; couplet = &apos;&apos;&apos;Rough winds do shake the darling buds of May,</div><div class="line">...           And Summer&apos;s lease hath all too short a date:&apos;&apos;&apos;</div><div class="line">&gt;&gt;&gt; print couplet</div><div class="line">Rough winds do shake the darling buds of May,</div><div class="line">And Summer&apos;s lease hath all too short a date:</div></pre></td></tr></table></figure>
<hr>
<h2 id="Regular-Expressions-for-Detecting-Word-Patterns"><a href="#Regular-Expressions-for-Detecting-Word-Patterns" class="headerlink" title="Regular Expressions for Detecting Word Patterns"></a>Regular Expressions for Detecting Word Patterns</h2><p>正则表达式基本元字符，其中包括通配符、范围和闭包：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Operator</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr>
<td>.</td>
<td>Wildcard, matches any character</td>
</tr>
<tr>
<td>^abc</td>
<td>Matches some pattern abc at the start of a string</td>
</tr>
<tr>
<td>abc$</td>
<td>Matches some pattern abc at the end of a string</td>
</tr>
<tr>
<td>[abc]</td>
<td>Matches one of a set of characters</td>
</tr>
<tr>
<td>[A-Z0-9]</td>
<td>Matches one of a range of characters</td>
</tr>
<tr>
<td>ed/ing/s</td>
<td>Matches one of the specified strings (disjunction)</td>
</tr>
<tr>
<td>*</td>
<td>Zero or more of previous item, e.g.,a<em>,[a-z]</em>(also known as Kleene Closure)</td>
</tr>
<tr>
<td>+</td>
<td>One or more of previous item, e.g.,a+,[a-z]+</td>
</tr>
<tr>
<td>?</td>
<td>Zero or one of the previous item (i.e., optional), e.g.,a?,[a-z]?</td>
</tr>
<tr>
<td>{n}</td>
<td>Exactly n repeats where n is a non-negative integer</td>
</tr>
<tr>
<td>{n,}</td>
<td>At least n repeats</td>
</tr>
<tr>
<td>{,n}</td>
<td>No more than n repeats</td>
</tr>
<tr>
<td>{m,n}</td>
<td>At least m and no more than n repeats</td>
</tr>
<tr>
<td>a(b/c)+</td>
<td>Parentheses that indicate the scope of the operators</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>正则表达式是用来指定模式的一种强大而灵活的方法。只要导入了 <strong><code>re</code></strong> 模块，就可以使用 <strong><code>re.findall()</code></strong> 找到一个字符串中匹配一个模式的所有子字符串。</li>
<li>如果正则表达式字符串包含反斜杠，应该使用原始字符串与 r 前缀：<strong><code>r&#39;regexp&#39;</code></strong>，告诉 Python 不要预处理这个字符串。</li>
</ul>
<hr>
<h2 id="Finding-Word-Stems"><a href="#Finding-Word-Stems" class="headerlink" title="Finding Word Stems"></a>Finding Word Stems</h2><p>书中提到的，抽出一个词的词干的方法，是直接去掉任何看起来像后缀的字符。听起来很棒，但是仍然存在一个问题。比如这个词 <strong>processes</strong>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; re.findall(r<span class="string">'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$'</span>, <span class="string">'processes'</span>)</div><div class="line">[(<span class="string">'processe'</span>, <span class="string">'s'</span>)]</div></pre></td></tr></table></figure></p>
<p>正则表达式错误的找到了后缀 ‘-s’，而不是后缀 ‘-es’。这表明另一个微妙之处：<br><strong><code>*</code></strong> 操作符是“贪婪的”，所以表达式的 <strong><code>.*</code></strong> 部分试图尽可能多地匹配输入的字符串。如果使用“非贪婪”版本的 <strong><code>*</code></strong> 操作符，写成 <strong><code>*?</code></strong> 操作符，就得到想要的结果。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; re.findall(r<span class="string">'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$'</span>, <span class="string">'processes'</span>)</div><div class="line">[(<span class="string">'process'</span>, <span class="string">'es'</span>)]</div></pre></td></tr></table></figure></p>
<p>还可以通过将第二个括号中的内容变成可选来得到空后缀。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; re.findall(r<span class="string">'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$'</span>, <span class="string">'language'</span>)</div><div class="line">[(<span class="string">'language'</span>, <span class="string">''</span>)]</div></pre></td></tr></table></figure></p>
<p>（虽然以上方法还有许多问题…）</p>
<hr>
<h2 id="Searching-Tokenized-Text"><a href="#Searching-Tokenized-Text" class="headerlink" title="Searching Tokenized Text"></a>Searching Tokenized Text</h2><p>可以使用一种特殊的正则表达式搜索一个文本中多个词。例如，在大型文本语料库中搜索 ‘x and other ys’ 形式的表达式来发现上位词。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; from nltk.corpus import brown</div><div class="line">&gt;&gt;&gt; hobbies_learned = nltk.Text(brown.words(categories=[<span class="string">'hobbies'</span>, <span class="string">'learned'</span>]))</div><div class="line">&gt;&gt;&gt; hobbies_learned.findall(r<span class="string">"&lt;\w*&gt; &lt;and&gt; &lt;other&gt; &lt;\w*s&gt;"</span>)</div><div class="line">speed and other activities; water and other liquids; tomb and other</div><div class="line">landmarks; Statues and other monuments; pearls and other jewels;</div><div class="line">charts and other items; roads and other features; figures and other</div><div class="line">objects; military and other areas; demands and other factors;</div><div class="line">abstracts and other compilations; iron and other metals</div></pre></td></tr></table></figure></p>
<hr>
<h2 id="Normalizing-Text"><a href="#Normalizing-Text" class="headerlink" title="Normalizing Text"></a>Normalizing Text</h2><h3 id="Stemmers"><a href="#Stemmers" class="headerlink" title="Stemmers"></a>Stemmers</h3><p><strong>词干提取器</strong>。NLTK 中包括了一些现成的词干提取器，如果需要使用词干提取器，应该优先使用它们中的一个，而不是使用正则表达式制作自己的词干提取器，因为 NLTK 中的词干提取器能处理的不规则情况很广泛。Porter 和 Lancaster 词干提取器按照它们自己的规则剥离词缀。下面的例子表明 Porter 词干提取器正确处理了词 lying（将它映射为 lie），而 Lancaster 词干提取器并没有处理好。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; porter = nltk.PorterStemmer()</div><div class="line">&gt;&gt;&gt; lancaster = nltk.LancasterStemmer()</div><div class="line">&gt;&gt;&gt; [porter.stem(t) <span class="keyword">for</span> t <span class="keyword">in</span> tokens]</div><div class="line">[<span class="string">'DENNI'</span>, <span class="string">':'</span>, <span class="string">'Listen'</span>, <span class="string">','</span>, <span class="string">'strang'</span>, <span class="string">'women'</span>, <span class="string">'lie'</span>, <span class="string">'in'</span>, <span class="string">'pond'</span>,</div><div class="line"><span class="string">'distribut'</span>, <span class="string">'sword'</span>, <span class="string">'is'</span>, <span class="string">'no'</span>, <span class="string">'basi'</span>, <span class="string">'for'</span>, <span class="string">'a'</span>, <span class="string">'system'</span>, <span class="string">'of'</span>, <span class="string">'govern'</span>,</div><div class="line"><span class="string">'.'</span>, <span class="string">'Suprem'</span>, <span class="string">'execut'</span>, <span class="string">'power'</span>, <span class="string">'deriv'</span>, <span class="string">'from'</span>, <span class="string">'a'</span>, <span class="string">'mandat'</span>, <span class="string">'from'</span>,</div><div class="line"><span class="string">'the'</span>, <span class="string">'mass'</span>, <span class="string">','</span>, <span class="string">'not'</span>, <span class="string">'from'</span>, <span class="string">'some'</span>, <span class="string">'farcic'</span>, <span class="string">'aquat'</span>, <span class="string">'ceremoni'</span>, <span class="string">'.'</span>]</div><div class="line">&gt;&gt;&gt; [lancaster.stem(t) <span class="keyword">for</span> t <span class="keyword">in</span> tokens]</div><div class="line">[<span class="string">'den'</span>, <span class="string">':'</span>, <span class="string">'list'</span>, <span class="string">','</span>, <span class="string">'strange'</span>, <span class="string">'wom'</span>, <span class="string">'lying'</span>, <span class="string">'in'</span>, <span class="string">'pond'</span>, <span class="string">'distribut'</span>,</div><div class="line"><span class="string">'sword'</span>, <span class="string">'is'</span>, <span class="string">'no'</span>, <span class="string">'bas'</span>, <span class="string">'for'</span>, <span class="string">'a'</span>, <span class="string">'system'</span>, <span class="string">'of'</span>, <span class="string">'govern'</span>, <span class="string">'.'</span>, <span class="string">'suprem'</span>,</div><div class="line"><span class="string">'execut'</span>, <span class="string">'pow'</span>, <span class="string">'der'</span>, <span class="string">'from'</span>, <span class="string">'a'</span>, <span class="string">'mand'</span>, <span class="string">'from'</span>, <span class="string">'the'</span>, <span class="string">'mass'</span>, <span class="string">','</span>, <span class="string">'not'</span>,</div><div class="line"><span class="string">'from'</span>, <span class="string">'som'</span>, <span class="string">'farc'</span>, <span class="string">'aqu'</span>, <span class="string">'ceremony'</span>, <span class="string">'.'</span>]</div></pre></td></tr></table></figure></p>
<p>词干提取过程没有明确定义，通常选择最合适应用的词干提取器。<br>书本上的例子不错，使用词干提取器索引文本：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">IndexedText</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, stemmer, text)</span>:</span></div><div class="line">        self._text = text</div><div class="line">        self._stemmer = stemmer</div><div class="line">        self._index = nltk.Index((self._stem(word), i)</div><div class="line">                                 <span class="keyword">for</span> (i, word) <span class="keyword">in</span> enumerate(text))</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">concordance</span><span class="params">(self, word, width=<span class="number">40</span>)</span>:</span></div><div class="line">        key = self._stem(word)</div><div class="line">        wc = width/<span class="number">4</span>                <span class="comment"># words of context</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> self._index[key]:</div><div class="line">            lcontext = <span class="string">''</span>.join(self._text[i-wc:i])</div><div class="line">            rcontext = <span class="string">' '</span>.join(self._text[i:i+wc])</div><div class="line">            ldisplay = <span class="string">'%*s'</span>  % (width, lcontext[-width:])</div><div class="line">            rdisplay = <span class="string">'%-*s'</span> % (width, rcontext[:width])</div><div class="line">            <span class="keyword">print</span> ldisplay, rdisplay</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_stem</span><span class="params">(self, word)</span>:</span></div><div class="line">        <span class="keyword">return</span> self._stemmer.stem(word).lower()</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; porter = nltk.PorterStemmer()</div><div class="line">&gt;&gt;&gt; grail = nltk.corpus.webtext.words(&apos;grail.txt&apos;)</div><div class="line">&gt;&gt;&gt; text = IndexedText(porter, grail)</div><div class="line">&gt;&gt;&gt; text.concordance(&apos;lie&apos;)</div><div class="line">r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no</div><div class="line"> beat a very brave retreat . ROBIN : All lies ! MINSTREL : [singing] Bravest of</div><div class="line">       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !</div><div class="line">doctors immediately ! No , no , please ! Lie down . [clap clap] PIGLET : Well</div><div class="line">ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which</div><div class="line">   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --</div><div class="line">h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k</div><div class="line">not stop our fight &apos; til each one of you lies dead , and the Holy Grail returns t</div></pre></td></tr></table></figure>
<hr>
<h2 id="Formatting-From-Lists-to-Strings"><a href="#Formatting-From-Lists-to-Strings" class="headerlink" title="Formatting: From Lists to Strings"></a>Formatting: From Lists to Strings</h2><h3 id="from-Lists-to-Strings"><a href="#from-Lists-to-Strings" class="headerlink" title="from Lists to Strings"></a>from Lists to Strings</h3><p>从链表到字符串。用于文本处理最简单的结构化对象是词链表。当需要把这些输出到显示器或者文件中时，必须把这些词的链表转换成字符串。在 Python 中，使用 <strong><code>join()</code></strong> 方法，并制定作为“胶水”使用的字符串。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; silly = [<span class="string">'We'</span>, <span class="string">'called'</span>, <span class="string">'him'</span>, <span class="string">'Tortoise'</span>, <span class="string">'because'</span>, <span class="string">'he'</span>, <span class="string">'taught'</span>, <span class="string">'us'</span>, <span class="string">'.'</span>]</div><div class="line">&gt;&gt;&gt; <span class="string">''</span>.join(silly)</div><div class="line"><span class="string">'We called him Tortoise because he taught us .'</span></div><div class="line">&gt;&gt;&gt; <span class="string">';'</span>.join(silly)</div><div class="line"><span class="string">'We;called;him;Tortoise;because;he;taught;us;.'</span></div><div class="line">&gt;&gt;&gt; <span class="string">''</span>.join(silly)</div><div class="line"><span class="string">'WecalledhimTortoisebecausehetaughtus.'</span></div></pre></td></tr></table></figure></p>
<ul>
<li>书本提到了 <strong> 间接地提供占位符的值</strong>。例子：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; template = <span class="string">'Lee wants a %s right now'</span></div><div class="line">&gt;&gt;&gt; menu = [<span class="string">'sandwich'</span>, <span class="string">'spam fritter'</span>, <span class="string">'pancake'</span>]</div><div class="line">&gt;&gt;&gt; <span class="keyword">for</span> snack <span class="keyword">in</span> menu:</div><div class="line">...     <span class="built_in">print</span> template % snack</div><div class="line">...</div><div class="line">Lee wants a sandwich right now</div><div class="line">Lee wants a spam fritter right now</div><div class="line">Lee wants a pancake right now</div></pre></td></tr></table></figure>
<hr>
<h1 id="Need-to-Know"><a href="#Need-to-Know" class="headerlink" title="Need to Know:"></a>Need to Know:</h1><p>在 Python 2.x 当中是可以使用：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> urlopen</div></pre></td></tr></table></figure></p>
<p>如果使用的是 Python 3.x 的话，需要更改为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</div></pre></td></tr></table></figure></p>
<hr>
<p>在「处理 HTML」的模块中：</p>
<p>书本提到从 HTML 中提取文本，采用辅助函数 <strong><code>nltk.clean_html()</code></strong> 将 HTML 字符串作为参数，返回原始文本。</p>
<p>然而，现在这个辅助函数已不支持。为了实现这一目的，我们可以下载<strong><code>Beautiful Soup 4</code></strong>。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ pip install beautifulsoup4</div></pre></td></tr></table></figure></p>
<p>随后在代码部分中，调用 BeautifulSoup：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> nltk, re, pprint</div><div class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> *</div><div class="line"></div><div class="line">url = <span class="string">"http://youraddress"</span></div><div class="line">html = urlopen(url).read()</div><div class="line">soup = BeautifulSoup(html)</div><div class="line">raw = soup.get_text()</div><div class="line">tokens = nltk.word_tokenize(raw)</div></pre></td></tr></table></figure></p>
<hr>
<p>在「分词」的模块中：</p>
<blockquote>
<p>Now the segmentation task becomes a search problem: find the bit string that causes the text string to be correctly segmented into words.<br>现在分词的任务变成一个搜索问题：找到能将文本字符串正确地分割成词汇的字位串。</p>
<p><strong>We assume the learner is acquiring words and storing them in an internal lexicon. Given a suitable lexicon, it is possible to reconstruct the source text as a sequence of lexical items.</strong></p>
<p>假定学习者接受字词，并将它们存储在一个内部的词典当中。给定一个合适的词典，我们是能够使用词典中的词的序列来进行重构文本的。</p>
<p>Following (Brent &amp; Cart- wright, 1995), we can define an <strong>objective function</strong>, a scoring function whose value we will try to optimize, based on the size of the lexicon and the amount of information needed to reconstruct the source text from the lexicon.<br>为了衡量我们这个词典的优劣，这里我们需要定义一个目标函数（Brent &amp; Cart-wright 在 1995 提出的方法），即一个打分函数，依据两个因素，第一个因素是词典的大小，第二个是使用词典来重构原文本所需的信息量。</p>
</blockquote>
<p><img src="https://farm1.staticflickr.com/767/31488720242_48aae8823f_o.png" alt=""></p>
<p>计算目标函数：给定一个假设的源文本的分词（左），推导出一个词典和推导表，它能让源文本重构，然后合计每个词项（包括边界标志）与推导表的字符数，作为分词质量的得分；得分值越小表明分词越好。</p>
<p>用代码来实现这个目标函数，计算存储词典和重构源文本的成本：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(text, segs)</span>:</span></div><div class="line">    words = segment(text, segs)</div><div class="line">    text_size = len(words)</div><div class="line">    lexicon_size = len(<span class="string">''</span>.join(list(set(words))))</div><div class="line">    <span class="keyword">return</span> text_size + lexicon_size</div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; text = <span class="string">"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy"</span></div><div class="line">&gt;&gt;&gt; seg1 = <span class="string">"0000000000000001000000000010000000000000000100000000000"</span></div><div class="line">&gt;&gt;&gt; seg2 = <span class="string">"0100100100100001001001000010100100010010000100010010000"</span></div><div class="line">&gt;&gt;&gt; seg3 = <span class="string">"0000100100000011001000000110000100010000001100010000001"</span></div><div class="line">&gt;&gt;&gt; segment(text, seg3)</div><div class="line">[<span class="string">'doyou'</span>, <span class="string">'see'</span>, <span class="string">'thekitt'</span>, <span class="string">'y'</span>, <span class="string">'see'</span>, <span class="string">'thedogg'</span>, <span class="string">'y'</span>, <span class="string">'doyou'</span>, <span class="string">'like'</span>,</div><div class="line"> <span class="string">'thekitt'</span>, <span class="string">'y'</span>, <span class="string">'like'</span>, <span class="string">'thedogg'</span>, <span class="string">'y'</span>]</div><div class="line">&gt;&gt;&gt; evaluate(text, seg3)</div><div class="line">46</div><div class="line">&gt;&gt;&gt; evaluate(text, seg2)</div><div class="line">47</div><div class="line">&gt;&gt;&gt; evaluate(text, seg1)</div><div class="line">63</div></pre></td></tr></table></figure>
<hr>
<h2 id="Simulated-Annealing-SA"><a href="#Simulated-Annealing-SA" class="headerlink" title="Simulated Annealing(SA)"></a>Simulated Annealing(SA)</h2><p><strong>模拟退火算法 </strong>。在提到模拟退火算法之前，我来先介绍一下<strong> 爬山算法（Hill Climbing）</strong>。爬山算法是一种简单的贪心搜索算法，该算法每次从当前的临近解空间中选择一个最优解作为当前解，直到达到一个局部最优解。<br>爬山算法实现很简单，其主要的缺点就是会陷入局部最优解而不一定能搜索到全局最优解。如图所示，假设 C 点为当前解，爬山算法搜索到 A 点这个局部最优解就会停止搜索，因为 A 点无论向哪个方向小幅度移动都不能得到更优的解。</p>
<p><img src="https://farm1.staticflickr.com/768/31263141960_9e2f2a7c44_o.png" alt=""></p>
<p>爬山算法是完完全全的贪心算法，每一次都是鼠目寸光地选择一个当前最优解，因此只能搜索到局部的最有值。模拟退火其实也是一种贪心算法，但是它的搜索过程引入了随机因素。模拟退火算法 <strong> 以一定的概率 </strong> 来接受一个比当前解要差的解，因此 <strong> 有可能 </strong> 会跳出这个局部的最优解，达到全局的最优解。如上图为例，模拟退火算法在搜索到局部最优解 A 后，会以 <strong> 一定的概率 </strong> 接受向 E 的移动。也许经过几次这样的不是局部最优的移动后会到达 D 点，于是就跳出了局部最大值 A。<br>模拟退火算法描述：</p>
<ul>
<li><p>若 $ J(Y(i+1)) \geqslant  J(Y(i)) $  (即移动后得到更优解)，则总是接受该移动</p>
</li>
<li><p>若 $ J(Y(i+1)) &lt;  J(Y(i)) $  (即移动后的解比当前解要差)，则以一定的概率接受移动，而且这个概率随着时间推移逐渐降低（逐渐降低才能趋向稳定）</p>
</li>
</ul>
<p><strong>这里的“一定的概率”的计算参考了金属冶炼的退火过程，这也是模拟退火算法名称的由来。</strong></p>
<p>根据热力学的原理，在温度为 <strong><em>T</em></strong> 时，出现能量差为 <strong><em>dE</em></strong> 的降温的概率为<strong><em>P(dE)</em></strong>，表示为：</p>
<script type="math/tex; mode=display">
P(\mathrm{d} E) = exp(\mathrm{d}E/kT)</script><p>其中 <strong><em>k</em></strong> 是一个常数，<strong><em>exp</em></strong>表示自然指数，且 <strong><em>dE&lt;0</em></strong>。这条公式说白了就是：温度越高，出现一次能量差为<strong><em>dE</em></strong> 的降温的概率就越大；温度越低，则出现降温的概率就越小。又由于 <strong><em>dE</em></strong> 总是小于 0（否则就不叫退火了），因此 <strong><em>dE/kT &lt; 0</em></strong>，所以<strong><em>P(dE)</em></strong> 的函数取值范围是 $(0,1)$ 。</p>
<p>　　随着温度 <strong><em>T</em></strong> 的降低，<strong><em>P(dE)</em></strong>会逐渐降低。</p>
<p>我们将一次向较差解的移动看做一次温度跳变过程，我们以概率 <strong><em>P(dE)</em></strong> 来接受这样的移动。</p>
<p>关于爬山算法与模拟退火，有一个有趣的比喻：</p>
<p>爬山算法：兔子朝着比现在高的地方跳去。它找到了不远处的最高山峰。但是这座山不一定是珠穆朗玛峰。这就是爬山算法，它不能保证局部最优值就是全局最优值。</p>
<p>模拟退火：兔子喝醉了。它随机地跳了很长时间。这期间，它可能走向高处，也可能踏入平地。但是，它渐渐清醒了并朝最高方向跳去。这就是模拟退火。</p>
<hr>
<p>接着，让我们使用带有模拟退火算法思想的非确定性搜索，来确定构建分词最好的词典：</p>
<ol>
<li>一开始仅搜索短语分词；</li>
<li>随机扰动 0 和 1，它们与“温度”成一定比例；</li>
<li>每次迭代温度都会降低，扰动边界会减少。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</div><div class="line"></div><div class="line"><span class="comment">#flip()函数，随机扰动 0 和 1</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flip</span><span class="params">(segs, pos)</span>:</span></div><div class="line">    <span class="keyword">return</span> segs[:pos] + str(<span class="number">1</span>-int(segs[pos])) + segs[pos+<span class="number">1</span>:]</div><div class="line"></div><div class="line"><span class="comment">#flip_n()函数，n 为迭代次数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flip_n</span><span class="params">(segs, n)</span>:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">        segs = flip(segs, randint(<span class="number">0</span>,len(segs)<span class="number">-1</span>))</div><div class="line">    <span class="keyword">return</span> segs</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">anneal</span><span class="params">(text, segs, iterations, cooling_rate)</span>:</span>    <span class="comment">#cooling_rate“降温”的快慢</span></div><div class="line">    temperature = float(len(segs))    <span class="comment"># 初始温度</span></div><div class="line">    <span class="keyword">while</span> temperature &gt; <span class="number">0.5</span>:</div><div class="line">        <span class="comment"># 每一次“降温”的结果，若由于前一次，则会更改 segs 的值并进行下一次“降温”</span></div><div class="line">        best_segs, best = segs, evaluate(text, segs)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(iterations):</div><div class="line">            guess = flip_n(segs, int(round(temperature)))</div><div class="line">            score = evaluate(text, guess)</div><div class="line">            <span class="keyword">if</span> score &lt; best:</div><div class="line">                best, best_segs = score, guess</div><div class="line">        score, segs = best, best_segs</div><div class="line">        temperature = temperature / cooling_rate</div><div class="line">        <span class="keyword">print</span> evaluate(text, segs), segment(text, segs)</div><div class="line">    <span class="keyword">print</span></div><div class="line">    <span class="keyword">return</span> segs</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; text = <span class="string">"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy"</span></div><div class="line">&gt;&gt;&gt; seg1 = <span class="string">"0000000000000001000000000010000000000000000100000000000"</span></div><div class="line">&gt;&gt;&gt; anneal(text, seg1, 5000, 1.2)</div><div class="line">60 [<span class="string">'doyouseetheki'</span>, <span class="string">'tty'</span>, <span class="string">'see'</span>, <span class="string">'thedoggy'</span>, <span class="string">'doyouliketh'</span>, <span class="string">'ekittylike'</span>, <span class="string">'thedoggy'</span>]</div><div class="line">58 [<span class="string">'doy'</span>, <span class="string">'ouseetheki'</span>, <span class="string">'ttysee'</span>, <span class="string">'thedoggy'</span>, <span class="string">'doy'</span>, <span class="string">'o'</span>, <span class="string">'ulikethekittylike'</span>, <span class="string">'thedoggy'</span>]</div><div class="line">56 [<span class="string">'doyou'</span>, <span class="string">'seetheki'</span>, <span class="string">'ttysee'</span>, <span class="string">'thedoggy'</span>, <span class="string">'doyou'</span>, <span class="string">'liketh'</span>, <span class="string">'ekittylike'</span>, <span class="string">'thedoggy'</span>]</div><div class="line">54 [<span class="string">'doyou'</span>, <span class="string">'seethekit'</span>, <span class="string">'tysee'</span>, <span class="string">'thedoggy'</span>, <span class="string">'doyou'</span>, <span class="string">'likethekittylike'</span>, <span class="string">'thedoggy'</span>]</div><div class="line">53 [<span class="string">'doyou'</span>, <span class="string">'seethekit'</span>, <span class="string">'tysee'</span>, <span class="string">'thedoggy'</span>, <span class="string">'doyou'</span>, <span class="string">'like'</span>, <span class="string">'thekitty'</span>, <span class="string">'like'</span>, <span class="string">'thedoggy'</span>]</div><div class="line">51 [<span class="string">'doyou'</span>, <span class="string">'seethekittysee'</span>, <span class="string">'thedoggy'</span>, <span class="string">'doyou'</span>, <span class="string">'like'</span>, <span class="string">'thekitty'</span>, <span class="string">'like'</span>, <span class="string">'thedoggy'</span>]</div><div class="line">42 [<span class="string">'doyou'</span>, <span class="string">'see'</span>, <span class="string">'thekitty'</span>, <span class="string">'see'</span>, <span class="string">'thedoggy'</span>, <span class="string">'doyou'</span>, <span class="string">'like'</span>, <span class="string">'thekitty'</span>, <span class="string">'like'</span>, <span class="string">'thedoggy'</span>]</div><div class="line"><span class="string">'0000100100000001001000000010000100010000000100010000000'</span></div></pre></td></tr></table></figure>
<p>有了足够的数据，就可能以一个较为合理的准确度自动将文本分割成词汇。</p>
<p>这种方法可用于那些词的边界没有任何视觉表示的书写系统分词。</p>
<hr>
<h1 id="Correct-errors-in-printing"><a href="#Correct-errors-in-printing" class="headerlink" title="Correct errors in printing:"></a>Correct errors in printing:</h1><ul>
<li>暂无</li>
</ul>
<hr>
<h1 id="Practice"><a href="#Practice" class="headerlink" title="Practice:"></a>Practice:</h1><blockquote>
<p>5.○ What happens if you ask the interpreter to evaluate <strong><code>monty[::-1]</code></strong>? Explain why this is a reasonable result.</p>
</blockquote>
<ul>
<li>逆序输出。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a=<span class="string">'python'</span></div><div class="line">&gt;&gt;&gt; a[::-1]</div><div class="line"><span class="string">'nohtyp'</span></div></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>6.○ Describe the class of strings matched by the following regular expressions: </p>
<ol>
<li>[a-zA-Z]+</li>
<li>[A-Z][a-z]*</li>
<li>p[aeiou]{,2}t</li>
<li>\d+(\.\d+)?</li>
<li>([^aeiou][aeiou][^aeiou])</li>
<li>\w+|[^\w\s]+</li>
</ol>
<p>Test your answers using <strong><code>nltk.re_show()</code></strong>.</p>
</blockquote>
<ol>
<li>字母字符串</li>
<li>开头大写后小写字母不限（小写字母可有可没有）</li>
<li>p 开头 t 结尾，中间有少于 2 个的元音字母</li>
<li>整数或者带小数的整数（整数与浮点数）</li>
<li>（（非元音）（元音）（非元音））（可有可没有） 例如’pot’</li>
<li>要么是字母一个或多个，要么不是字母、空格一个或多个</li>
</ol>
<hr>
<blockquote>
<p>7.○ Write regular expressions to match the following classes of strings:</p>
<ol>
<li>A single determiner (assume that <strong><em>a</em></strong>, <strong><em>an</em></strong>, and <strong><em>the</em></strong> are the only determiners)</li>
<li>An arithmetic expression using integers, addition, and multiplication, such as <strong>2*3+8</strong></li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="number">1.</span> &gt;&gt;&gt; nltk.re_show(<span class="string">'an?|the'</span>, <span class="string">'thesisiaishihsthean'</span>, left=<span class="string">'&#123;'</span>, right=<span class="string">'&#125;'</span>)</div><div class="line"><span class="number">2.</span> &gt;&gt;&gt; nltk.re_show(<span class="string">'\d+\*\d+\+\d+'</span>, <span class="string">'2*3+8'</span>, left=<span class="string">'&#123;'</span>, right=<span class="string">'&#125;'</span>)</div></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>9.○ Save some text into a file corpus.txt. Define a function <strong><code>load(f)</code></strong> that reads from the file named in its sole argument, and returns a string containing the text of the file.</p>
<ol>
<li>Use <strong><code>nltk.regexp_tokenize()</code></strong>to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multiline regular expression inline comments, using the verbose flag<strong><code>(?x)</code></strong>.</li>
<li>Use <strong><code>nltk.regexp_tokenize()</code></strong> to create a tokenizer that tokenizes the following kinds of expressions: monetary amounts; dates; names of people and organizations.    </li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="number">1.</span> &gt;&gt;&gt; pattern = <span class="string">r'''(?x)</span></div><div class="line">   ...           [][.,;"'?()=-_`]</div><div class="line">   ...           '''</div><div class="line">   &gt;&gt;&gt; nltk.regexp_tokenize(text, pattern)</div><div class="line"></div><div class="line"><span class="number">2.</span> &gt;&gt;&gt; pattern = <span class="string">r'''(?x)</span></div><div class="line">   ...           ([A-Z]\.)+    # eg.  U.S.A</div><div class="line">   ...           |([A-Z][a-z]*\s[A-Z][a-z]*)    # words with optional internal</div><div class="line">   ...           |\$?\d+(\.\d+)?%    # currency and percentages eg. $12.40, 82%</div><div class="line">   ...           |\d+-\d+-\d+</div><div class="line">   ...           '''</div><div class="line">   &gt;&gt;&gt; nltk.regexp_tokenize(text, pattern)</div></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>20.◑ Write code to access a favorite web page and extract some text from it. For example, access a weather site and extract the forecast top temperature for your town or city today.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test20</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># search weather</span></div><div class="line">    url = <span class="string">'http://en.weather.com.cn/weather/101220101.shtml'</span></div><div class="line">    html = urlopen(url).read()</div><div class="line">    soup = BeautifulSoup(html, <span class="string">"lxml"</span>)</div><div class="line">    raw = soup.get_text()</div><div class="line">    tokens = nltk.word_tokenize(raw)</div><div class="line">    text = nltk.Text(tokens)</div><div class="line">    print(text)</div><div class="line">    <span class="keyword">print</span></div><div class="line">    print(text.concordance(<span class="string">'Hefei'</span>))</div></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>21.◑ Write a function <strong><code>unknown()</code></strong> that takes a URL as its argument, and returns a list of unknown words that occur on that web page. In order to do this, extract all substrings consisting of lowercase letters (using <strong><code>re.findall()</code></strong>) and remove any items from this set that occur in the Words Corpus (<strong>nltk.corpus.words</strong>). Try to categorize these words manually and discuss your findings.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test21</span><span class="params">()</span>:</span></div><div class="line">    url = <span class="string">"http://www.bbc.co.uk/news/world-middle-east-18650775"</span></div><div class="line">    wordsres = []</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">unknown</span><span class="params">(url)</span>:</span></div><div class="line">        html = urlopen(url).read()</div><div class="line">        soup = BeautifulSoup(html)</div><div class="line">        raw = soup.get_text()</div><div class="line">        words = re.findall(<span class="string">r'[a-z]+'</span>, raw)</div><div class="line">        wordlist = [w <span class="keyword">for</span> w <span class="keyword">in</span> nltk.corpus.words.words(<span class="string">'en'</span>) <span class="keyword">if</span> w.islower()]</div><div class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:</div><div class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> wordlist:</div><div class="line">                wordsres.append(word)</div><div class="line">        <span class="keyword">return</span> wordsres</div><div class="line">    wordsres = unknown(url)</div><div class="line">    print(wordsres)</div></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>25.◑ <strong><em>Pig Latin</em></strong> is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of the word to the end, then append <strong><em>ay</em></strong>, e.g., <strong><em>string</em></strong> → <strong><em>ingstray</em></strong>, <strong><em>idle</em></strong> → <strong><em>idleay</em></strong> (see <em><a href="http://en.wikipedia.org/wiki/Pig_Latin" target="_blank" rel="external">http://en.wikipedia.org/wiki/Pig_Latin</a></em>).</p>
<ol>
<li>Write a function to convert a word to Pig Latin.</li>
<li>Write code that converts text, instead of individual words.</li>
<li>Extend it further to preserve capitalization, to keep <strong>qu</strong> together (so that <strong>quiet</strong> becomes <strong>ietquay</strong>, for example), and to detect when <strong>y</strong> is used as a consonant (e.g., <strong>yellow</strong>) versus a vowel (e.g., <strong>style</strong>).</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test25</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># Pig Latin</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pig_latin</span><span class="params">(word)</span>:</span></div><div class="line">        result = []</div><div class="line">        <span class="keyword">if</span> <span class="string">'qu'</span> <span class="keyword">in</span> word.lower():</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word)):</div><div class="line">                <span class="keyword">if</span> word[i] <span class="keyword">in</span> <span class="string">'[AEIOUaeiou]'</span>:</div><div class="line">                    pig_word = [word[i+<span class="number">1</span>:], word[:i+<span class="number">1</span>], <span class="string">'ay'</span>]</div><div class="line">                    result = <span class="string">''</span>.join(pig_word)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word)):</div><div class="line">                <span class="keyword">if</span> word[i] <span class="keyword">in</span> <span class="string">'[AEIOUaeiou]'</span>:</div><div class="line">                    pig_word = [word[i:], word[:i], <span class="string">'ay'</span>]</div><div class="line">                    result = <span class="string">''</span>.join(pig_word)</div><div class="line">        <span class="keyword">return</span> result</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">translate</span><span class="params">()</span>:</span></div><div class="line">        object = open(<span class="string">'text25.txt'</span>)</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            text = object.read()</div><div class="line">        <span class="keyword">finally</span>:</div><div class="line">            object.close()</div><div class="line">        words = nltk.word_tokenize(text)</div><div class="line">        result = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(words)):</div><div class="line">            result.append(pig_latin(words[i]))</div><div class="line">        <span class="keyword">return</span> result</div><div class="line">    result = translate()</div><div class="line">    print(result)</div><div class="line">    print(pig_latin(<span class="string">'quiet'</span>))</div><div class="line">    print(pig_latin(<span class="string">'string'</span>))</div></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>27.◑ Python’s <strong>random</strong> module includes a function <strong><code>choice()</code></strong> which randomly chooses an item from a sequence; e.g., <strong><code>choice(&quot;aehh &quot;)</code></strong> will produce one of four possible characters, with the letter h being twice as frequent as the others. Write a generator expression that produces a sequence of 500 randomly chosen letters drawn from the string <strong>“aehh “</strong>, and put this expression inside a call to the <strong><code>&#39;&#39;.join()</code></strong> function, to concatenate them into one long string. You should get a result that looks like uncontrolled sneezing or maniacal laughter: <strong>he haha ee heheeh eha</strong>. Use <strong><code>split()</code></strong> and <strong><code>join()</code></strong> again to normalize the whitespace in this string.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test27</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># choice</span></div><div class="line">    string = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">        string.append(random.choice(<span class="string">'hahe'</span>))</div><div class="line">    result = <span class="string">''</span>.join(string).split()</div><div class="line">    print(result)</div></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>29.◑ Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define $μ_w$ to be the average number of letters per word, and $μ_s$ to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: $4.71 μ_w + 0.5 μ_s - 21.43$. Compute the ARI score for various sections of the Brown Corpus, including section f (popular lore) and j (learned). Make use of the fact that <strong><code>nltk.corpus.brown.words()</code></strong> produces a se- quence of words, whereas <strong><code>nltk.corpus.brown.sents()</code></strong> produces a sequence of sentences.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test29</span><span class="params">()</span>:</span></div><div class="line">    words1 = [len(word) <span class="keyword">for</span> word <span class="keyword">in</span> nltk.corpus.brown.words(categories = <span class="string">'lore'</span>)]</div><div class="line">    sents1 = [len(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> nltk.corpus.brown.sents(categories = <span class="string">'lore'</span>)]</div><div class="line">    words2 = [len(word) <span class="keyword">for</span> word <span class="keyword">in</span> nltk.corpus.brown.words(categories = <span class="string">'learned'</span>)]</div><div class="line">    sents2 = [len(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> nltk.corpus.brown.sents(categories = <span class="string">'learned'</span>)]</div><div class="line">    wordsum = <span class="number">0</span></div><div class="line">    sentsum = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> wlength <span class="keyword">in</span> words1 :</div><div class="line">        wordsum += int(wlength)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> slength <span class="keyword">in</span> sents1 :</div><div class="line">        sentsum += slength</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ARI</span><span class="params">(uw,us)</span>:</span></div><div class="line">        <span class="keyword">return</span> <span class="number">4.71</span>*uw + <span class="number">0.5</span>*us - <span class="number">21.43</span></div><div class="line"></div><div class="line">    uw = wordsum/len(words1)</div><div class="line">    us = sentsum/len(sents1)</div><div class="line">    print(us)</div><div class="line">    print(ARI(uw, us))</div></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>30.◑ Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer, and see if you ob- serve any differences.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test30</span><span class="params">()</span>:</span></div><div class="line">    saying = [<span class="string">'After'</span>, <span class="string">'all'</span>, <span class="string">'is'</span>, <span class="string">'said'</span>, <span class="string">'and'</span>, <span class="string">'done'</span>, <span class="string">','</span>, <span class="string">'more'</span>, <span class="string">'is'</span>, <span class="string">'said'</span>, <span class="string">'than'</span>, <span class="string">'done'</span>, <span class="string">'.'</span>]</div><div class="line">    porter = nltk.PorterStemmer()</div><div class="line">    lancaster = nltk.LancasterStemmer()</div><div class="line">    result_porter = [porter.stem(t) <span class="keyword">for</span> t <span class="keyword">in</span> saying]</div><div class="line">    result_lancaster = [lancaster.stem(t) <span class="keyword">for</span> t <span class="keyword">in</span> saying]</div><div class="line">    print(result_porter)</div><div class="line">    print(result_lancaster)</div></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>32.◑ Define a variable silly to contain the string: <strong>‘newly formed bland ideas are inexpressible in an infuriating way’</strong>. (This happens to be the legitimate inter- pretation that bilingual English-Spanish speakers can assign to Chomsky’s famous nonsense phrase <strong><em>colorless green ideas sleep furiously</em></strong>, according to Wikipedia). Now write code to perform the following tasks:</p>
<ol>
<li>Split <strong>silly</strong> into a list of strings, one per word, using Python’s <strong><code>split()</code></strong> opera- tion, and save this to a variable called <strong>bland</strong>.</li>
<li>Extract the second letter of each word in <strong>silly</strong> and join them into a string, to get <strong>‘eoldrnnnna’</strong>.</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test32</span><span class="params">()</span>:</span></div><div class="line">    silly=<span class="string">'newly formed bland ideas are inexpressible in an infuriating way'</span></div><div class="line">    bland = silly.split()</div><div class="line">    print(bland)</div><div class="line">    result = <span class="string">''</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(bland)):</div><div class="line">        result = result + bland[i][<span class="number">1</span>]</div><div class="line">    print(result,type(result))</div></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>35.◑ Read the LanguageLog post on phrases of the form <strong><em>as best as p can</em></strong> and <strong><em>as best p can</em></strong>, where <strong><em>p</em></strong> is a pronoun. Investigate this phenomenon with the help of a corpus and the <strong><code>findall()</code></strong> method for searching tokenized text described in Section 3.5. The post is at <em><a href="http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html" target="_blank" rel="external">http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html</a></em>.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test35</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># 为什么我的 brown.words()中没有 as best as p can 以及 as best p can 的形式？</span></div><div class="line">    text = nltk.Text(brown.words())</div><div class="line">    print(text)</div><div class="line">    print(text.findall(<span class="string">r'&lt;as&gt; &lt;\w*&gt; &lt;as&gt;'</span>))</div><div class="line">    <span class="keyword">print</span></div><div class="line">    print(text.findall(<span class="string">r'&lt;\w*&gt; &lt;and&gt; &lt;other&gt; &lt;\w*s&gt;'</span>))</div></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>37.◑ Read about the <strong><code>re.sub()</code></strong> function for string substitution using regular expres- sions, using <strong><code>help(re.sub)</code></strong> and by consulting the further readings for this chapter. Use <strong><code>re.sub</code></strong> in writing code to remove HTML tags from an HTML file, and to normalize whitespace.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test37</span><span class="params">()</span>:</span></div><div class="line">    object = open(<span class="string">'Language Log: Asbestos she can.html'</span>)</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        text = object.read()</div><div class="line">        pattern = <span class="string">'''(?x)&lt;html&gt;|&lt;/html&gt;'''</span></div><div class="line">        text = re.sub(pattern, <span class="string">''</span>, text)</div><div class="line">        object_copy = open(<span class="string">'text36.txt'</span>, <span class="string">'w+'</span>)</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            object_copy.write(text)</div><div class="line">        <span class="keyword">finally</span>:</div><div class="line">            object_copy.close()</div><div class="line"></div><div class="line">    <span class="keyword">finally</span>:</div><div class="line">        object.close()</div></pre></td></tr></table></figure>
<hr>
]]></content>
    
    <summary type="html">
    
      本文是关于「Natural Language Processing with Python」这本书的 Chapter 3 的学习笔记。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
      <category term="Natural Language Processing" scheme="http://randolph.pro/categories/Machine-Learning/Natural-Language-Processing/"/>
    
      <category term="Book:「NLP with Python」" scheme="http://randolph.pro/categories/Machine-Learning/Natural-Language-Processing/Book-%E3%80%8CNLP-with-Python%E3%80%8D/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
      <category term="Natural Language Processing" scheme="http://randolph.pro/tags/Natural-Language-Processing/"/>
    
  </entry>
  
  <entry>
    <title>♞「NLP with Python」 Chapter 1 &amp; 2</title>
    <link href="http://randolph.pro/2015/09/11/%E2%99%9E%E3%80%8CNLP%20with%20Python%E3%80%8D%20Chapter%201%20&amp;%202/"/>
    <id>http://randolph.pro/2015/09/11/♞「NLP with Python」 Chapter 1 &amp; 2/</id>
    <published>2015-09-10T16:00:00.000Z</published>
    <updated>2017-08-14T12:56:49.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://farm5.staticflickr.com/4423/36425940061_fe957aaf15_o.jpg" alt=""></p>
<p>有关该书的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/Natural-Language-Processing/Book-「NLP-with-Python」/">Book:「NLP with Python」</a></p>
<h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><ul>
<li><strong>NLTK</strong></li>
<li><strong>concordance() function</strong></li>
<li><strong>Word Sense Disambiguation &amp; Pronoun Resolution</strong></li>
<li><strong>Text Corpus Structure</strong></li>
<li><strong>WordNet</strong></li>
</ul>
<hr>
<h1 id="Key"><a href="#Key" class="headerlink" title="Key:"></a>Key:</h1><h2 id="What’s-NTLK"><a href="#What’s-NTLK" class="headerlink" title="What’s NTLK?"></a><strong>What’s NTLK?</strong></h2><p>NTLK 是一个自然语言工具包，最初创建于 2001 年，最初是宾州大学计算机与信息科学系计算语言学课程的一部分，大部分 NLP 研究者入门的首选 tool。</p>
<p>另外，这本书是关于用 Python 进行自然语言处理的一本入门书，基本上可以看做是 NLTK 这个库的 HandBook，使用的方法均是 nltk 库中的方法。如果希望查阅 API 文档或者是下载安装 NLTK，可以前往 <a href="http://www.nltk.org" target="_blank" rel="external"> 官方网站 </a> 下载，官网上提供和的 API 文档涵盖了工具包中的每一个模块、类和函数，详细说明了各种参数，以及用法示例，在此不再赘述。</p>
<ul>
<li><strong>简单介绍一下 NLTK 的几个重要的模块以及功能描述：</strong></li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>语言处理任务</th>
<th>NLTK 模块</th>
<th>功能描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>获取语料库</td>
<td>nltk.corpus</td>
<td>语料库和字典的标准化接口</td>
</tr>
<tr>
<td>字符串处理</td>
<td>nltk.tokenize, nltk.stem</td>
<td>分词、句子分解、提取主干</td>
</tr>
<tr>
<td>搭配探究</td>
<td>nltk.collocations</td>
<td>t- 检验、卡方、点互信息</td>
</tr>
<tr>
<td>词性标识符</td>
<td>nltk.tag</td>
<td>n-gram、backoff、Brill、HMM、TnT</td>
</tr>
<tr>
<td>分类</td>
<td>nltk.classify，nltk.cluster</td>
<td>决策树、最大熵、朴素贝叶斯、EM、k-means</td>
</tr>
<tr>
<td>分块</td>
<td>nltk.chunk</td>
<td>正则表达式、n-gram、命名实体</td>
</tr>
<tr>
<td>解析</td>
<td>nltk.parse</td>
<td>图表、基于特征、一致性、概率性、依赖项</td>
</tr>
<tr>
<td>语义解释</td>
<td>nltk.sem，nltk.inference</td>
<td>ℷ 演算、一阶逻辑、模型检验</td>
</tr>
<tr>
<td>指标评测</td>
<td>nltk.metrice</td>
<td>精度、召回率、协议系数</td>
</tr>
<tr>
<td>概率与估计</td>
<td>nltk.probability</td>
<td>频率分布、平滑概率分布</td>
</tr>
<tr>
<td>应用</td>
<td>nltk.app，nltk.chat</td>
<td>图形化的关键词排序、分析器、WordNet 查看器、聊天机器人</td>
</tr>
<tr>
<td>语言学领域的工作</td>
<td>nltk.toolbox</td>
<td>处理 SIL 数据格式的工具箱</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="concordance-function"><a href="#concordance-function" class="headerlink" title="concordance function"></a>concordance <strong>function</strong></h2><ul>
<li><strong><code>concordance()</code></strong> 函数：这个函数挺有意思的，是 nltk 下的一个函数，可以显示指定单词的出现情况（使用这个函数，指定单词的大小写不敏感），同时还可以显示一些上下文。下面是该函数的使用场景（其中 text1 的内容是 nltk.book 导入后中的 text1）:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; text1.concordance(&quot;monstrous&quot;)</div><div class="line">    Building index...</div><div class="line">    Displaying 11 of 11 matches:</div><div class="line">    ong the former , one was of a most monstrous size . ... This came towards us ,</div><div class="line">    ON OF THE PSALMS . &quot; Touching that monstrous bulk of the whale or ork we have r</div><div class="line">    ll over with a heathenish array of monstrous clubs and spears . Some were thick</div><div class="line">    d as you gazed , and wondered what monstrous cannibal and savage could ever hav</div><div class="line">    that has survived the flood ; most monstrous and most mountainous ! That Himmal</div><div class="line">    they might scout at Moby Dick as a monstrous fable , or still worse and more de</div><div class="line">    th of Radney .&apos;&quot; CHAPTER 55 Of the monstrous Pictures of Whales . I shall ere l</div><div class="line">    ing Scenes . In connexion with the monstrous pictures of whales , I am strongly</div><div class="line">    ere to enter upon those still more monstrous stories of them which are to be fo</div></pre></td></tr></table></figure>
<p>这个函数的具体实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">concordance</span><span class="params">(self, word, width=<span class="number">79</span>, lines=<span class="number">25</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Print a concordance for ``word`` with the specified context window.</div><div class="line">    Word matching is not case-sensitive.</div><div class="line">    :seealso: ``ConcordanceIndex``</div><div class="line">"""</div><div class="line">    </div><div class="line">    <span class="keyword">if</span> <span class="string">'_concordance_index'</span> <span class="keyword">not</span> <span class="keyword">in</span> self.__dict__:</div><div class="line">        print(<span class="string">"Building index..."</span>)</div><div class="line">        self._concordance_index = ConcordanceIndex(self.tokens,</div><div class="line">                                                   key=<span class="keyword">lambda</span> s:s.lower())</div><div class="line"></div><div class="line">    self._concordance_index.print_concordance(word, width, lines)</div></pre></td></tr></table></figure>
<hr>
<h2 id="Word-Sense-Disambiguation-amp-Pronoun-Resolution"><a href="#Word-Sense-Disambiguation-amp-Pronoun-Resolution" class="headerlink" title="Word Sense Disambiguation &amp; Pronoun Resolution"></a>Word Sense Disambiguation &amp; Pronoun Resolution</h2><ul>
<li>Word Sense Disambiguation</li>
</ul>
<p>词义消歧，简而言之，我们需要做的就是分析出特定上下文中的词被赋予的是哪个意思。例如：</p>
<blockquote>
<p>a. <strong>serve</strong>: help with food or drink; hold an office; put ball into  play </p>
<p>b. <strong>dish</strong>: plate; course of a meal; communications device</p>
</blockquote>
<ul>
<li>Pronoun Resolution</li>
</ul>
<p>指代消解，是解决“词义消歧”的一个手段，解决“谁对谁做了什么”，即检测动词的主语和宾语，另外还有 <strong> 语义角色标注</strong>（semantic role labing）— 确定名词短语如何与动词相关联（如代理、受事、工具等）。</p>
<hr>
<h2 id="Text-Corpus-Structure"><a href="#Text-Corpus-Structure" class="headerlink" title="Text Corpus Structure"></a>Text Corpus Structure</h2><p>以下是几种常见的语料库结构：<br><img src="https://farm1.staticflickr.com/445/31263100710_d839312795_o.png" alt=""></p>
<ul>
<li>最简单的一种语料库是一些孤立的没有什么特别结构的文本集合；</li>
<li>一些语料库按如文体（布朗语料库）等分类成组织结构；</li>
<li>一些分类会重叠，如主题类别（路透社语料库）；</li>
<li>另外一些语料库可以表示随时间变化，语言用法的改变（就职演说语料库）；</li>
</ul>
<hr>
<h2 id="WordNet"><a href="#WordNet" class="headerlink" title="WordNet"></a>WordNet</h2><ul>
<li>Senses and Synonyms.（意义与同义词）</li>
<li>Synonyms and Synset.（同义词集与词条）</li>
<li>The WordNet Hierarchy.（WordNet 的层次结构）</li>
</ul>
<blockquote>
<p>WordNet synsets correspond to abstract concepts, and they don’t always have corre- sponding words in English. These concepts are linked together in a hierarchy. Some concepts are very general, such as Entity, State, Event; these are called unique begin- ners or root synsets. Others, such as gas guzzler and hatchback, are much more specific.</p>
</blockquote>
<p>WordNet 概念的层次片段：每个节点对应一个同义词集；边表示上位词 / 下位词关系，即上级概念与从属概念的关系。<br><img src="https://farm1.staticflickr.com/474/31598383846_537809b299_o.png" alt=""></p>
<ul>
<li>Hyponyms and Hypernyms.（下位词与上位词）</li>
<li>Antonyms.（反义词）</li>
</ul>
<hr>
<h1 id="Correct-errors-in-printing"><a href="#Correct-errors-in-printing" class="headerlink" title="Correct errors in printing:"></a>Correct errors in printing:</h1><ul>
<li>P19:</li>
</ul>
<p>在 「Your Trun」 的那块内容中：</p>
<blockquote>
<p>使用 text2 尝试前面频率分布的例子。…如果得到的是错误信息：NameError: name ‘FreqDist’is not defined，则需要在一开始输入  <strong><code>from nltk.book import *</code></strong>。</p>
</blockquote>
<p>需更正为：</p>
<blockquote>
<p>使用 text2 尝试前面频率分布的例子。…如果得到的是错误信息：NameError: name ‘FreqDist’is not defined，则需要在一开始输入 <strong><code>from nltk import *</code></strong>。</p>
</blockquote>
<p><strong>原因：<code>nltk.book</code> 中并不存在 <code>FreqDist()</code> 这一 function.</strong></p>
<hr>
<ul>
<li>P48:</li>
</ul>
<p>在 「Inaugural Address Corpus」 的那块代码部分中：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(</div><div class="line">...           (target, file[:4])</div><div class="line">...           <span class="keyword">for</span> fileid <span class="keyword">in</span> inaugural.fileids()</div></pre></td></tr></table></figure></p>
<p>需更正为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(</div><div class="line">...           (target, fileid[:4])</div><div class="line">...           <span class="keyword">for</span> fileid <span class="keyword">in</span> inaugural.fileids()</div></pre></td></tr></table></figure>
<hr>
<h1 id="Practice"><a href="#Practice" class="headerlink" title="Practice:"></a>Practice:</h1><blockquote>
<p>6.○ In the discussion of comparative wordlists, we created an object called <strong>translate</strong>, which you could look up using words in both German and Italian in order to get corresponding words in English. What problem might arise with this approach? Can you suggest a way to avoid this problem?</p>
</blockquote>
<ul>
<li>如果输入错误（不存在的词语或者其他没有通过 <strong><code>translate.update(dict(xx))</code></strong> 加入字典的语言词语，则会引发 <strong>KeyError</strong>）。其中一个解决办法是，添加一个错误处理情况。</li>
</ul>
<blockquote>
<p>8.◑ Define a conditional frequency distribution over the Names Corpus that allows you to see which initial letters are more frequent for males versus females (see Figure 2-7).</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(</div><div class="line">...            (fileid, name[1]</div><div class="line">...            <span class="keyword">for</span> fileid <span class="keyword">in</span> names.fileids()</div><div class="line">...            <span class="keyword">for</span> name <span class="keyword">in</span> names.words(fileid))</div></pre></td></tr></table></figure>
<blockquote>
<p>14.◑ Define a function <strong><code>supergloss(s)</code></strong> that takes a synset s as its argument and returns a string consisting of the concatenation of the definition of <strong>s</strong>, and the definitions of all the hypernyms and hyponyms of <strong>s</strong>.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">supergloss</span><span class="params">(s)</span>:</span></div><div class="line">    s = wn.synset(<span class="string">'s'</span>)</div><div class="line">    hyponyms_of_s = s.hyponyms()</div><div class="line">    hypernyms_of_s = s.hypernyms()</div><div class="line">    <span class="keyword">return</span> str(s) + str(hyponyms_of_s) + str(hypernyms_of_s)</div></pre></td></tr></table></figure>
<blockquote>
<p>17.◑ Write a function that finds the 50 most frequently occurring words of a text that are not stopwords.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">most_fifty_words</span><span class="params">(text)</span>:</span></div><div class="line">    stopwords = nltk.corpus.stopwords.words(<span class="string">'english'</span>)</div><div class="line">    content = [w <span class="keyword">for</span> w <span class="keyword">in</span> text <span class="keyword">if</span> w.lower() <span class="keyword">not</span> <span class="keyword">in</span> stopwords]</div><div class="line">    fdist = FreqDist(content)</div><div class="line">    vocabulary = list(fdist.keys())</div><div class="line">    <span class="keyword">return</span> vocabulary[:<span class="number">50</span>]</div></pre></td></tr></table></figure>
<hr>
]]></content>
    
    <summary type="html">
    
      本文是关于「Natural Language Processing with Python」这本书的 Chapter 1 &amp; 2 的学习笔记。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://randolph.pro/categories/Machine-Learning/"/>
    
      <category term="Natural Language Processing" scheme="http://randolph.pro/categories/Machine-Learning/Natural-Language-Processing/"/>
    
      <category term="Book:「NLP with Python」" scheme="http://randolph.pro/categories/Machine-Learning/Natural-Language-Processing/Book-%E3%80%8CNLP-with-Python%E3%80%8D/"/>
    
    
      <category term="Machine Learning" scheme="http://randolph.pro/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://randolph.pro/tags/Python/"/>
    
      <category term="Natural Language Processing" scheme="http://randolph.pro/tags/Natural-Language-Processing/"/>
    
  </entry>
  
</feed>
