<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-mac-osx.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Python,Machine Learning," />





  <link rel="alternate" href="/atom.xml" title="黃某人" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="本文是关于周志华「Machine Learning」这本书的 Clustering 的学习笔记。">
<meta name="keywords" content="Python,Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="♞「Machine Learning」 Clustering">
<meta property="og:url" content="http://randolph.pro/2017/01/03/<Machine Learning> Clustering/index.html">
<meta property="og:site_name" content="黃某人">
<meta property="og:description" content="本文是关于周志华「Machine Learning」这本书的 Clustering 的学习笔记。">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://farm5.staticflickr.com/4307/36174772306_62cfbc8cd6_o.jpg">
<meta property="og:image" content="https://farm5.staticflickr.com/4303/36049857562_cf0480eb62_o.png">
<meta property="og:image" content="https://farm1.staticflickr.com/33/31680514905_7dba9b551a_o.jpg">
<meta property="og:image" content="https://farm1.staticflickr.com/353/31565301411_fc7281b525_o.jpg">
<meta property="og:image" content="https://farm5.staticflickr.com/4320/36049858912_c400743b7f_o.png">
<meta property="og:image" content="https://farm6.staticflickr.com/5569/31429001070_b16d937809_o.png">
<meta property="og:image" content="https://farm1.staticflickr.com/403/30990399723_0db34a3cde_o.png">
<meta property="og:image" content="https://farm1.staticflickr.com/715/31427609470_fe9357f0cc_o.png">
<meta property="og:updated_time" content="2017-07-28T08:09:35.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="♞「Machine Learning」 Clustering">
<meta name="twitter:description" content="本文是关于周志华「Machine Learning」这本书的 Clustering 的学习笔记。">
<meta name="twitter:image" content="https://farm5.staticflickr.com/4307/36174772306_62cfbc8cd6_o.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"right","display":"hide","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: 'KGO9YVRPZZ',
      apiKey: 'dcf60767769bda7791ae195caf3dfb10',
      indexName: 'Randolph',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://randolph.pro/2017/01/03/<Machine Learning> Clustering/"/>





  <title>♞「Machine Learning」 Clustering | 黃某人</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?80985281ddae6899cfd57ad9d458b284";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->










</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-right page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">黃某人</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">痴</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://randolph.pro/2017/01/03/<Machine Learning> Clustering/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Randolph">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="黃某人">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">♞「Machine Learning」 Clustering</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-plus-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-01-03T00:00:00+08:00">
                2017-01-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/Book-「Machine-Learning」/" itemprop="url" rel="index">
                    <span itemprop="name">Book:「Machine Learning」</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/01/03/<Machine Learning> Clustering/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/01/03/<Machine Learning> Clustering/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/01/03/<Machine Learning> Clustering/" class="leancloud_visitors" data-flag-title="♞「Machine Learning」 Clustering">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-fire"></i>
               </span>
               <span class="leancloud-visitors-count"></span>
               <span>℃</span>
               <span class="post-meta-divider">|</span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                <span title="Words count in article">
                  8,611
                </span>
                <span> words </span>
                
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                <span title="Reading time">
                  37
                </span>
                <span> mins </span>
              
            </div>
          

          
              <div class="post-description">
                  本文是关于周志华「Machine Learning」这本书的 Clustering 的学习笔记。
              </div>
          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><img src="https://farm5.staticflickr.com/4307/36174772306_62cfbc8cd6_o.jpg" alt=""></p>
<p>关于该书的其他学习笔记系列：<a href="http://randolph.pro/categories/Machine-Learning/Book-「Machine-Learning」/">「Machine Learning」</a></p>
<h1 id="聚类介绍"><a href="# 聚类介绍" class="headerlink" title="聚类介绍"></a>聚类介绍</h1><ul>
<li>聚类是从数据集中挖掘相似观测值集合的方法。</li>
<li>聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”（cluster）。通过这样的划分，每个簇可能对应于一些潜在的概念（类别）。</li>
<li>聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者自己来把握。</li>
<li>聚类既能作为一个单独的过程用于寻找数据内在的分布结构，也可以作为分类等其他学习任务的前驱过程。</li>
</ul>
<hr>
<h2 id="聚类算法"><a href="# 聚类算法" class="headerlink" title="聚类算法"></a>聚类算法 </h2><p><strong> 角度 I：</strong></p>
<ul>
<li><strong>基于原型的聚类（Prototype-based Clustering）</strong><ul>
<li><strong>K 均值聚类（K-means）</strong></li>
<li><strong>学习向量量化聚类（Learning Vector Quantization）</strong></li>
<li><strong>高斯混合模型聚类 （Gaussian Mixture Model）</strong></li>
</ul>
</li>
<li><strong>基于密度的聚类 （Density-based Clustering）</strong><ul>
<li><strong>DBSCAN （Density-Based Spatial Clustering of Application with Noise）</strong></li>
<li><strong>OPTICS （Ordering Points To Identify the Clustering Structure）</strong></li>
</ul>
</li>
<li><strong>层次聚类 （Hierarchical Clustering）</strong></li>
<li><strong>基于模型的聚类 （Model-based Clustering）</strong><ul>
<li><strong>混合回归模型 （Mixture Regression Model）</strong></li>
</ul>
</li>
</ul>
<p><strong>角度 II：</strong></p>
<ul>
<li><strong>基于中心的聚类： kmeans 聚类</strong></li>
<li><strong>基于分布的聚类： GMM 聚类</strong></li>
<li><strong>基于密度的聚类： DBSCAN, OPTICS</strong></li>
<li><strong>基于连通性的聚类： 层次聚类</strong></li>
<li><strong>基于模型的聚类： Miture Regression Model</strong></li>
<li><strong>其他聚类方法： 谱聚类, Chameleon, Canopy…</strong></li>
</ul>
<hr>
<h2 id="聚类数据设置"><a href="# 聚类数据设置" class="headerlink" title="聚类数据设置"></a>聚类数据设置 </h2><p> 假定样本集 $ D $ 包含 $n$ 个无标记样本：</p>
<script type="math/tex; mode=display">
D = \{x_1, x_2, \ldots, x_n \}</script><p>每个样本是一个 $p$ 维特征向量：</p>
<script type="math/tex; mode=display">
x_i=(x_{i1}; x_{i2}; \ldots; x_{ip})</script><p>聚类算法将样本集 $D$ 划分为 $k$ 个不相交的簇：</p>
<script type="math/tex; mode=display">
\{C_l|l=1, 2, \ldots, k\}</script><p>其中， <script type="math/tex">(C_{l^{'}} \cap_{l^{'} \neq l} C_{l} = \emptyset)</script> 且 <script type="math/tex">(D=\cup_{l=1}^{k}C_{l})</script></p>
<p>相应的，用</p>
<script type="math/tex; mode=display">
\lambda_{i} \in {1, 2, \ldots, k}</script><p>表示样本 $x_{i}$ 的“簇标记”（cluster label）, 即：</p>
<script type="math/tex; mode=display">
x_{i} \in C_{\lambda_{i}}</script><p>于是，聚类的结果可用包含 $n$ 个元素的簇标记向量表示：</p>
<script type="math/tex; mode=display">
\lambda=(\lambda_{1}; \lambda_{2}, \ldots, \lambda_{n})</script><hr>
<h2 id="聚类性能度量"><a href="# 聚类性能度量" class="headerlink" title="聚类性能度量"></a>聚类性能度量 </h2><p><strong> 聚类性能度量亦称聚类“有效性指标”（validity index）。</strong></p>
<p><strong>设置聚类性能度量的目的:</strong></p>
<ul>
<li>对聚类结果，通过某种性能度量来评估其好坏；</li>
<li>若明确了最终将要使用的性能度量，则可直接将其作为聚类过程的优化目标，从而更好地得到符合要求的聚类结果。</li>
</ul>
<p><strong>什么样的聚类结果比较好？</strong></p>
<ul>
<li>“簇内相似度”（intra-cluster similarity）高</li>
<li>“蔟间相似度”（inter-cluster similarity）低</li>
</ul>
<p><strong>聚类性能度量分类：</strong></p>
<ul>
<li>“外部指标”（external index）：将聚类结果与某个“参考模型”（reference model）进行比较</li>
<li>“内部指标”（internal index）：直接考察聚类结果而不利用任何参考模型</li>
</ul>
<h3 id="外部指标"><a href="# 外部指标" class="headerlink" title="外部指标"></a>外部指标 </h3><p> 对数据集  <script type="math/tex">D = \{x_1, x_2, \ldots, x_n\}</script>，假定通过聚类，给出的簇划分为 <script type="math/tex">V=\{v_{1}, v_{2}, \ldots, v_{C}\}</script>，参考模型给出的簇划分为 <script type="math/tex">U=\{u_{1}, u_{2}\, \ldots, u_{R}\}</script>。相应地，令<script type="math/tex">\lambda</script> 与 <script type="math/tex">\lambda^{*}</script> 分别表示与 $V$ 和 $U$ 对应的簇标记向量。我们将样本两两配对考虑，定义：</p>
<script type="math/tex; mode=display">
\begin{align}
a & = |SS|, SS =\left \{(x_i,x_j)|(\lambda_i = \lambda_j, \lambda_i^* =  \lambda_j^* , i < j)\right \} \cr
b & = |SD|, SD =\left \{(x_i,x_j)|(\lambda_i = \lambda_j, \lambda_i^* \neq  \lambda_j^* , i < j)\right \} \cr
c & = |DS|, DS =\left \{(x_i,x_j)|(\lambda_i \neq \lambda_j, \lambda_i^* =  \lambda_j^* , i < j)\right \} \cr
d & = |DD|, DD =\left \{(x_i,x_j)|(\lambda_i \neq \lambda_j, \lambda_i^* \neq  \lambda_j^* , i < j)\right \}
\end{align}</script><p>其中：</p>
<ul>
<li>集合 $SS$ 包含了在 $V$ 中隶属于相同簇且在 $U$ 中也隶属于相同簇的样本对；</li>
<li>集合 $SD$ 包含了在 $V$ 中隶属于相同簇且在 $U$ 中也隶属于不同簇的样本对；</li>
<li>集合 $DS$ 包含了在 $V$ 中隶属于不同簇且在 $U$ 中也隶属于相同簇的样本对；</li>
<li>集合 $DD$ 包含了在 $V$ 中隶属于不同簇且在 $U$ 中也隶属于不同簇的样本对；</li>
</ul>
<p>这样，由于每个样本对 <script type="math/tex">((x_{i}, x_{j})(i<j))</script> 仅能出现在一个集合中，因此有：</p>
<script type="math/tex; mode=display">
a+b+c+d=n(n-1)/2</script><h4 id="JC-Jaccard-Coefficient"><a href="#JC-Jaccard-Coefficient" class="headerlink" title="JC(Jaccard Coefficient)"></a>JC(Jaccard Coefficient)</h4><script type="math/tex; mode=display">
JC = \frac{a}{a+b+c}</script><p>JC 系数的结果分布在 $[0,1]$ 区间，值越大越好。个人总结，JC 系数是一个比较“爱憎分明”的相似性度量指标，对于聚类效果越好的模型的 JC 计算结果会很好，对于聚类效果越差的模型的 JC 计算结果会很差。关于 JC 系数的数学解释及实际意义，可以参考 Wiki 对应词条，讲的非常清晰：<a href="https://en.wikipedia.org/wiki/Jaccard_index" target="_blank" rel="external">Jaccard index</a>。</p>
<p>在此我摘取其中的重点：</p>
<blockquote>
<p>When used for binary attributes, the Jaccard index is very similar to the Simple matching coefficient. The main difference is that the SMC has the term $M_{00}$ in its numerator and denominator, whereas the Jaccard index does not. Thus, the SMC compares the number of matches with the entire set of the possible attributes, whereas the Jaccard index only compares it to tshe attributes that have been chosen by at least A or B.</p>
<p><strong>In market basket analysis for example, the basket of two consumers who we wish to compare might only contain a small fraction of all the available products in the store, so the SMC would always return very small values compared to the Jaccard index. Using the SMC would then induce a bias by systematically considering, as more similar, two customers with large identical baskets compared to two customers with identical but smaller baskets; thus making the Jaccard index a better measure of similarity in that context.</strong></p>
<p>In other contexts, where 0 and 1 carry equivalent information (symmetry), the SMC is a better measure of similarity. For example, vectors of demographic variables stored in dummies variables, such as gender, would be better compared with the SMC than with the Jaccard index since the impact of gender on similarity should be equal, independent of whether male is defined as a 0 and female as a 1 or the other way around. However, when we have symmetric dummy variables, one could replicate the behaviour of the SMC by splitting the dummies into two binary attributes (in this case, male and female), thus transforming them into asymmetric attributes, allowing the use of the Jaccard index without introducing the bias. By using this trick, the Jaccard index can be considered as making the SMC a fully redundant metric. The SMC remains however more computationally efficient in the case of symmetric dummy variables since it doesn’t require adding extra dimensions.</p>
<p><strong>In general, the Jaccard index can be considered as an indicator of local “similarity” while SMC evaluates “similarity” relative to the whole “universe”.</strong>Similarity and dissimilarity must be understood in a relative sense. For example, if there are only 2 attributes (x,y), then A=(1,0) is intuitively very different from B=(0,1). However if there are 10 attributes in the “universe”, A=(1,0,0,0,0,0,0,0,0,0) and B=(0,1,0,0,0,0,0,0,0) are not intuitively so different anymore. If the focus comes back to be just on A and B, the remaining 8 attributes are often considered as redundant. As a result, A and B are very different in a “local” sense (which the Jaccard Index measures efficiently), but less different in a “global” sense (which the SMC measures efficiently). From this point of view, the choice of using SMC or the Jaccard index comes down to more than just symmetry and asymmetry of information in the attributes. The distribution of sets in the defined “universe” and the nature of the problem to be modeled should also be considered.</p>
<p>The Jaccard index is also more general than the SMC and can be used to compare other data types than just vectors of binary attributes, such as Probability measures.</p>
</blockquote>
<p>上段重点的大体意思是：JC 的一个适用场景，例如商场或者电商（亚马逊）的用户们，在买东西的时候，我们对其相似性进行判断的时候，SMC（Simple Matching Coefficient）简单匹配系数并不太适用，是因为 SMC 中添加了（两个顾客都不感兴趣的商品这一信息），而 JC 并没有考虑这一部分。对于琳琅满目的商品信息而言，两个顾客不感兴趣的商品应该远远多于他们感兴趣的商品，简而言之，如果将顾客对所有的商品向量进行标记，感兴趣的为 1，不感兴趣的为 0，那么得到的这个向量应当是一个非常稀疏的。在这种情况下，如果使用 SMC，势必会导致结果很小，但是实际上我们完全可以不考虑两个顾客都不感兴趣的内容，而是考虑两个顾客感兴趣的商品信息之和，对其进行计算，相对于 SMC，得到的结果会好解释许多。就像那句话说的，<strong>JC 就好比是一个局域性的“相似性”比较的衡量指标，而 SMC 就好比是要考虑整个宏观宇宙之下的“相似性”比较</strong>。</p>
<h4 id="FMI-Fowlkes-and-Mallows-Index"><a href="#FMI-Fowlkes-and-Mallows-Index" class="headerlink" title="FMI(Fowlkes and Mallows Index)"></a>FMI(Fowlkes and Mallows Index)</h4><script type="math/tex; mode=display">
FMI = \sqrt{\frac{a}{a+b}\cdot \frac{a}{a+c}}</script><p>FMI 系数的结果分布在 $[0,1]$ 区间，值越大越好。个人总结，FMI 系数是一个比较“温文儒雅”的相似性度量指标，对于聚类效果特别好的模型的 FMI 计算结果不会好得特别夸张，对于聚类效果特别越差的模型的 FMI 计算结果也不会差得特别夸张。同样，FMI 系数的数学解释及实际意义，对应的 Wiki 词条：<a href="https://en.wikipedia.org/wiki/Fowlkes–Mallows_index" target="_blank" rel="external">Fowlkes–Mallows index</a>。</p>
<p>在此我摘取其中的重点：</p>
<blockquote>
<p>Since the index is directly proportional to the number of true positives, a higher index means greater similarity between the two clusterings used to determine the index. <strong>One of the most basic thing to test the validity of this index is to compare two clusterings that are unrelated to each other.</strong> Fowlkes and Mallows showed that on using two unrelated clusterings, the value of this index approaches zero as the number of total data points chosen for clustering increase; whereas the value for the Rand index for the same data quickly approaches making Fowlkes–Mallows index a much more accurate representation for unrelated data. <strong>This index also performs well if noise is added to an existing dataset and their similarity compared. Fowlkes and Mallows showed that the value of the index decreases as the component of the noise increases.</strong> The index also showed similarity even when the noisy dataset had a different number of clusters than the clusters of the original dataset. Thus making it a reliable tool for measuring similarity between two clusters.</p>
</blockquote>
<p>上段重点的大体意思是：“衡量一个相似性度量指标是否可靠，应当将对于两个完全不相关簇的研究也作为重要的判定基础之一”，这样讲可能太绕了，解释一下就是如果我们对一个相似性度量指标的可靠性进行判断，不仅仅需要：我们的模型与“参考模型”两个模型越相似（聚类效果越好），指标系数越大（或越小）；同样的，我们的模型与“参考模型”越不相似（聚类效果越差），指标系数也应该呈现一个单调变大（或变小）的趋势。</p>
<p>FMI 与其他相似性度量指标的区别是，特别是与 JC 系数对比，JC 系数 $[0,1]$ 的特点是假如我们的模型聚类效果越好，结果就越趋向于 1，反之，如果我们的模型聚类效果越不好，结果就越趋向于 0；而 FMI $[0,1]$ 则不会显得那么“爱憎分明”，因为在 $[0,1]$ 范围内根号运算的存在，使得模型越好的结果不会好得特别夸张，但是模型越差的结果也不会显得特别夸张，因为根号运算起到一个“缓冲 ”的作用。另外一点，就是 FMI 系数对于含有噪声的数据集的判定效果仍然不错。</p>
<h4 id="RI-Rand-Index"><a href="#RI-Rand-Index" class="headerlink" title="RI(Rand Index)"></a>RI(Rand Index)</h4><script type="math/tex; mode=display">
RI = \frac{a+d}{a+b+c+d}</script><p>RI 系数的结果分布在 $[0,1]$ 区间，值越大越好。关于 RI 系数的数学解释及实际意义，可以参考 Wiki 对应词条，重点是要看它的改进指标 ARI：<a href="https://en.wikipedia.org/wiki/Rand_index" target="_blank" rel="external">Rand  index</a>。</p>
<h4 id="ARI-Adjusted-Rand-Index"><a href="#ARI-Adjusted-Rand-Index" class="headerlink" title="ARI(Adjusted Rand Index)"></a>ARI(Adjusted Rand Index)</h4><p>  在 <strong>RI（Rand Index）</strong> 的评判基础上，为了实现“在聚类结果随机产生的情况下，指标应该接近零”，<strong>ARI（Adjusted Rand Index）</strong>系数被提出，它具有更高的区分度。</p>
<blockquote>
<p>A problem with the Rand index is that the expected value of the Rand index of two random partitions does not take a constant value (say zero). The adjusted Rand index proposed by [Hubert and Arabie, 1985] assumes the generalized hypergeometric distribution as the model of randomness, i.e., the $U$ and $V$ partitions are picked at random such that the number of objects in the classes and clusters are fixed.</p>
<script type="math/tex; mode=display">
ARI  = \frac{Rand \ Index - expected \ index}{maximum \ index - expected \ index} = \frac{RI - E(RI)}{1-E(RI)}</script></blockquote>
<p>进行进一步的展开推导后：</p>
<script type="math/tex; mode=display">
API = \frac{\frac{a+d}{a+b+c+d} - \frac{(a+b)(a+c)+(c+d)(b+d)}{(a+b+c+d)^2}}{\frac{(a+b)(a+c)+(c+d)(b+d)}{(a+b+c+d)^2}}</script><p>ARI 系数结果分布在 $[-1,1]$ 区间，负数代表结果不好，越接近于 1 意味着聚类结果与真实情况越吻合。个人总结，ARI 系数对于任意数量的聚类中心和样本数，随机聚类的 ARI 都非常接近于 0。ARI 相比于 RI 好在，它是负数的时候就说明我们自己的模型很糟糕，有个更加相对明确的评判标准。（从广义的角度上来说，ARI 衡量的是两个数据分布的吻合程度）</p>
<h4 id="举个具体例子计算 -ARI"><a href="# 举个具体例子计算 -ARI" class="headerlink" title="举个具体例子计算 ARI"></a>举个具体例子计算 ARI</h4><p>Let <script type="math/tex">n_{ij}</script> be the number of objects that are in both class <script type="math/tex">u_{i}</script> and cluster <script type="math/tex">v_{j}</script>. Let <script type="math/tex">n_{i.}</script> and <script type="math/tex">n_{.j}</script> be the number of objects in class <script type="math/tex">u_{i}</script> and cluster <script type="math/tex">v_{j}</script> respectively.The notations are illustrated in Table below:</p>
<script type="math/tex; mode=display">
\begin{array}{c|cccc|c}
Class \ Cluster & v_{1}  & v_{2} & ... & v_{C} & Sums \cr
\hline
u_{1} & n_{11} & n_{12} & ... & n_{1C} & n_{1.}  \cr
u_{2} & n_{21} & n_{22} & ... & n_{2C} & n_{2.}  \cr
... & ... & ... & ... & ... & ...  \cr
u_{R} & n_{R1} & n_{R2} & ... & n_{RC} & n_{R.}  \cr
\hline
Sums & n_{.1} & n_{.2} & ... & n_{.C} & n.. = n  \cr
\end{array}</script><p>Here is the example:</p>
<script type="math/tex; mode=display">
\begin{array}{c|ccc|c}
Class \ Cluster & v_{1}  & v_{2} &  v_{3} & Sums \cr
\hline
u_{1} & 1 & 1 &  0 & 2  \cr
u_{2} & 1 & 2 &  1 & 4  \cr
u_{3} & 0 & 0 &  4 & 4  \cr
\hline
Sums & 2 & 3 & 5 & n = 10  \cr
\end{array}</script><p>$a$ is defined as the number of pairs of objects in the same class $U$ and same cluster in $V$,hence $a$ can be written as <script type="math/tex">\sum_{i,j} \binom{n_{ij}}{2}</script>.In Example,<script type="math/tex">a = \binom{2}{2} + \binom{4}{2} = 7</script>(所有不超过 2 的都不需要考虑，因为<script type="math/tex">\binom{1}{2} = 0</script>).</p>
<p>$b$ is defined as the number of pairs of objects in the same class in $U$ but not in the same cluster in $V$.In terms of the notation in Table, $b$ can be writtern as <script type="math/tex">\sum_{i}\binom{n_{i.}}{2}-\sum_{i,j}\binom{n_{ij}}{2}</script>. In Example, <script type="math/tex">b = \binom{2}{2} + \binom{4}{2} + \binom{4}{2} - 7 = 6</script>.</p>
<p>Similarly, $c$ is defined as the number of pairs of objects in the same cluster in $V$ but not in the same class in $U$, so $c$ can be writtern as <script type="math/tex">\sum_{j}\binom{n_{.j}}{2}-\sum_{i,j}\binom{n_{ij}}{2} = \binom{2}{2} + \binom{3}{2} + \binom{5}{2} -7 = 7</script>.</p>
<p>$d$ is defined as the number of pairs of objects that are not in the same class in $U$ and not in the same cluster in $V$. Since <script type="math/tex">a + b + c + d = \binom{n}{2}</script>, <script type="math/tex">d = \binom{10}{2} -7 - 6 - 7 = 25</script>.</p>
<p>The Rand Index for comparing the two partitions in Example is $\frac{7+25}{45} = 0.711$, while the adjusted Rand Index is 0.311.</p>
<p><strong>The Rand index is much higher than the adjusted Rand index, which is typical. Since the Rand index lies between 0 and 1, the expected value of the Rand index (although not a constant value) must be greater than or equal to 0.On the other hand, the expected value of the adjusted Rand index has value zero and the maximum value of the adjusted Rand is  also 1. Hence, there is a wider range of values that the adjusted Rand index can take on, thus increasing the sensitivity of the index.</strong></p>
<h3 id="内部指标"><a href="# 内部指标" class="headerlink" title="内部指标"></a>内部指标 </h3><p> 根据聚类结果的簇划分 <script type="math/tex">(C={C_{1}, C_{2}, \ldots, C_{k}})</script> , 定义：</p>
<ul>
<li>簇 $C$ 内样本间的平均距离</li>
</ul>
<script type="math/tex; mode=display">
avg(C)=\frac{2}{|C|(|C|-1)}\sum_{1<i<j<|C|}dist(x_{i}, x_{j})</script><ul>
<li><p>簇 $C$ 内样本间的最远距离</p>
<script type="math/tex; mode=display">
diam(C)=max_{1<i<j<|C|}dist(x_{i}, x_{j})</script></li>
<li><p>簇 <script type="math/tex">C_{i}</script> 与簇 <script type="math/tex">C_{j}</script> 最近样本间的距离</p>
<script type="math/tex; mode=display">
d{min}(C_{i}, C_{j})=min_{1<i<j<|C|}dist(x_{i}, x_{j})</script></li>
<li>簇 <script type="math/tex">C_{i}</script> 与簇 <script type="math/tex">C_{j}</script> 中心点间的距离<script type="math/tex; mode=display">
d_{cen}(C_{i}, C_{j})=dist(\mu_{i}, \mu_{j})</script></li>
</ul>
<p>其中：</p>
<ul>
<li>$dist(,)$ 是两个样本之间的距离</li>
<li>$\mu$ 是簇 $C$ 的中心点 <script type="math/tex">\mu=\frac{1}{|C|}\sum_{1<i<|C|}x_{i}</script></li>
</ul>
<h4 id="CP-Compactness"><a href="#CP-Compactness" class="headerlink" title="CP(Compactness)"></a>CP(Compactness)</h4><script type="math/tex; mode=display">
\begin{align}
\overline{CP_{i}} & =\frac{1}{|C_{i}|}\sum_{x_{i} \in C_{i}} dist(x_{i}, \mu_{i}) \cr
\overline{CP} & =\frac{1}{k}\sum_{k=1}^{k} \overline{CP_{k}}
\end{align}</script><p>CP 紧密性，其计算的是每个簇中各个点到簇中心的平均距离，值越低意味着簇内聚类距离越近，缺点就是没有考虑到簇间效果。</p>
<h4 id="SP-Separation"><a href="#SP-Separation" class="headerlink" title="SP(Separation)"></a>SP(Separation)</h4><script type="math/tex; mode=display">
SP=\frac{2}{k^2-k}\sum^{k}_{i=1}\sum^{k}_{j=i+1} d_{cen}(\mu_{i}, \mu_{j})</script><p>SP 间隔性，其计算的是各簇中心两两之间的平均距离，值越高意味着簇间距离越远，缺点是没有考虑簇内效果。</p>
<h4 id="DBI-Davies-Bouldin-Index"><a href="#DBI-Davies-Bouldin-Index" class="headerlink" title="DBI(Davies-Bouldin Index)"></a>DBI(Davies-Bouldin Index)</h4><script type="math/tex; mode=display">
DBI=\frac{1}{k}\sum^{k}_{i=1}\underset{j \neq i}{max}\bigg(\frac{avg(C_{i})+avg(C_{j})}{d_{cen}(\mu_{i}, \mu_{j})}\bigg)</script><p>关于 DBI 系数的数学解释及实际意义，可以参考 Wiki 对应词条：<a href="https://en.wikipedia.org/wiki/Davies–Bouldin_index" target="_blank" rel="external">Davies-Bouldin Index</a>。</p>
<blockquote>
<p>These conditions constrain the index so defined to be symmetric and non-negative. Due to the way it is defined, as a function of the ratio of the within cluster scatter, to the between cluster separation, a lower value will mean that the clustering is better. <strong>It happens to be the average similarity between each cluster and its most similar one, averaged over all the clusters, where the similarity is defined as $S_{i}$ above.</strong> This affirms the idea that no cluster has to be similar to another, and hence the best clustering scheme essentially minimizes the Davies–Bouldin index. This index thus defined is an average over all the $i$ clusters, and hence a good measure of deciding how many clusters actually exists in the data is to plot it against the number of clusters it is calculated over. The number <em>i</em> for which this value is the lowest is a good measure of the number of clusters the data could be ideally classified into. This has applications in deciding the value of $k$ in the <a href="https://en.wikipedia.org/wiki/Kmeans" target="_blank" rel="external">kmeans</a> algorithm, where the value of k is not known apriori. </p>
</blockquote>
<p>DBI 系数结果为非负数，值越小意味着簇内距离越小，同时簇间距离越大。个人总结，DBI 其实就是将几个 <script type="math/tex">d_{max}</script> 的值叠加，当我们模型最后聚集了多少个 Cluster 簇，就会出现多少个 <script type="math/tex">d_{max}</script>，这些<script type="math/tex">d_{max}</script> 值叠加的结果越小，说明我们模型的聚类越合理。可以通过 DBI 系数来确定 k-means 中 $k$ 的最佳值，即不同的 $k$ 值对应不同的 DBI 结果，选取 DBI 结果最小时对应的 $k$ 值，意味着此时这样划分 $k$ 个簇是最合理，效果最佳的。其缺点是，因为使用的是欧式距离，所以对于环状分布，聚类评测比较糟糕。</p>
<h4 id="DI-Dunn-Index"><a href="#DI-Dunn-Index" class="headerlink" title="DI(Dunn Index)"></a>DI(Dunn Index)</h4><script type="math/tex; mode=display">
DI=\underset{1 \leqslant i \leqslant k}{min}\bigg\{\underset{j \neq i}{min}\bigg(\frac{d_{min}(C_{i}, C_{j})}{max_{1 \leqslant l \leqslant k}diam(C_{l})}\bigg)\bigg\}</script><p>关于 DI 系数的数学解释及实际意义，可以参考 Wiki 对应词条：<a href="https://en.wikipedia.org/wiki/Dunn_index" target="_blank" rel="external">Dunn Index</a>。</p>
<blockquote>
<p>Being defined in this way, the <em>DI</em> depends on $k$, the number of clusters in the set. If the number of clusters is not known apriori, the $k$ for which the <em>DI</em> is the highest can be chosen as the number of clusters. There is also some flexibility when it comes to the definition of d(x,y) where any of the well known metrics can be used, like <a href="https://en.wikipedia.org/wiki/Manhattan_distance" target="_blank" rel="external">Manhattan distance</a> or <a href="https://en.wikipedia.org/wiki/Euclidean_distance" target="_blank" rel="external">Euclidean distance</a> based on the geometry of the clustering problem. <strong>This formulation has a peculiar problem, in that if one of the clusters is badly behaved, where the others are tightly packed, since the denominator contains a ‘max’ term instead of an average term, the Dunn Index for that set of clusters will be uncharacteristically low. This is thus some sort of a worst case indicator, and has to be kept in mind.</strong></p>
</blockquote>
<p>DI 系数结果为非负数，值越大意味着簇间距离越大，同时簇内距离越小。个人总结，DI 系数计算的是，任意两个簇的最短距离（簇间）除以任意簇中的最大距离（簇内），其优点是对于离散点的聚类评测效果不错，但缺点是对于环状分布评测效果比较差。还有上述重点提到的，因为分母采用的是取一个 max 距离而不是取平均值 average，所以会出现一个奇怪的问题，对于某个聚类效果特别差的一个簇，它可能”一个老鼠屎坏了一锅粥”这种，导致 DI 系数会特别低，这也是当我们需要使用 DI 系数时，对于某些聚类表现非常差的簇需要注意的原因。</p>
<h2 id="聚类距离计算"><a href="# 聚类距离计算" class="headerlink" title="聚类距离计算"></a>聚类距离计算 </h2><p><strong> 距离度量（distance measure）函数 $dist(,)$ 需满足的基本性质：</strong></p>
<ul>
<li><strong>非负性</strong>：<script type="math/tex">dist(x_{i}, x_{j}) \geqslant 0</script></li>
<li><strong>同一性</strong>：<script type="math/tex">dist(x_{i}, x_{j})=0</script> 当且仅当 <script type="math/tex">x_{i}=x_{j}</script></li>
<li><strong>对称性</strong>：<script type="math/tex">dist(x_{i}, x_{j})=dist(x_{j}, x_{i})</script></li>
<li><strong>直递性</strong>：<script type="math/tex">dist(x_{i}, x_{j}) \leqslant dist(x_{i}, x_{k}) + dist(x_{k}, x_{j})</script> (可不满足)</li>
</ul>
<p><strong>变量属性：</strong></p>
<ul>
<li>连续属性： 闵可夫斯基距离</li>
<li>离散属性<ul>
<li>有序属性： 闵可夫斯基距离</li>
<li>无序属性：VDM (Value Difference Metric)</li>
</ul>
</li>
<li>混合属性：闵可夫斯基距离 与 VDM 混合距离</li>
</ul>
<h3 id="闵可夫斯基距离（Minkowski-distance）"><a href="# 闵可夫斯基距离（Minkowski-distance）" class="headerlink" title="闵可夫斯基距离（Minkowski distance）"></a>闵可夫斯基距离（Minkowski distance）</h3><p>样本：<script type="math/tex">x_{i}=(x_{i1}, x_{i2}, \ldots, x_{in})</script> 与 <script type="math/tex">x_{j}=(x_{j1}, x_{j2}, \ldots, x_{jn})</script></p>
<script type="math/tex; mode=display">
dist_{mk}(x_{i}, x_{j})=\bigg(\sum_{u=1}^{n}|x_{iu}-x_{ju}|^{p}\bigg)^{\frac{1}{p}}</script><h3 id="VDM-Value-Difference-Metric"><a href="#VDM-Value-Difference-Metric" class="headerlink" title="VDM(Value Difference Metric)"></a>VDM(Value Difference Metric)</h3><blockquote>
<p>我们常将属性划分为“连续属性”（continuous attribute）和“离散属性”（categorical attribute），前者在定义域上有无穷多个可能的取值，后者在定义域上是有限个取值。然而，在讨论距离计算时，属性上是否定义了“序”关系更为重要．例如定义域为 $\lbrace 1,2,3 \rbrace$ 的离散属性与连续属性的性质更接近一些， 能直接在属性值上计算距离：“1”与“2”比较接近、与“3”比较远，这样的属性称为“有序属性”（ordinal attribute）; 而定义域为｛飞机，火车，轮船｝这样的离散属性则不能直接在属性值上计算距离，称为“无序属性”（non-ordinal attribute）。显然， 闵可夫斯基距离可用于有序属性。</p>
<p>对无序属性可采用 VDM（Value Difference Metric） [Stanfill and Waltz, 1986]。</p>
</blockquote>
<p>令 <script type="math/tex">m_{u,a}</script> 表示在属性 $u$ 上取值为 $a$ 的样本数，<script type="math/tex">m_{u, a, i}</script> 表示在第 $i$ 个样本簇中在属性 $u$ 上取值为 $a$ 的样本数，$k$ 为样本簇数，则属性 $u$ 上两个离散值 $a$ 与 $b$ 之间的 VDM 距离为：</p>
<script type="math/tex; mode=display">
VDM_{q}(a, b)=\sum^{k}_{i=1}\bigg|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}\bigg|^{p}.</script><p>（这里的 $p$ 同闵可夫斯基距离中的 $p$ 一样）</p>
<h3 id="闵可夫斯基距离与 VDM 混合距离"><a href="# 闵可夫斯基距离与 VDM 混合距离" class="headerlink" title="闵可夫斯基距离与 VDM 混合距离"></a>闵可夫斯基距离与 VDM 混合距离 </h3><p> 假设有 <script type="math/tex">n_{c}</script> 个有序属性，<script type="math/tex">n-n_{c}</script> 个无序属性，有序属性排列在无序属性之前：</p>
<script type="math/tex; mode=display">
MinkovDM_{p}(x_{i}, x_{j})=\bigg(\sum^{n_{c}}_{u=1}|x_{i,u}-x_{j,u}|^{p}+\sum^{n}_{u=n_{c}+1}VDM_{p}(x_{i,u},x_{j,u})\bigg)^{\frac{1}{p}}</script><h3 id="加权闵可夫斯基距离"><a href="# 加权闵可夫斯基距离" class="headerlink" title="加权闵可夫斯基距离"></a>加权闵可夫斯基距离 </h3><p> 当样本在空间中不同属性的重要性不同时：</p>
<script type="math/tex; mode=display">
dist_{wmk}(x_{i}, x_{j})=(w_{1}\cdot|x_{i1}-x_{j1}|^{p}+w_{2}\cdot|x_{i2}-x_{j2}|^{p}+\ldots+w_{n}\cdot|x_{in}-x_{jn}|^{p})^{\frac{1}{p}}</script><p>其中： 权重<script type="math/tex">w_{i} \geqslant 0(i=1, 2, \ldots, p)</script> 表示不同属性的重要性，通常 <script type="math/tex">\sum_{i=1}^{n}w_{i}=1</script>.</p>
<hr>
<h1 id="聚类算法介绍及实现"><a href="# 聚类算法介绍及实现" class="headerlink" title="聚类算法介绍及实现"></a>聚类算法介绍及实现 </h1><p><strong> 聚类算法类型：</strong></p>
<ul>
<li><p><strong>基于原型的聚类（Prototype-based Clustering）</strong></p>
<ul>
<li><strong>K 均值聚类（K-means）</strong></li>
<li><strong>学习向量量化聚类（Learning vector Quantization）</strong></li>
<li><strong>高斯混合聚类（Mixture-of-Gaussian）</strong></li>
</ul>
</li>
<li><p><strong>基于密度的聚类（Density-based Clustering）</strong></p>
</li>
<li><p><strong>层次聚类（Hierarchical Clustering）</strong></p>
</li>
</ul>
<hr>
<h2 id="基于原型的聚类"><a href="# 基于原型的聚类" class="headerlink" title="基于原型的聚类"></a>基于原型的聚类 </h2><p><strong> 基于原型的聚类（Prototype-based Clustering），此类算法假设聚类结构能通过一组原形刻画。通常情况下，算法先对原型进行初始化，然后对原型进行迭代更新求解，采用不同的原型表示，不同的求解方式，将产生不同的算法。</strong></p>
<ul>
<li><strong>基于原型的聚类（Prototype-based Clustering）</strong><ul>
<li><strong>K 均值聚类（K-means）</strong></li>
<li><strong>学习向量量化聚类（Learning vector Quantization）</strong></li>
<li><strong>高斯混合聚类（Mixture-of-Gaussian）</strong></li>
</ul>
</li>
</ul>
<h3 id="K 均值聚类（K-means）"><a href="#K 均值聚类（K-means）" class="headerlink" title="K 均值聚类（K-means）"></a>K 均值聚类（K-means）</h3><h4 id="算法介绍"><a href="# 算法介绍" class="headerlink" title="算法介绍"></a>算法介绍 </h4><p> 给定样本集 <script type="math/tex">D= \lbrace x_{1}, x_{2}, \ldots, x_{n} \rbrace</script>， K-means 算法针对聚类所得簇划分 <script type="math/tex">C= \lbrace C_{1}, C_{2}, \ldots, C_{k} \rbrace</script> , 最小化平方误差：</p>
<script type="math/tex; mode=display">
E = \sum^{k}_{i=1}\sum_{x \in C_{i}}|x-\mu_{i}|_{2}^{2}</script><p>其中 <script type="math/tex">\mu_{i}</script> 是簇 <script type="math/tex">C_{i}</script> 的均值向量：</p>
<script type="math/tex; mode=display">
\mu_{i}=\frac{1}{|C_{i}|}\sum_{x \in C_{i}}x</script><p>直观上看，$E$ 在一定程度上刻画了簇内样本围绕均值向量的紧密程度， $E$ 值越小簇内样本相似度越高。但最小化 $E$ 不容易，是一个 NP 难问题， K-means 算法采用了贪心策略，通过迭代优化来近似求解 $E$ 的最小值。具体算法如下：</p>
<p><img src="https://farm5.staticflickr.com/4303/36049857562_cf0480eb62_o.png" alt="$k$ 均值算法"></p>
<h4 id="算法实现 -（R 语言）"><a href="# 算法实现 -（R 语言）" class="headerlink" title="算法实现 （R 语言）"></a>算法实现 （R 语言）</h4><p>Python 的实现可以参考<a href="http://randolph.pro/2016/03/19/%3CProgramming%20Collective%20Intelligence%3E%20Chapter%203/">「Programming Collective Intelligence」 Chapter 3</a> 这篇文章。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">head(iris)</div></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">  Sepal.Length Sepal.Width Petal.Length Petal.Width Species</div><div class="line"><span class="number">1</span>          <span class="number">5.1</span>         <span class="number">3.5</span>          <span class="number">1.4</span>         <span class="number">0.2</span>  setosa</div><div class="line"><span class="number">2</span>          <span class="number">4.9</span>         <span class="number">3.0</span>          <span class="number">1.4</span>         <span class="number">0.2</span>  setosa</div><div class="line"><span class="number">3</span>          <span class="number">4.7</span>         <span class="number">3.2</span>          <span class="number">1.3</span>         <span class="number">0.2</span>  setosa</div><div class="line"><span class="number">4</span>          <span class="number">4.6</span>         <span class="number">3.1</span>          <span class="number">1.5</span>         <span class="number">0.2</span>  setosa</div><div class="line"><span class="number">5</span>          <span class="number">5.0</span>         <span class="number">3.6</span>          <span class="number">1.4</span>         <span class="number">0.2</span>  setosa</div><div class="line"><span class="number">6</span>          <span class="number">5.4</span>         <span class="number">3.9</span>          <span class="number">1.7</span>         <span class="number">0.4</span>  setosa</div></pre></td></tr></table></figure>
<p>After a little bit of exploration, I found that Petal.Length and Petal.Width were similar among the same species but varied considerably between different species, as demonstrated below:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(ggplot2)</div><div class="line">ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()</div></pre></td></tr></table></figure>
<p><img src="https://farm1.staticflickr.com/33/31680514905_7dba9b551a_o.jpg" alt=""></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">set.seed(<span class="number">20</span>)</div><div class="line">irisCluster &lt;- kmeans(iris[, <span class="number">3</span>:<span class="number">4</span>], centers = <span class="number">3</span>, nstart = <span class="number">20</span>)</div><div class="line">irisCluster</div></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">K-means clustering with <span class="number">3</span> clusters of sizes <span class="number">50</span>, <span class="number">52</span>, <span class="number">48</span></div><div class="line"></div><div class="line">Cluster means:</div><div class="line">  Petal.Length Petal.Width</div><div class="line"><span class="number">1</span>     <span class="number">1.462000</span>    <span class="number">0.246000</span></div><div class="line"><span class="number">2</span>     <span class="number">4.269231</span>    <span class="number">1.342308</span></div><div class="line"><span class="number">3</span>     <span class="number">5.595833</span>    <span class="number">2.037500</span></div><div class="line"></div><div class="line">Clustering vector:</div><div class="line">  [<span class="number">1</span>] <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></div><div class="line"> [<span class="number">33</span>] <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span></div><div class="line"> [<span class="number">65</span>] <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">3</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">3</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span></div><div class="line"> [<span class="number">97</span>] <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">2</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">2</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">2</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">2</span> <span class="number">3</span></div><div class="line">[<span class="number">129</span>] <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">2</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span></div><div class="line"></div><div class="line">Within cluster sum of squares by cluster:</div><div class="line">[<span class="number">1</span>]  <span class="number">2.02200</span> <span class="number">13.05769</span> <span class="number">16.29167</span></div><div class="line"> (between_SS / total_SS =  <span class="number">94.3</span> %)</div><div class="line"></div><div class="line">Available components:</div><div class="line"></div><div class="line">[<span class="number">1</span>] <span class="string">"cluster"</span>      <span class="string">"centers"</span>      <span class="string">"totss"</span>        <span class="string">"withinss"</span>    </div><div class="line">[<span class="number">5</span>] <span class="string">"tot.withinss"</span> <span class="string">"betweenss"</span>    <span class="string">"size"</span>         <span class="string">"iter"</span>        </div><div class="line">[<span class="number">9</span>] <span class="string">"ifault"</span></div></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">table(irisCluster$cluster, iris$Species)</div></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"> </div><div class="line">  setosa versicolor virginica</div><div class="line"><span class="number">1</span>     <span class="number">50</span>          <span class="number">0</span>         <span class="number">0</span></div><div class="line"><span class="number">2</span>      <span class="number">0</span>         <span class="number">48</span>         <span class="number">4</span></div><div class="line"><span class="number">3</span>      <span class="number">0</span>          <span class="number">2</span>        <span class="number">46</span></div></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; irisCluster$cluster &lt;- as.factor(irisCluster$cluster)</div><div class="line">&gt; ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point()</div></pre></td></tr></table></figure>
<p><img src="https://farm1.staticflickr.com/353/31565301411_fc7281b525_o.jpg" alt=""></p>
<hr>
<h3 id="学习向量量化聚类（Learning-vector-Quantization）"><a href="# 学习向量量化聚类（Learning-vector-Quantization）" class="headerlink" title="学习向量量化聚类（Learning vector Quantization）"></a>学习向量量化聚类（Learning vector Quantization）</h3><h4 id="算法介绍 -1"><a href="# 算法介绍 -1" class="headerlink" title="算法介绍"></a>算法介绍</h4><p><strong>LVQ 假设数据样本带有类别标记, 学习过程利用样本的这些监督信息来辅助聚类。</strong></p>
<p>给定样本集 <script type="math/tex">D=\lbrace (x_{1}, y_{1}), (x_{2}, y_{2}), \ldots, (x_{m}, y_{m}) \rbrace</script>, 每个样本 <script type="math/tex">x_{j}</script> 是由 $n$ 个属性描述的特征向量 <script type="math/tex">(x_{j1}; x_{j2}; \ldots; x_{jn})</script>,<script type="math/tex">y_{j} \in \mathcal{Y}</script> 是样本 <script type="math/tex">x_{j}</script> 的类别标记. LVQ 的目标是学得一组 $n$ 维原型向量<script type="math/tex">\lbrace p_{1}, p_{2}, \ldots, p_{q} \rbrace</script>, 每个原型向量代表一个聚类簇，簇标记为<script type="math/tex">t_{i}\in \mathcal{Y}</script>.</p>
<p><img src="https://farm5.staticflickr.com/4320/36049858912_c400743b7f_o.png" alt="学习向量量化方法"></p>
<p><strong>算法解释</strong></p>
<ul>
<li>算法第 1 行：对原型向量进行初始化。例如：对第 $i,i=(1,2,\ldots,q)$ 个簇, 可以从类别标记为 $t_{i}$ 的样本中随机选取一个作为原型向量。</li>
<li><p>算法第 2-12 行：对原型向量进行迭代优化。在每一轮迭代中，算法随机选取一个有标记训练样本，找出与其距离最近的原型向量，并根据两者的类别标记是否一致来对原型向量进行相应的更新。</p>
<ul>
<li>算法第 5 行：这是竞争学习的“胜者为王“的策略。SOM 是基于无标记样本的聚类算法，而 LVQ 可以看作是 SOM 基于监督信息的扩展。</li>
<li><p>算法第 6-10 行：如何更新原型向量。对样本 $x_{j}$ ，</p>
<ul>
<li><p>若距离 $x<em>{j}$ 最近的原型向量 $p</em>{i^{<em>}}$ 与 $x<em>{j}$ 的标记相同，则令 $p</em>{i^{</em>}}$ 向 $x_{j}$ 的方向靠拢，此时新的原型向量为 </p>
<script type="math/tex; mode=display">
p' = p_{i^{*}} + \eta \cdot (x_{j}-p_{i^{*}})</script><p>​        $p^{‘}$ 与 $x_{j}$ 之间的距离为 </p>
<script type="math/tex; mode=display">
||p'-x_{j}||_{2}=(1-\eta) \cdot ||p_{i^{*}}-x_{j}||_{2}</script><p>​       原型向量 $p<em>{i^{*}}$ 更新为 $p’$ 之后将更接近 $x</em>{j}$.</p>
</li>
<li>若距离 $x<em>{j}$ 最近的原型向量 $p</em>{i^{<em>}}$ 与 $x<em>{j}$ 的标记不同，则令 $p</em>{i^{</em>}}$ 向 $x_{j}$ 的方向远离，此时新的原型向量为 <script type="math/tex; mode=display">
p'= p_{i^{*}} - \eta \cdot (x_{j}-p_{i^{*}})</script>​       $p^{‘}$ 与 $x_{j}$ 之间的距离为 <script type="math/tex; mode=display">
||p'-x_{j}|_{2}=(1+\eta) \cdot |p_{i^{*}}-x_{j}||_{2}</script>​       原型向量 $p<em>{i^{*}}$ 更新为 $p’$ 之后将更远离 $x</em>{j}$.</li>
</ul>
</li>
</ul>
</li>
<li>算法第 12 行：若算法的停止条件已满足(例如已达到最大迭代轮数，或原型向量更新很小甚至不再更新)，则将当前原型向量作为最终结果返回。</li>
<li>在学得一组原型向量 $ \lbrace p<em>{1}, p</em>{2}, \ldots, p_{q} \rbrace$ 后即可实现对样本空间 $\mathcal{X}$ 的簇划分.<ul>
<li>对任意样本 $x$, 他将被划入与其距离最近的原型向量所代表的簇中，每个原型向量 $p<em>{i}$ 定义了与之相关的一个区域 $R</em>{i}$，该区域中每个样本与 $p<em>{i}$ 的距离不大于他与其他原型向量 $p</em>{i’} (i \neq i’)$，即 <script type="math/tex; mode=display">
R_{i}= \lbrace x \in \mathcal{X} \ |\ ||x-p_{i}||_{2} \leqslant ||x-p_{i'}||_{2}, i \neq i'\rbrace</script>​       由此形成了对样本空间 $\mathcal{X}$ 的簇划分 ${R<em>{1}, R</em>{2}, \ldots, R_{q}}$，该划分通常称为“Voronoi 剖分”(Voronoi tessellation).</li>
</ul>
</li>
</ul>
<h4 id="算法实现"><a href="# 算法实现" class="headerlink" title="算法实现"></a>算法实现 </h4><h4 id="补充"><a href="# 补充" class="headerlink" title="补充"></a> 补充 </h4><h5 id="竞争型学习"><a href="# 竞争型学习" class="headerlink" title="竞争型学习"></a> 竞争型学习 </h5><p> 竞争型学习是神经网络中一种常见的无监督学习策略，在使用该策略时，网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制。这种机制亦称为”胜者通吃”(winner-take-all)原则。</p>
<h5 id="ART- 网络"><a href="#ART- 网络" class="headerlink" title="ART 网络"></a>ART 网络</h5><p>ART(Adaptive Resonance Theory, 自适应谐振理论）网络「Carpenter and Grossberg, 1987」是竞争型学习的重要代表。该网络由比较层、识别层、识别阈值和重置模块构成。其中，比较层负责接收输入样本，并将其传递给识别层神经元。识别层每个神经元对应一个模式类（模式类可以认为是某类别的“子类”），神经元数目可在训练过程中动态增长以增加新的模式类。</p>
<p>在接收到比较层的输入信号后，识别层神经元之间相互竞争以产生获胜神经元。竞争的最简单方式是，计算输入向量与每个识别层神经元所对应的模式类的代表向量之间的距离，距离最小者胜．获胜神经元将向其他识别层神经元发送信号，抑制其激活。若输入向量与获胜神经元所对应的代表向量之间的相似度大于识别阈值，则当前输入样本将被归为该代表向量所属类别，同时，网络 连接权将会更新，使得以后在接收到相似输入样本时该模式类会计算出更大的相似度，从而使该获胜神经元有更大可能获胜；若相似度不大于识别阈值，则重置模块将在识别层增设一个新的神经元，其代表向量就设置为当前输入向量。</p>
<p>显然，识别阈值对 ART 网络的性能有重要影响。当识别阈值较高时，输入样本将会被分成比较多、比较精细的模式类，而如果识别阈值较低，则会产生比较少、比较粗略的模式类。</p>
<p>ART 比较好地缓解了竞争型学习中的“可塑性一稳定性窘境”(stability— plasticity dilemma)，可塑性是指神经网络要有学习新知识的能力，而稳定性则是指神经网络在学习新知识时要保持对旧知识的记忆．这就使得 ART 网络具有一个很重要的优点：可进行 <strong> 增量学习（incremental learning）</strong>或 <strong> 在线学习（online learning)</strong>。</p>
<p>早期的 ART 网络只能处理布尔型输入数据，此后 ART 发展成了一个算法族，包括能处理实值输入的 ART2 网络、结合模糊处理的 FuzzyART 网络，以及可进行监督学习的 ARTMAP 网络等。 </p>
<h5 id="SOM- 网络"><a href="#SOM- 网络" class="headerlink" title="SOM 网络"></a>SOM 网络</h5><p>SOM(Self-Organizing Map，自组织映射）网络「Kohollen, 1982」是一种竞争学习型的无监督神经网络，它能将高维输入数据映射到低维空间（通常为二维），同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元。</p>
<p><img src="https://farm6.staticflickr.com/5569/31429001070_b16d937809_o.png" alt=""></p>
<p>如图所示，SOM 网络中的输出层神经元以矩阵方式排列在二维空间中，每个神经元都拥有一个权向量，网络在接收输入向量后，将会确定输出层获胜神经元，它决定了该输入向量在低维空间中的位置。SOM 的训练目标就是为每个输出层神经元找到合适的权向量，以达到保持拓扑结构的目的。 SOM 的训练过程很简单：在接收到一个训练样本后，每个输出层神经元会计算该样本与自身携带的权向量之间的距离，距离最近的神经元成为竞争获胜者，称为最佳匹配单元（best matching unit)。然后，最佳匹配单元及其邻近神经元的权向量将被调整，以使得这些权向量与当前输入样本的距离缩小。这个过程不断迭代，直至收敛。</p>
<hr>
<h3 id="高斯混合聚类（Mixture-of-Gaussian）"><a href="# 高斯混合聚类（Mixture-of-Gaussian）" class="headerlink" title="高斯混合聚类（Mixture-of-Gaussian）"></a>高斯混合聚类（Mixture-of-Gaussian）</h3><h4 id="算法介绍 -2"><a href="# 算法介绍 -2" class="headerlink" title="算法介绍"></a>算法介绍 </h4><p> 与 $k$ 均值、LVQ 用原型向量来刻画聚类结构不同，<strong>高斯混合聚类 (Mixture-of-Gaussian) 采用概率模型来表达聚类原型</strong>。</p>
<h5 id="多元 - 高斯分布："><a href="# 多元 - 高斯分布：" class="headerlink" title="(多元)高斯分布："></a>(多元)高斯分布：</h5><p>对 $n$ 维样本空间 $\mathcal{X}$ 中的随机向量 $x$，若 $x$ 服从 (多元) 高斯分布，其概率密度函数为：</p>
<script type="math/tex; mode=display">
p(x| \mu, \Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} e^{- \frac{1}{2} (x-\mu)^{T} \Sigma^{-1} (x-\mu)}</script><p>其中：$\mu​$ 是 $n​$ 维均值向量，$\Sigma​$ 是 $n \times n​$ 协方差矩阵。($\Sigma​$：对称正定矩阵；$|\Sigma|​$：$\Sigma​$ 的行列式；$\Sigma^{1}​$：$\Sigma​$ 的逆矩阵   )</p>
<h5 id="多元 - 高斯混合分布："><a href="# 多元 - 高斯混合分布：" class="headerlink" title="(多元)高斯混合分布："></a>(多元)高斯混合分布：</h5><p>对 $n$ 维样本空间 $\mathcal{X}$ 中的随机向量 $x$, 若 $x$ 服从 (多元) 高斯混合分布, 其概率密度函数为：</p>
<script type="math/tex; mode=display">
p_{\mathcal{M}}(x)=\sum_{i=1}^{k}\alpha_{i} \cdot p(x| \mu_{i}, \Sigma_{i})</script><p>该分布由 $k$ 个混合成分组成，每个成分对应一个 (多元) 高斯分布, 其中：$\mu<em>{i}$, $\Sigma</em>{i}$ 是第 $i$ 个高斯混合成分的参数, 而 $\alpha<em>{i}$ 为相应的“混合系数”(mixture coeffcient), $\sum</em>{i=1}^{k}\alpha_{i}=1$.</p>
<h5 id="样本集的生成模型："><a href="# 样本集的生成模型：" class="headerlink" title="样本集的生成模型："></a>样本集的生成模型：</h5><p>假设样本集 $D= \lbrace x<em>{1}, x</em>{2}, \ldots, x_{n} \rbrace $ 的生成过程由高斯混合分布给出：</p>
<ul>
<li><p>首先：根据 $\alpha<em>{1}, \alpha</em>{2}, \ldots,\alpha<em>{k}$ 定义的先验分布选择高斯混合成分，其中 $\alpha</em>{i}$ 为选择第 $i$ 个混合成分的概率;</p>
</li>
<li><p>然后：根据被选择的混合成分的概率密度函数进行采样, 生成相应的样本。</p>
</li>
</ul>
<p>令随机变量 $z<em>{j} \in \lbrace 1, 2, \ldots,k \rbrace$ 表示生成样本 $x</em>{j}$ 的高斯混合成分， 其取值未知。</p>
<p>$z_{j}$ 的先验概率：</p>
<script type="math/tex; mode=display">
P(z{j}=i)=\alpha{i} \quad (i=1, 2, \ldots,k).</script><p>由 Bayesian 定理得 $z_{j}$ 的后验分布为：</p>
<script type="math/tex; mode=display">
\begin{align*}
P_{\mathcal{M}}(z_{j}=i|x_{j}) &=\frac{P(z_{j}=i) \cdot p_{\mathcal{M}}(x_{j}|z_{j}=i)}{p_{\mathcal{M}}(x_{j})} \cr
& =\frac{\alpha_{i}\cdot p(x_{j}|\mu_{i},\Sigma_{i})}{\sum^{k}_{l=1}\alpha_{l}\cdot p(x_{j}|\mu_{l},\Sigma_{l})}
\end{align*}</script><p>$P<em>{\mathcal{M}}(z</em>{j}=i|x<em>{j})$ 给出了样本 $x</em>{j}$ 由第 $i$ 个高斯混合成分生成的后验概率, 记：</p>
<script type="math/tex; mode=display">
\gamma_{ji}=P_{\mathcal{M}}(z_{j}=i|x_{j}) \quad (i=1, 2, \ldots,k)</script><h5 id="高斯混合聚类策略："><a href="# 高斯混合聚类策略：" class="headerlink" title="高斯混合聚类策略："></a>高斯混合聚类策略：</h5><ul>
<li>若 (多元) 高斯混合分布 $p_{\mathcal{M}}(x)$ 已知，高斯混合聚类将把样本集 $D$ 划分为 $k$ 个簇<script type="math/tex; mode=display">
C= \lbrace C_{1}, C_{2}, \ldots,C_{k} \rbrace</script></li>
</ul>
<p>每个样本 $x<em>{j}$ 的簇标记 $\lambda</em>{j}$ 为:</p>
<script type="math/tex; mode=display">
\lambda_{j}=arg \underset{i \in {1, 2, \ldots,k}}{max}\gamma_{ji}</script><ul>
<li>(多元)高斯混合分布 $p<em>{\mathcal{M}}(x)$ 参数 $\lbrace (\alpha</em>{i},\mu<em>{i}, \Sigma</em>{i})|1 \leqslant i \leqslant k \rbrace$ 的求解采用极大似然估计(MLE):</li>
</ul>
<p>给定样本集 $D$, 最大化 (对数) 似然函数：</p>
<script type="math/tex; mode=display">
\begin{align*}
LL(D) & = ln\Big(\prod^{n}_{j=1}p_{\mathcal{M}}(x_{j})\Big) \cr
&=\sum^{n}_{j=1}\Big(\sum^{k}_{i=1}\alpha_{i}\cdot p(x_{j}|\mu_{i}, \Sigma_{i})\Big)
\end{align*}</script><p>MLE 解为：</p>
<script type="math/tex; mode=display">
\begin{align}
\mu_{i} & = \frac{\sum^{n}_{j=1}\gamma_{ji}x_{j}}{\sum^{n}_{j=1}\gamma{ji}} \cr
\Sigma_{i} & = \frac{\sum^{n}_{j=1}\gamma_{ji}(x_{j}-\mu_{i})(x_{j}-\mu_{i})^{T}}{\sum^{n}_{j=1}\gamma_{ji}} \cr
\alpha_{i} &= \frac{1}{n}\sum^{n}_{j=1}\gamma_{ji}
\end{align}</script><h5 id="算法："><a href="# 算法：" class="headerlink" title="算法："></a>算法：</h5><blockquote>
<p><strong>输入:</strong><br>样本集: $D= \lbrace x<em>{1}, x</em>{2}, \ldots, x_{n} \rbrace$;<br>高斯混合成分个数 $k$.<br><strong>过程: </strong></p>
<ol>
<li>初始化高斯混合分布的模型参数 : $\lbrace (\alpha<em>{i},\mu</em>{i}, \Sigma_{i})|1 \leqslant i \leqslant k \rbrace$</li>
<li><strong>repeat</strong></li>
<li>​    <strong>for</strong> $j=1, 2, \ldots, n$ <strong>do</strong></li>
<li>​        根据先验概率计算 $x_{j}$ 由各混合成分生成的后验概率, 即</li>
<li>​         $\gamma<em>{ji}=p</em>{\mathcal{M}}(z<em>{j}=i|x</em>{j})(1 \leqslant i \leqslant k)$</li>
<li>​    <strong>end for</strong></li>
<li>​    <strong>for</strong> $i=1, 2, \ldots, k$ <strong>do</strong></li>
<li>​        计算新均值向量: $\mu<em>{i}’=\frac{\sum^{n}</em>{j=1}\gamma<em>{ji}x</em>{j}}{\sum^{n}<em>{j=1}\gamma</em>{ji}}$;</li>
<li>​        计算新协方差矩阵: $\Sigma<em>{i}’=\frac{\sum^{n}</em>{j=1}\gamma<em>{ji}(x</em>{j}-\mu<em>{i}’)(x</em>{j}-\mu<em>{i}’)^{T}}{\sum^{n}</em>{j=1}\gamma_{ji}}$;</li>
<li>​        计算新混合系数: $\alpha<em>{i}’=\frac{\sum^{n}</em>{j=1}\gamma_{ji}}{n}$;</li>
<li>​    <strong>end for</strong></li>
<li>​        将模型参数 $\lbrace (\alpha<em>{i},\mu</em>{i}, \Sigma<em>{i})|1 \leqslant i \leqslant k \rbrace$ 更新为 $\lbrace (\alpha</em>{i}’,\mu<em>{i}’, \Sigma</em>{i}’)|1 \leqslant i \leqslant k \rbrace$</li>
<li>​    <strong>until</strong> 满足停止条</li>
<li>​    $C_{i}=\emptyset (1 \leqslant i \leqslant k)$</li>
<li>​    <strong>for</strong> $j=1, 2, \ldots, n$ <strong>do</strong></li>
<li>​        确定 $x<em>{j}$ 的簇标记 $\lambda</em>{j}$;</li>
<li>​        将 $x<em>{j}$ 划入相应的簇: $C</em>{\lambda<em>{j}}=C</em>{\lambda<em>{j}} \cup \lbrace x</em>{j} \rbrace$</li>
<li><strong>end for</strong><br><strong>输出:</strong> 簇划分 $C={C<em>{1}, C</em>{2}, \ldots, C_{k}}$</li>
</ol>
</blockquote>
<h4 id="算法实现 -1"><a href="# 算法实现 -1" class="headerlink" title="算法实现"></a>算法实现</h4><hr>
<h1 id="各聚类算法过程"><a href="# 各聚类算法过程" class="headerlink" title="各聚类算法过程"></a><strong>各聚类算法过程 </strong></h1><p> 使用西瓜数据集合：</p>
<script type="math/tex; mode=display">
\begin{array}{ccc|ccc|ccc}
\hline
编号 & 密度 & 含糖率 & 编号 & 密度 & 含糖率 & 编号 & 密度 & 含糖率 \cr
\hline
1 & 0.697 & 0.460 & 11 & 0.245 & 0.057 & 21 & 0.748 & 0.232 \cr
2 & 0.774 & 0.376 & 12 & 0.343 & 0.099 & 22 & 0.714 & 0.346 \cr
3 & 0.634 & 0.264 & 13 & 0.639 & 0.161 & 23 & 0.483 & 0.312 \cr
4 & 0.608 & 0.318 & 14 & 0.657 & 0.198 & 24 & 0.478 & 0.437 \cr
5 & 0.556 & 0.215 & 15 & 0.360 & 0.370 & 25 & 0.525 & 0.369 \cr
6 & 0.403 & 0.237 & 16 & 0.593 & 0.042 & 26 & 0.751 & 0.489 \cr
7 & 0.481 & 0.149 & 17 & 0.719 & 0.103 & 27 & 0.532 & 0.472 \cr
8 & 0.437 & 0.211 & 18 & 0.359 & 0.188 & 28 & 0.473 & 0.376 \cr
9 & 0.666 & 0.091 & 19 & 0.339 & 0.241 & 29 & 0.725 & 0.445 \cr
10 & 0.243 & 0.267 & 20 & 0282 & 0.257 & 30 & 0.446 & 0.459 \cr
\end{array}</script><p>（其中编号为 9~21 的类别是”坏瓜”，其他样本的类别是”好瓜”） </p>
<h2 id="使用 -k- 均值算法"><a href="# 使用 -k- 均值算法" class="headerlink" title="使用 $k$ 均值算法"></a>使用 $k$ 均值算法 </h2><p> 假定聚类簇数 $k=3$，算法开始时随机选取的三个样本 $x<em>{6},x</em>{12},x_{27}$ 作为初始均值向量，即</p>
<script type="math/tex; mode=display">
\mu_{1} = (0.403;0.237), \mu_{2} = (0.343;0.099),\mu_{3} = (0.532;0.472).</script><p>考察样本 $x<em>{1}=(0.697;0.460)$，它与当前均值向量 $\mu</em>{1},\mu<em>{2},\mu</em>{3}$ 的距离分别是 0.369，0.506，0.166，因此 $x<em>{1}$ 将被划入簇 $C</em>{3}$ 中。类似的，对数据集中的所有样本考察一遍后，可得当前簇划分为：</p>
<script type="math/tex; mode=display">
\begin{align}
C_{1} & = \lbrace x_{5},x_{6},x_{7},x_{8},x_{9},x_{10},x_{13},x_{14},x_{15},x_{17},x_{18},x_{19},x_{20},x_{23} \rbrace; \cr
C_{2} & = \lbrace x_{11},x_{12},x_{16} \rbrace; \cr
C_{3} & = \lbrace x_{1},x_{2},x_{3},x_{4},x_{21},x_{22},x_{24},x_{25},x_{26},x_{27},x_{28},x_{29},x_{30} \rbrace
\end{align}</script><p>于是，可以从 $C<em>{1},C</em>{2},C_{3}$ 分别求出新的均值向量</p>
<script type="math/tex; mode=display">
\mu_{1}^{'} = (0.473;0.214), \mu_{2}^{'} = (0.394;0.066),\mu_{3}^{'} = (0.623;0.388).</script><p>更新当前均值向量后，不断重复上述过程，如图所示，第五轮迭代产生的结果与第四轮迭代结果相同，于是算法停止，得到最终的簇划分，其中样本点与均值向量分别用”●”与”+”表示，红色虚线显示出簇划分。</p>
<p><img src="https://farm1.staticflickr.com/403/30990399723_0db34a3cde_o.png" alt=""></p>
<h2 id="使用学习向量量化"><a href="# 使用学习向量量化" class="headerlink" title="使用学习向量量化"></a>使用学习向量量化 </h2><p> 令数据集中编号为 9-21 的样本的类别标记为 $c<em>{2}$ ，其他样本的类别标记为 $c</em>{1}$ 。假定 $q=5$，即学习目标是找到 5 个原型向量 $p<em>{1},p</em>{2},p<em>{3},p</em>{4},p<em>{5}$ ，并假定其对应的类别标记分别为 $c</em>{1},c<em>{2},c</em>{2},c<em>{1},c</em>{1}$ 。</p>
<p>算法开始时，根据样本的类别标记和簇的预设类别标记，对原型向量进行随机初始化，假定初始化为样本 $x<em>{5},x</em>{12},x<em>{18},x</em>{23},x<em>{29}$ 。在第一轮迭代中，假定随机选取的样本为 $x</em>{1}$ ，该样本与当前原型向量 $p<em>{1},p</em>{2},p<em>{3},p</em>{4},p<em>{5}$ 的距离分别为 $0.283,0.506,0.434,0.260,0.032$ 。由于 $p</em>{5}$ 与 $x<em>{1}$ 距离最近且两者具有相同的类别标记 $c</em>{2}$ ，假定学习率 $\eta = 0.1$ ，则 LVQ 更新 $p_{5}$ 得到新原型向量：</p>
<script type="math/tex; mode=display">
\begin{align}
p^{'} & = p_{5} + \eta \cdot (x_{1} - p_{5}) \cr
      & = (0.725;0.445) + 0.1 \cdot ((0.697;0.460) - (0.725;0.445)) \cr
      & = (0.722;0.442).
\end{align}</script><p>将 $p<em>{5}$ 更新为 $p^{‘}$ 后，不断重复上述过程，不同轮数之后的聚类结果如下图所示，其中 $c</em>{1}$ ， $c_{2}$ 类别样本点与原型向量分别用”●”,”○”与”+”表示，红色虚线显示出聚类形成的 Voronoi 剖分。</p>
<p><img src="https://farm1.staticflickr.com/715/31427609470_fe9357f0cc_o.png" alt=""></p>
<h2 id="使用高斯混合聚类"><a href="# 使用高斯混合聚类" class="headerlink" title="使用高斯混合聚类"></a>使用高斯混合聚类 </h2><p> 令高斯混合成分的个数 $k=3$。算法开始时，假定将高斯混合分布的模型参数初始化为：$\alpha<em>{1} = \alpha</em>{2} = \alpha<em>{3} = \frac{1}{3}$；$\mu</em>{1} = x<em>{6}$，$\mu</em>{2}=x<em>{22}$，$\mu</em>{3} = x<em>{27}$；$\Sigma</em>{1} = \Sigma<em>{2} = \Sigma</em>{3} = \binom{0.1 \ 0.0}{0.0 \ 0.1}$。</p>
<p>在第一轮迭代中，先计算样本由各混合成分生成的后验概率。</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Python/" rel="tag"># Python</a>
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/12/30/<Machine Learning> Chapter 2/" rel="next" title="♞「Machine Learning」 Chapter 2">
                <i class="fa fa-chevron-left"></i> ♞「Machine Learning」 Chapter 2
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/03/13/<Tensorflow> Use WeChat to Monitor Your Network/" rel="prev" title="「TensorFlow」 Use WeChat to Monitor Your Network">
                「TensorFlow」 Use WeChat to Monitor Your Network <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="Randolph" />
          <p class="site-author-name" itemprop="name">Randolph</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">13</span>
                <span class="site-state-item-name">Posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">Categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">Tags</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/RandolphVI" target="_blank" title="Github">
                  
                    <i class="fa fa-fw fa-github-alt"></i>
                  
                  Github
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/威-黄-88060b74/" target="_blank" title="LinkedIn">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  LinkedIn
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://music.163.com/#/user/home?id=57901575" target="_blank" title="Music">
                  
                    <i class="fa fa-fw fa-music"></i>
                  
                  Music
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://steamcommunity.com/id/Chinawolfman/" target="_blank" title="Steam">
                  
                    <i class="fa fa-fw fa-steam"></i>
                  
                  Steam
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="https://www.tensorflow.org" title="TensorFlow" target="_blank">TensorFlow</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://www.kaggle.com" title="Kaggle" target="_blank">Kaggle</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://leetcode.com" title="LeetCode" target="_blank">LeetCode</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://weekly.codetengu.com" title="CodeTengu" target="_blank">CodeTengu</a>
                </li>
              
            </ul>
          </div>
        
        
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=320 height=150 src="//music.163.com/outchain/player?type=0&id=752986314&auto=1&height=90"></iframe>

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#聚类介绍"><span class="nav-number">1.</span> <span class="nav-text">聚类介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#聚类算法"><span class="nav-number">1.1.</span> <span class="nav-text">聚类算法 </span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#聚类数据设置"><span class="nav-number">1.2.</span> <span class="nav-text">聚类数据设置 </span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#聚类性能度量"><span class="nav-number">1.3.</span> <span class="nav-text">聚类性能度量 </span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#外部指标"><span class="nav-number">1.3.1.</span> <span class="nav-text">外部指标 </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#JC-Jaccard-Coefficient"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">JC(Jaccard Coefficient)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#FMI-Fowlkes-and-Mallows-Index"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">FMI(Fowlkes and Mallows Index)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RI-Rand-Index"><span class="nav-number">1.3.1.3.</span> <span class="nav-text">RI(Rand Index)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ARI-Adjusted-Rand-Index"><span class="nav-number">1.3.1.4.</span> <span class="nav-text">ARI(Adjusted Rand Index)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#举个具体例子计算 -ARI"><span class="nav-number">1.3.1.5.</span> <span class="nav-text">举个具体例子计算 ARI</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#内部指标"><span class="nav-number">1.3.2.</span> <span class="nav-text">内部指标 </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CP-Compactness"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">CP(Compactness)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SP-Separation"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">SP(Separation)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DBI-Davies-Bouldin-Index"><span class="nav-number">1.3.2.3.</span> <span class="nav-text">DBI(Davies-Bouldin Index)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DI-Dunn-Index"><span class="nav-number">1.3.2.4.</span> <span class="nav-text">DI(Dunn Index)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#聚类距离计算"><span class="nav-number">1.4.</span> <span class="nav-text">聚类距离计算 </span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#闵可夫斯基距离（Minkowski-distance）"><span class="nav-number">1.4.1.</span> <span class="nav-text">闵可夫斯基距离（Minkowski distance）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VDM-Value-Difference-Metric"><span class="nav-number">1.4.2.</span> <span class="nav-text">VDM(Value Difference Metric)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#闵可夫斯基距离与 VDM 混合距离"><span class="nav-number">1.4.3.</span> <span class="nav-text">闵可夫斯基距离与 VDM 混合距离 </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#加权闵可夫斯基距离"><span class="nav-number">1.4.4.</span> <span class="nav-text">加权闵可夫斯基距离 </span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#聚类算法介绍及实现"><span class="nav-number">2.</span> <span class="nav-text">聚类算法介绍及实现 </span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基于原型的聚类"><span class="nav-number">2.1.</span> <span class="nav-text">基于原型的聚类 </span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#K 均值聚类（K-means）"><span class="nav-number">2.1.1.</span> <span class="nav-text">K 均值聚类（K-means）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#算法介绍"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">算法介绍 </span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#算法实现 -（R 语言）"><span class="nav-number">2.1.1.2.</span> <span class="nav-text">算法实现 （R 语言）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习向量量化聚类（Learning-vector-Quantization）"><span class="nav-number">2.1.2.</span> <span class="nav-text">学习向量量化聚类（Learning vector Quantization）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#算法介绍 -1"><span class="nav-number">2.1.2.1.</span> <span class="nav-text">算法介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#算法实现"><span class="nav-number">2.1.2.2.</span> <span class="nav-text">算法实现 </span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#补充"><span class="nav-number">2.1.2.3.</span> <span class="nav-text"> 补充 </span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#竞争型学习"><span class="nav-number">2.1.2.3.1.</span> <span class="nav-text"> 竞争型学习 </span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ART- 网络"><span class="nav-number">2.1.2.3.2.</span> <span class="nav-text">ART 网络</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SOM- 网络"><span class="nav-number">2.1.2.3.3.</span> <span class="nav-text">SOM 网络</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#高斯混合聚类（Mixture-of-Gaussian）"><span class="nav-number">2.1.3.</span> <span class="nav-text">高斯混合聚类（Mixture-of-Gaussian）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#算法介绍 -2"><span class="nav-number">2.1.3.1.</span> <span class="nav-text">算法介绍 </span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#多元 - 高斯分布："><span class="nav-number">2.1.3.1.1.</span> <span class="nav-text">(多元)高斯分布：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#多元 - 高斯混合分布："><span class="nav-number">2.1.3.1.2.</span> <span class="nav-text">(多元)高斯混合分布：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#样本集的生成模型："><span class="nav-number">2.1.3.1.3.</span> <span class="nav-text">样本集的生成模型：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#高斯混合聚类策略："><span class="nav-number">2.1.3.1.4.</span> <span class="nav-text">高斯混合聚类策略：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#算法："><span class="nav-number">2.1.3.1.5.</span> <span class="nav-text">算法：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#算法实现 -1"><span class="nav-number">2.1.3.2.</span> <span class="nav-text">算法实现</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#各聚类算法过程"><span class="nav-number">3.</span> <span class="nav-text">各聚类算法过程 </span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#使用 -k- 均值算法"><span class="nav-number">3.1.</span> <span class="nav-text">使用 $k$ 均值算法 </span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用学习向量量化"><span class="nav-number">3.2.</span> <span class="nav-text">使用学习向量量化 </span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用高斯混合聚类"><span class="nav-number">3.3.</span> <span class="nav-text">使用高斯混合聚类 </span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="motto" >
  <span class="motto">「莫怕真理无穷 进一寸便有进一寸的欢喜」</span>
</div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  
  <span class="author" itemprop="copyrightHolder">Randolph</span>
  
</div>

<!--  -->
<!-- <div class="powered-by"> -->
<!--   Powered by <a class="theme-link" href="https://hexo.io">Hexo</a> -->
<!-- </div> -->
<!--  -->
<!-- <div class="theme-info"> -->
<!--   Theme - -->
<!--   <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next"> -->
<!--     NexT.Mist -->
<!--   </a> -->
<!-- </div> -->


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 鸿儒之顾
      <span class="stop">:</span>
      <span class="busuanzi-value" id="busuanzi_value_site_uv" style="display: inline;"></span>
      
    </span>
    
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 管中窥豹
      <span class="stop">:</span>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  

    
      <script id="dsq-count-scr" src="https://randolphvi.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://randolph.pro/2017/01/03/<Machine Learning> Clustering/';
          this.page.identifier = '2017/01/03/<Machine Learning> Clustering/';
          this.page.title = '♞「Machine Learning」 Clustering';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://randolphvi.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  








  




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.1"></script>



  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("rlX1ugcCzQRlB8ljks0eiIKp-gzGzoHsz", "kpFrsFctrAimlBvhHvlgWnhV");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  

  

  
  


  

  

</body>
</html>
