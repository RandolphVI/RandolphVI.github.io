<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[「TensorFlow」 Use WeChat to Monitor Your Network]]></title>
    <url>%2F2017%2F03%2F13%2F%3CTensorflow%3E%20Use%20WeChat%20to%20Monitor%20Your%20Network%2F</url>
    <content type="text"><![CDATA[Use WeChat to Monitor Your Network平时，大家自己的机器模型在训练期间（特别是深度网络），训练时间通常几小时到十几小时不等，甚至可能会花上好几天，那么在这段时间，你们又会干些什么事情呢？ 作为程序员，这里提供一个「有趣的」方式，用你的微信来监控你的模型在训练期间的一举一动。 大概的效果是： 程序用到的主角是 Python 中的微信个人号接口 itchat。What’s itchat? （itchat 的介绍及安装过程） 这次，我们要监控的模型是先前提到过的 基于 MNIST 手写体数据集的「CNN」模型。 注意： 文章要求读者事先下载安装好 itchat。 文章不会详细介绍 TensorFlow 以及 Tensorboard 的知识。 EnvironmentOS: macOS Sierra 10.12.x Python Version: 3.4.x TensorFlow: 1.0 itchat: 1.2.3 Code Use WeChat to Monitor Your Network（tensorboard 绘图） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259# 基于 MNIST 数据集 的 「CNN」（tensorboard 绘图）from tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tfimport numpy as npimport scipy# Import itchat &amp; threadingimport itchatimport threading# Create a running status flaglock = threading.Lock()running = False# Parameterslearning_rate = 0.001training_iters = 200000batch_size = 128display_step = 10def weight_variable(shape): initial = tf.truncated_normal(shape, stddev = 0.1) return tf.Variable(initial) def bias_variable(shape): initial = tf.constant(0.1, shape = shape) return tf.Variable(initial)def conv2d(x, W, strides=1): return tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')def max_pool_2x2(x, k=2): return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME') def variable_summaries(var): """Attach a lot of summaries to a Tensor (for TensorBoard visualization).""" with tf.name_scope('summaries'): mean = tf.reduce_mean(var) tf.summary.scalar('mean', mean) with tf.name_scope('stddev'): stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean))) tf.summary.scalar('stddev', stddev) tf.summary.scalar('max', tf.reduce_max(var)) tf.summary.scalar('min', tf.reduce_min(var)) tf.summary.histogram('histogram', var)def add_layer(input_tensor, weights_shape, biases_shape, layer_name, act = tf.nn.relu, flag = 1): """Reusable code for making a simple neural net layer. It does a matrix multiply, bias add, and then uses relu to nonlinearize. It also sets up name scoping so that the resultant graph is easy to read, and adds a number of summary ops. """ with tf.name_scope(layer_name): with tf.name_scope('weights'): weights = weight_variable(weights_shape) variable_summaries(weights) with tf.name_scope('biases'): biases = bias_variable(biases_shape) variable_summaries(biases) with tf.name_scope('Wx_plus_b'): if flag == 1: preactivate = tf.add(conv2d(input_tensor, weights), biases) else: preactivate = tf.add(tf.matmul(input_tensor, weights), biases) tf.summary.histogram('pre_activations', preactivate) if act == None: outputs = preactivate else: outputs = act(preactivate, name = 'activation') tf.summary.histogram('activation', outputs) return outputsdef nn_train(wechat_name, param): global lock, running # Lock with lock: running = True # 参数 learning_rate, training_iters, batch_size, display_step = param # Import data mnist_data_path = 'MNIST_data/' mnist = input_data.read_data_sets(mnist_data_path, one_hot = True) # Network Parameters n_input = 28*28 # MNIST data input (img shape: 28*28) n_classes = 10 # MNIST total classes (0-9 digits) dropout = 0.75 # Dropout, probability to keep units with tf.name_scope('Input'): x = tf.placeholder(tf.float32, [None, n_input], name = 'input_x') y_ = tf.placeholder(tf.float32, [None, n_classes], name = 'target_y') keep_prob = tf.placeholder(tf.float32, name = 'keep_prob') #dropout (keep probability) def cnn_net(x, weights, biases, dropout): # Reshape input picture x_image = tf.reshape(x, [-1, 28, 28 ,1]) # First Convolutional Layer conv_1 = add_layer(x_image, weights['conv1_w'], biases['conv1_b'], 'First_Convolutional_Layer', flag = 1) # First Pooling Layer pool_1 = max_pool_2x2(conv_1) # Second Convolutional Layer conv_2 = add_layer(pool_1, weights['conv2_w'], biases['conv2_b'], 'Second_Convolutional_Layer', flag = 1) # Second Pooling Layer pool_2 = max_pool_2x2(conv_2) # Densely Connected Layer pool_2_flat = tf.reshape(pool_2, [-1, weight_variable(weights['dc1_w']).get_shape().as_list()[0]]) dc_1 = add_layer(pool_2_flat, weights['dc1_w'], biases['dc1_b'], 'Densely_Connected_Layer', flag = 0) # Dropout dc_1_drop = tf.nn.dropout(dc_1, keep_prob) # Readout Layer y = add_layer(dc_1_drop, weights['out_w'], biases['out_b'], 'Readout_Layer', flag = 0) return y # Store layers weight &amp; bias weights = &#123; # 5x5 conv, 1 input, 32 outputs 'conv1_w': [5, 5, 1, 32], # 5x5 conv, 32 inputs, 64 outputs 'conv2_w': [5, 5, 32, 64], # fully connected, 7*7*64 inputs, 1024 outputs 'dc1_w': [7*7*64, 1024], # 1024 inputs, 10 outputs (class prediction) 'out_w': [1024, n_classes] &#125; biases = &#123; 'conv1_b': [32], 'conv2_b': [64], 'dc1_b': [1024], 'out_b': [n_classes] &#125; y = cnn_net(x, weights, biases, dropout) # Optimizer with tf.name_scope('cost'): cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = y)) tf.summary.scalar('cost', cost) tf.summary.histogram('cost', cost) # Train with tf.name_scope('train'): optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Test with tf.name_scope('accuracy'): with tf.name_scope('correct_prediction'): correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) with tf.name_scope('accuracy'): accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) tf.summary.scalar('accuracy', accuracy) sess = tf.InteractiveSession() merged = tf.summary.merge_all() train_writer = tf.summary.FileWriter('train/', sess.graph) test_writer = tf.summary.FileWriter('test/') tf.global_variables_initializer().run() # Train the model, and also write summaries. # Every 10th step, measure test-set accuracy, and write test summaries # All other steps, run train_step on training data, &amp; add training summaries # Keep training until reach max iterations print('Wait for lock') with lock: run_state = running print('Start') step = 1 while step * batch_size &lt; training_iters and run_state: batch_x, batch_y = mnist.train.next_batch(batch_size) # Run optimization op (backprop) sess.run(optimizer, feed_dict = &#123;x: batch_x, y_: batch_y, keep_prob: dropout&#125;) if step % display_step == 0: # Record execution stats run_options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE) run_metadata = tf.RunMetadata() summary, _ = sess.run([merged, optimizer], feed_dict = &#123;x: batch_x, y_: batch_y, keep_prob: 1.&#125;, options = run_options, run_metadata = run_metadata) train_writer.add_run_metadata(run_metadata, 'step %d ' % step) train_writer.add_summary(summary, step) print('Adding run metadata for', step) summary, loss, acc = sess.run([merged, cost, accuracy], feed_dict = &#123;x: batch_x, y_: batch_y, keep_prob: 1.&#125;) print("Iter " + str(step*batch_size) + ", Minibatch Loss= " + \ "&#123;:.6f&#125;".format(loss) + ", Training Accuracy= " + \ "&#123;:.5f&#125;".format(acc)) itchat.send("Iter " + str(step*batch_size) + ", Minibatch Loss= " + \ "&#123;:.6f&#125;".format(loss) + ", Training Accuracy= " + \ "&#123;:.5f&#125;".format(acc), 'filehelper') else: summary, _ = sess.run([merged, optimizer], feed_dict = &#123;x: batch_x, y_: batch_y, keep_prob: 1.&#125;) train_writer.add_summary(summary, step) step += 1 with lock: run_state = running print("Optimization Finished!") itchat.send("Optimization Finished!", 'filehelper') # Calculate accuracy for 256 mnist test images summary, acc = sess.run([merged, accuracy], feed_dict = &#123;x: mnist.test.images[:256], y_: mnist.test.labels[:256], keep_prob: 1.&#125; ) text_writer.add_summary(summary) print("Testing Accuracy:", acc) itchat.send("Testing Accuracy: %s" % acc, wechat_name) @itchat.msg_register([itchat.content.TEXT])def chat_trigger(msg): global lock, running, learning_rate, training_iters, batch_size, display_step if msg['Text'] == u'开始': print('Starting') with lock: run_state = running if not run_state: try: threading.Thread(target=nn_train, args=(msg['FromUserName'], (learning_rate, training_iters, batch_size, display_step))).start() except: msg.reply('Running') elif msg['Text'] == u'停止': print('Stopping') with lock: running = False elif msg['Text'] == u'参数': itchat.send('lr=%f, ti=%d, bs=%d, ds=%d'%(learning_rate, training_iters, batch_size, display_step),msg['FromUserName']) else: try: param = msg['Text'].split() key, value = param print(key, value) if key == 'lr': learning_rate = float(value) elif key == 'ti': training_iters = int(value) elif key == 'bs': batch_size = int(value) elif key == 'ds': display_step = int(value) except: passif __name__ == '__main__': itchat.auto_login(hotReload=True) itchat.run() 大家可以看到，我对先前的代码进行了一些修改。 下面我会对代码中用到 itchat 的部分进行一些简短的说明。 代码部分截图： 说明： 首先我导入了 itchat 和 threading。 在原先所有 print 消息的地方，都添加了 itchat.send() 来输出我们的模型训练日志。 加了一个带锁的状态量 running 用来做为发送微信消息的运行开关。 写了一个 itchat 的 handler（就是上图）。其作用就是当程序运行，我们需要在微信中，对自己的微信号发送「开始」，模型才会开始训练，为了防止信息阻塞，所以要用到 threading 将其放在另一个线程当中。在训练的过程中，如果我们觉得结果已到达我们自己的预期，可以微信发送「停止」来停止模型的训练过程。 另外，脚本刚开始运行时，程序会弹出一个包含二维码的图片，我们需要通过微信来扫描该二维码，来登陆微信并启动 itchat 的服务。 程序是包含了 Tensorboard 绘图的，所以等模型训练好，我们依然是可以通过 Tensorboard 来更加详细地查看我们模型的训练过程。 至此，我们就可以一边通过微信来监控我们的模型训练过程，一边与身边的朋友们谈笑风生了。 如果看过 itchat 那个连接的读者，可以了解到 itchat 同样是可以发送图片信息的，所以我们可以写额外的脚本在训练的过程中每隔 100 次迭代， plot 到目前为止 loss，acc 等指标的趋势图。在此，我就不再进行拓展了。 关于各个模块的作用，以及各个变量的意义，我在此就不再赘述了。 如果有读者对于 CNN 卷积神经网络有些陌生或者是遗忘，可以参考我的另外一篇文章 CNN on TensorFlow。 如果读者对 Tensorboard 有所遗忘，可以参考我的另一篇文章 「TensorFlow 1.0」 Tensorboard。]]></content>
      <categories>
        <category>TensorFlow</category>
        <category>WeChat</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>WeChat</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F03%2F13%2F%3CTensorFlow%3E%20CNN%20Introduction%2F</url>
    <content type="text"><![CDATA[TensorFlow CNN Introduction[TOC] Build a Multilayer Convolutional Network在 CNN 卷积神经网络中，主要有四个操作： 卷积 非线性处理（ReLU） 池化或者亚采样 分类（全连接层） 这些操作对于各个卷积神经网络来说都是基本组件，因此理解它们的工作原理有助于充分了解卷积神经网络。下面我们将会尝试理解各步操作背后的原理。 What’s CNNConvolution卷积的主要目的是为了从输入图像中提取特征。卷积可以通过从输入的一小块数据中学到图像的特征，并可以保留像素间的空间关系。让我们举个例子来尝试理解一下卷积是如何处理图像的： 正如我们上面所说，每张图像都可以看作是像素值的矩阵。考虑一下一个 5 x 5 的图像，它的像素值仅为 0 或者 1（注意对于灰度图像而言，像素值的范围是 0 到 255，下面像素值为 0 和 1 的绿色矩阵仅为特例）： 同时，考虑下另一个 3 x 3 的矩阵，如下所示： 接下来，5 x 5 的图像和 3 x 3 的矩阵的卷积可以按下图所示的动画一样计算： 现在停下来好好理解下上面的计算是怎么完成的。我们用橙色的矩阵在原始图像（绿色）上滑动，每次滑动一个像素（也叫做「步长」），在每个位置上，我们计算对应元素的乘积（两个矩阵间），并把乘积的和作为最后的结果，得到输出矩阵（粉色）中的每一个元素的值。注意，3 x 3 的矩阵每次步长中仅可以看到输入图像的一部分。 在 CNN 的术语中，3x3 的矩阵叫做「滤波器」(filter) 或「核」(kernel) 或者 「特征检测器」(feature detector)，通过在图像上滑动滤波器并计算点乘得到矩阵叫做「卷积特征」(Convolved Feature) 或者 「激活图」(Activation Map) 或者 「特征图」(Feature Map)。记住，滤波器在原始输入图像上的作用是特征检测器。 从上面图中的动画可以看出，对于同样的输入图像，不同值的滤波器将会生成不同的特征图。比如，对于下面这张输入图像： 在下表中，我们可以看到不同滤波器对上图卷积的效果。正如表中所示，通过在卷积操作前修改滤波矩阵的数值，我们可以进行诸如边缘检测、锐化和模糊等操作 —— 这表明不同的滤波器可以从图中检测到不同的特征，比如边缘、曲线等。 另一个直观理解卷积操作的好方法是看下面这张图的动画： 滤波器（红色框）在输入图像滑过（卷积操作），生成一个特征图。另一个滤波器（绿色框）在同一张图像上卷积可以得到一个不同的特征图。注意卷积操作可以从原图上获取局部依赖信息。同时注意这两个不同的滤波器是如何从同一张图像上生成不同的特征图。记住上面的图像和两个滤波器仅仅是我们上面讨论的数值矩阵。 在实践中，CNN 会在训练过程中学习到这些滤波器的值（尽管我们依然需要在训练前指定诸如滤波器的个数、滤波器的大小、网络架构等参数）。我们使用的滤波器越多，提取到的图像特征就越多，网络所能在未知图像上识别的模式也就越好。 特征图的大小（卷积特征）由下面三个参数控制，我们需要在卷积前确定它们： 深度（Depth）：深度对应的是卷积操作所需的滤波器个数。在下图的网络中，我们使用三个不同的滤波器对原始图像进行卷积操作，这样就可以生成三个不同的特征图。你可以把这三个特征图看作是堆叠的 2d 矩阵，那么，特征图的「深度」就是 3。 步长（Stride）：步长是我们在输入矩阵上滑动滤波矩阵的像素数。当步长为 1 时，我们每次移动滤波器一个像素的位置。当步长为 2 时，我们每次移动滤波器会跳过 2 个像素。步长越大，将会得到更小的特征图。 零填充（Zero-padding）：有时，在输入矩阵的边缘使用零值进行填充，这样我们就可以对输入图像矩阵的边缘进行滤波。零填充的一大好处是可以让我们控制特征图的大小。使用零填充的也叫做泛卷积，不适用零填充的叫做严格卷积。 ReLUReLU表示修正线性单元（Rectified Linear Unit），是一个非线性操作。 $$Output = Max(zero, Input)$$ 为什么要引入非线性激励函数？ 如果不用激励函数（其实相当于激励函数是 $f(x) = x$ ），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐层效果相当，这种情况就是最原始的感知机（Perceptron）了。 正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。最早的想法是 sigmoid 函数或者 tanh 函数，输出有界，很容易充当下一层输入（以及一些人的生物解释balabala）。 为什么要引入 ReLU 而不是其他的非线性函数（例如 Sigmoid 函数）？ 采用 sigmoid 等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。 对于深层网络，sigmoid 函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），从而无法完成深层网络的训练。 Relu 会使一部分神经元的输出为 0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生（以及一些人的生物解释balabala）。 当然现在也有一些对 relu 的改进，比如 prelu，random relu等，在不同的数据集上会有一些训练速度上或者准确率上的改进，具体的可以找相关的paper看。 （多加一句，现在主流的做法，会多做一步 batch normalization，尽可能保证每一层网络的输入具有相同的分布。而最新的 paper，他们在加入bypass connection 之后，发现改变 batch normalization 的位置会有更好的效果。） ReLU 的优点与缺点？ 优点： 解决了gradient vanishing问题 (在正区间) 计算速度非常快，只需要判断输入是否大于0 收敛速度远快于sigmoid和tanh 缺点： ReLU 的输出不是 zero-centered Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate 太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用 Xavier 初始化方法，以及避免将 learning rate 设置太大或使用 adagrad 等自动调节 learning rate 的算法。 几十年的机器学习发展中，我们形成了这样一个概念：非线性激活函数要比线性激活函数更加先进。 尤其是在布满 Sigmoid 函数的 BP 神经网络，布满径向基函数的 SVM 神经网络中，往往有这样的幻觉，非线性函数对非线性网络贡献巨大。 该幻觉在 SVM 中更加严重。核函数的形式并非完全是 SVM 能够处理非线性数据的主力功臣（支持向量充当着隐层角色）。 那么在深度网络中，对非线性的依赖程度就可以缩一缩。另外，在上一部分提到，稀疏特征并不需要网络具有很强的处理线性不可分机制。 综合以上两点，在深度学习模型中，使用简单、速度快的线性激活函数可能更为合适。 ReLU 操作可以从下面的图中理解。这里的输出特征图也可以看作是“修正”过的特征图。 所谓麻雀虽小，五脏俱全，ReLU虽小，但也是可以改进的。 ReLU的种类ReLU的区分主要在负数端，根据负数端斜率的不同来进行区分，大致如下图所示。 普通的ReLU负数端斜率是0，Leaky ReLU则是负数端有一个比较小的斜率，而PReLU则是在后向传播中学习到斜率。而Randomized Leaky ReLU则是使用一个均匀分布在训练的时候随机生成斜率，在测试的时候使用均值斜率来计算。 效果其中，NDSB 数据集是 Kaggle 的比赛，而 RReLU 正是在这次比赛中崭露头角的。 通过上述结果，可以看到四点： 对于 Leaky ReLU 来说，如果斜率很小，那么与 ReLU 并没有大的不同，当斜率大一些时，效果就好很多。 在训练集上，PReLU 往往能达到最小的错误率，说明 PReLU 容易过拟合。 在 NSDB 数据集上 RReLU 的提升比 cifar10 和 cifar100 上的提升更加明显，而 NSDB 数据集比较小，从而可以说明，RReLU在与过拟合的对抗中更加有效。 对于 RReLU 来说，还需要研究一下随机化得斜率是怎样影响训练和测试过程的。 参考文献[1]. Xu B, Wang N, Chen T, et al. Empirical evaluation of rectified activations in convolutional network[J]. arXiv preprint arXiv:1505.00853, 2015. Pooling空间池化（Spatial Pooling）（也叫做亚采用或者下采样）降低了各个特征图的维度，但可以保持大部分重要的信息。空间池化有下面几种方式：最大化、平均化、加和等等。 对于最大池化（Max Pooling），我们定义一个空间邻域（比如，2x2 的窗口），并从窗口内的修正特征图中取出最大的元素。除了取最大元素，我们也可以取平均（Average Pooling）或者对窗口内的元素求和。在实际中，最大池化被证明效果更好一些。 下面的图展示了使用 2x2 窗口在修正特征图（在卷积 + ReLU 操作后得到）使用最大池化的例子。 我们以 2 个元素（也叫做“步长”）滑动我们 2x2 的窗口，并在每个区域内取最大值。如上图所示，这样操作可以降低我们特征图的维度。 在下图展示的网络中，池化操作是分开应用到各个特征图的（注意，因为这样的操作，我们可以从三个输入图中得到三个输出图）。 下图展示了我们在 ReLU 操作之后得到的修正特征图的池化操作的效果： 池化函数可以逐渐降低输入表示的空间尺度。特别地，Pooling 的好处是: 使输入表示（特征维度）变得更小，并且网络中的参数和计算的数量更加可控的减小，因此，可以控制过拟合。 使网络对于输入图像中更小的变化、冗余和变换变得不变性（输入的微小冗余将不会改变池化的输出——因为我们在局部邻域中使用了最大化/平均值的操作）。 帮助我们获取图像最大程度上的尺度不变性（准确的词是“不变性”）。它非常的强大，因为我们可以检测图像中的物体，无论它们位置在哪里。 ​ 到目前为止我们了解了卷积、ReLU 和池化是如何操作的。理解这些层是构建任意 CNN 的基础是很重要的。正如下图所示，我们有两组卷积、ReLU &amp; 池化层 —— 第二组卷积层使用六个滤波器对第一组的池化层的输出继续卷积，得到一共六个特征图。接下来对所有六个特征图应用 ReLU。接着我们对六个修正特征图分别进行最大池化操作。 这些层一起就可以从图像中提取有用的特征，并在网络中引入非线性，减少特征维度，同时保持这些特征具有某种程度上的尺度变化不变性。 第二组池化层的输出作为全连接层的输入，接下来我们将介绍全连接层。 Connect全连接层是传统的多层感知器，在输出层使用的是 softmax 激活函数（也可以使用其他像 SVM 的分类器，但在本文中只使用 softmax）。「全连接」(Fully Connected) 这个词表明前面层的所有神经元都与下一层的所有神经元连接。 卷积和池化层的输出表示了输入图像的高级特征。全连接层的目的是为了使用这些特征把输入图像基于训练数据集进行分类。比如，在下面图中我们进行的图像分类有四个可能的输出结果（注意下图并没有显示全连接层的节点连接）。 除了分类，添加一个全连接层也（一般）是学习这些特征的非线性组合的简单方法。从卷积和池化层得到的大多数特征可能对分类任务有效，但这些特征的组合可能会更好。 从全连接层得到的输出概率和为 1。这个可以在输出层使用 softmax 作为激活函数进行保证。softmax 函数输入一个任意大于 0 值的矢量，并把它们转换为零一之间的数值矢量，其和为一。 Use Backpropagation to Train whole network正如上面讨论的，卷积 + 池化层的作用是从输入图像中提取特征，而全连接层的作用是分类器。 注意在下面的图中，因为输入的图像是船，对于船这一类的目标概率是 1，而其他三类的目标概率是 0，即 输入图像 = 船 目标矢量 = [0, 0, 1, 0] 完整的卷积网络的训练过程可以总结如下： 第一步：我们初始化所有的滤波器，使用随机值设置参数/权重 第二步：网络接收一张训练图像作为输入，通过前向传播过程（卷积、ReLU 和池化操作，以及全连接层的前向传播），找到各个类的输出概率 我们假设船这张图像的输出概率是 [0.2, 0.4, 0.1, 0.3] 因为对于第一张训练样本的权重是随机分配的，输出的概率也是随机的 第三步：在输出层计算总误差（计算 4 类的和） Total Error = ∑ ½ (target probability – output probability) ² 第四步：使用反向传播算法，根据网络的权重计算误差的梯度，并使用梯度下降算法更新所有滤波器的值/权重以及参数的值，使输出误差最小化 权重的更新与它们对总误差的占比有关 当同样的图像再次作为输入，这时的输出概率可能会是 [0.1, 0.1, 0.7, 0.1]，这就与目标矢量 [0, 0, 1, 0] 更接近了 这表明网络已经通过调节权重/滤波器，可以正确对这张特定图像的分类，这样输出的误差就减小了 像滤波器数量、滤波器大小、网络结构等这样的参数，在第一步前都是固定的，在训练过程中保持不变——仅仅是滤波器矩阵的值和连接权重在更新 第五步：对训练数据中所有的图像重复步骤 1 ~ 4 上面的这些步骤可以训练 ConvNet —— 这本质上意味着对于训练数据集中的图像，ConvNet 在更新了所有权重和参数后，已经优化为可以对这些图像进行正确分类。 当一张新的（未见过的）图像作为 ConvNet 的输入，网络将会再次进行前向传播过程，并输出各个类别的概率（对于新的图像，输出概率是使用已经在前面训练样本上优化分类的参数进行计算的）。如果我们的训练数据集非常的大，网络将会（有希望）对新的图像有很好的泛化，并把它们分到正确的类别中去。 注 1: 上面的步骤已经简化，也避免了数学详情，只为提供训练过程的直观内容。 注 2:在上面的例子中我们使用了两组卷积和池化层。然而请记住，这些操作可以在一个 ConvNet 中重复多次。实际上，现在有些表现最好的 ConvNet 拥有多达十几层的卷积和池化层！同时，每次卷积层后面不一定要有池化层。如下图所示，我们可以在池化操作前连续使用多个卷积 + ReLU 操作。还有，请注意 ConvNet 的各层在下图中是如何可视化的。 Visualization on CNN一般而言，越多的卷积步骤，网络可以学到的识别特征就越复杂。比如，ConvNet 的图像分类可能在第一层从原始像素中检测出边缘，然后在第二层使用边缘检测简单的形状，接着使用这些形状检测更高级的特征，比如更高层的人脸。下面的图中展示了这些内容——我们使用卷积深度置信网络学习到的特征，这张图仅仅是用来证明上面的内容（这仅仅是一个例子：真正的卷积滤波器可能会检测到对我们毫无意义的物体）。 Adam Harley 创建了一个卷积神经网络的可视化结果，使用的是 MNIST 手写数字的训练集。我强烈建议使用它来理解 CNN 的工作原理。 我们可以在下图中看到网络是如何识别输入 「8」 的。注意下图中的可视化并没有单独展示 ReLU 操作。 输入图像包含 1024 个像素（32 x 32 大小），第一个卷积层（卷积层 1）由六个独特的 5x5 （步长为 1）的滤波器组成。如图可见，使用六个不同的滤波器得到一个深度为六的特征图。 卷积层 1 后面是池化层 1，在卷积层 1 得到的六个特征图上分别进行 2x2 的最大池化（步长为 2）的操作。你可以在池化层上把鼠标移动到任意的像素上，观察在前面卷积层（如上图所示）得到的 4x4 的小格。你会发现 4x4 小格中的最大值（最亮）的像素将会进入到池化层。 池化层 1 后面的是六个 5x5 （步长为 1）的卷积滤波器，进行卷积操作。后面就是池化层 2，进行 2x2 的最大池化（步长为 2）的操作。这两层的概念和前面描述的一样。 接下来我们就到了三个全连接层。它们是： 第一个全连接层有 120 个神经元 第二层全连接层有 100 个神经元 第三个全连接层有 10 个神经元，对应 10 个数字——也就做输出层 注意在下图中，输出层中的 10 个节点的各个都与第二个全连接层的所有 100 个节点相连（因此叫做全连接）。 同时，注意在输出层那个唯一的亮的节点是如何对应于数字 “8” 的——这表明网络把我们的手写数字正确分类（越亮的节点表明从它得到的输出值越高，即，8 是所有数字中概率最高的）。 同样的 3D 可视化可以在这里看到。 Other ConvNet卷积神经网络从上世纪 90 年代初期开始出现。我们上面提到的 LeNet 是早期卷积神经网络之一。其他有一定影响力的架构如下所示： LeNet (1990s)： 本文已介绍。 1990s to 2012：在上世纪 90 年代后期至 2010 年初期，卷积神经网络进入孵化期。随着数据量和计算能力的逐渐发展，卷积神经网络可以处理的问题变得越来越有趣。 AlexNet (2012) – 在 2012，Alex Krizhevsky （与其他人）发布了 AlexNet，它是比 LeNet 更深更宽的版本，并在 2012 年的 ImageNet 大规模视觉识别大赛（ImageNet Large Scale Visual Recognition Challenge，ILSVRC）中以巨大优势获胜。这对于以前的方法具有巨大的突破，当前 CNN 大范围的应用也是基于这个工作。 ZF Net (2013) – ILSVRC 2013 的获胜者是来自 Matthew Zeiler 和 Rob Fergus 的卷积神经网络。它以 ZFNet （Zeiler &amp; Fergus Net 的缩写）出名。它是在 AlexNet 架构超参数上进行调整得到的效果提升。 GoogLeNet (2014) – ILSVRC 2014 的获胜者是来自于 Google 的 Szegedy等人的卷积神经网络。它的主要贡献在于使用了一个 Inception 模块，可以大量减少网络的参数个数（4M，AlexNet 有 60M 的参数）。 VGGNet (2014) – 在 ILSVRC 2014 的领先者中有一个 VGGNet 的网络。它的主要贡献是展示了网络的深度（层数）对于性能具有很大的影响。 ResNets (2015) – 残差网络是何凯明（和其他人）开发的，并赢得 ILSVRC 2015 的冠军。ResNets 是当前卷积神经网络中最好的模型，也是实践中使用 ConvNet 的默认选择（截至到 2016 年五月）。 DenseNet (2016 八月) – 近来由 Gao Huang （和其他人）发表的，the Densely Connected Convolutional Network 的各层都直接于其他层以前向的方式连接。DenseNet 在五种竞争积累的目标识别基准任务中，比以前最好的架构有显著的提升。可以在这里看 Torch 实现。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F03%2F13%2F%3CTensorFlow%3E%20Softmax%20Regression%20on%20MNIST%2F</url>
    <content type="text"><![CDATA[TensorFlow Chapter 2[TOC] MNIST Complete program12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061"""A very simple MNIST classifier.See extensive documentation athttp://tensorflow.org/tutorials/mnist/beginners/index.md"""from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport argparseimport sysfrom tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tfFLAGS = Nonedef main(_): # Import data mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True) # Create the model x = tf.placeholder(tf.float32, [None, 784]) W = tf.Variable(tf.zeros([784, 10])) b = tf.Variable(tf.zeros([10])) y = tf.matmul(x, W) + b # Define loss and optimizer y_ = tf.placeholder(tf.float32, [None, 10]) # The raw formulation of cross-entropy, # # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)), # reduction_indices=[1])) # # can be numerically unstable. # # So here we use tf.nn.softmax_cross_entropy_with_logits on the raw # outputs of 'y', and then average across the batch. cross_entropy = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)) train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) sess = tf.InteractiveSession() tf.global_variables_initializer().run() # Train for _ in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;) # Test trained model correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))if __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--data_dir', type=str, default='/tmp/tensorflow/mnist/input_data', help='Directory for storing input data') FLAGS, unparsed = parser.parse_known_args() tf.app.run(main=main, argv=[sys.argv[0]] + unparsed) MNIST 数据集每一个 MNIST 数据单元有两部分组成：一张包含手写数字的图片和一个对应的标签。我们把这些图片设为 “xs” ，把这些标签设为 “ys” 。训练数据集和测试数据集都包含 xs 和 ys，比如训练数据集的图片是 mnist.train.images，训练数据集的标签是 mnist.train.labels。 每一张图片包含 $28 \times 28$ 个像素点。我们可以用一个数字数组来表示这张图片： 我们把这个数组展开成一个向量，长度是 $28 \times 28 = 784$。如何展开这个数组（数字间的顺序）不重要，只要保持各个图片采用相同的方式展开。从这个角度来看，MNIST数据集的图片就是在 $784$ 维向量空间里面的点，并且拥有比较复杂的结构 (提醒: 此类数据的可视化是计算密集型的)。 展平图片的数字数组会丢失图片的二维结构信息。这显然是不理想的，最优秀的计算机视觉方法会挖掘并利用这些结构信息，我们会在后续教程中介绍。但是在这个教程中我们忽略这些结构，所介绍的简单数学模型，softmax 回归(softmax regression)，不会利用这些结构信息。 mnist.train.xs 因此，在 MNIST 训练数据集中，mnist.train.images 是一个形状为$ [60000, 784]$ 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。在此张量里的每一个元素，都表示某张图片里的某个像素的强度值，值介于 0 和 1 之间。 mnist.train.ys 相对应的 MNIST 数据集的标签是介于 0 到 9 的数字，用来描述给定图片里表示的数字。为了用于这个教程，我们使标签数据是”one-hot vectors”。 一个 one-hot 向量除了某一位的数字是 1 以外其余各维度数字都是 0。所以在此教程中，数字 n 将表示成一个只有在第 n 维度（从 0 开始）数字为 1 的 10 维向量。比如，标签 0 将表示成([1,0,0,0,0,0,0,0,0,0,0])。因此， mnist.train.labels 是一个$[60000, 10]$ 的数字矩阵。 Softmax Regression对于 softmax Regression 模型可以用下面的图解释，对于输入的 xs 加权求和，再分别加上一个偏置量，最后再输入到 softmax 函数中： 如果把它写成一个等式，我们可以得到： 我们也可以用向量表示这个计算过程：用矩阵乘法和向量相加。这有助于提高计算效率。（这同样也是一种更有效的思考方式） Cross-Entropy为了训练我们的模型，我们首先需要定义一个指标来评估这个模型是好是坏。在 ML 中我们通常使用成本（cost）或损失（loss）函数来表示我们的模型离最好的理想模型差距有多大，然后尽可能地最小化这个指标。 一个常用的成本函数就是交叉熵（cross-entropy）。比较浅显地解释是，交叉熵用来衡量我们的模型预测值偏离实际真实值的程度大小。 最常见的例子就是在机器学习领域中的辨识问题（Classification）。假设说我们要训练机器辨识图片中的动物是狗还是猫，我们训练的模型可能会告诉我说这有 80% 机会是狗，20% 的机会是猫。假设正确的答案是狗，那么我们的模型有 80% 机率可以辨识出图片是狗到底够不够好? 如果是 85% 的话到底又好多少? 为了要让我们的模型最佳化，这个问题显得非常重要——因为我们需要一个方法来衡量我们模型的好坏。至于我们要最佳化什么，是需要根据这个模型最终是要被拿来做什么应用。这通常不会有一定的答案，结论是在某些时候，交叉熵就是我们所在乎的，而很多时候我们不知道训练出来的这个模型到底是如何被应用的时候，交叉熵就是一个很好的衡量标准。$$CrossEntropy = H_{y’}(y)= - \sum_i y’_i \cdot \log( yi )$$简单地讲，交叉熵描述的是模型预测值 $y’{i}$ 偏离实际真实值 $y_{i}$ 的程度大小。 当年香农 Shannon 创立信息论的时候，考虑的是每一次都是扔硬币，结果只有 2 个可能，所以用的是以 2 为底，发明了 bit 计量单位。而在 Tensorflow 软件里的实现，则是以 e 为底的 log。 tf.nn.softmax_cross_entropy_with_logitsTensorflow 中有个经常用到的函数叫做 tf.nn.softmax_cross_entropy_with_logits。这个函数的实现并不在 Python 中，所以我用 Numpy 实现一个同样功能的函数进行比对，确认它使用的是以 e 为底的log。理由很简单，因为 Softmax 函数里使用了 e 的指数，所以当 Cross Entropy 也使用以 e 的 log，然后这两个函数放到一起实现，可以进行很好的性能优化。 1234567891011121314151617181920212223242526272829303132333435363738394041424344import tensorflow as tfimport numpy as np# Make up some testing data, need to be rank 2x = np.array([ [0.,2.,1.], [0.,0.,2.] ])label = np.array([ [0.,0.,1.], [0.,0.,1.] ])# Numpy part #def softmax(logits): sf = np.exp(logits) sf = sf/np.sum(sf, axis=1).reshape(-1,1) return sfdef cross_entropy(softmax, labels): return -np.sum( labels * np.log(softmax), axis=1 )def loss(cross_entropy): return np.mean( cross_entropy )numpy_result = loss(cross_entropy( softmax(x), label ))print(numpy_result)# Tensorflow part #g = tf.Graph()with g.as_default(): tf_x = tf.constant(x) tf_label = tf.constant(label) # labels 真实值 logits 预测值 tf_ret = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits = tf_x, labels = tf_label) )with tf.Session(graph=g) as ss: tensorflow_result = ss.run([tf_ret])print(tensorflow_result) tf.train.GradientDescentOptimizer1train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) 我们使用 Tensorflow 中的 Gradient Descent algorithm 以 0.5 的学习速率最小化交叉熵。梯度下降算法是一个简单的学习过程，Tensorflow 只需要将每个变量一点点地往使成本不断降低的方向移动。当然 Tensorflow 也提供了其他许多优化算法 ，我们所做的只需要简单地调整一行代码就可以使用其他的算法。 TensorFlow在这里实际上所做的是，它会在后台给你描述的计算图里面增加一系列新的计算操作单元用于实现反向传播算法和梯度下降算法。然后，它返回给你的只是一个单一的操作，当运行这个操作时，它用梯度下降算法训练你的模型，微调你的变量，不断减少成本。 123for _ in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;) 该循环的每个步骤中，我们都会随机抓取训练数据中的 100 个批处理数据点，然后我们用这些数据点作为参数替换之前的占位符来运行得到 train_step 的值。 使用一小部分的随机数据来进行训练被称为随机训练（stochastic training）- 在这里更确切的说是随机梯度下降训练。在理想情况下，我们希望用我们所有的数据来进行每一步的训练，因为这能给我们更好的训练结果，但显然这需要很大的计算开销。所以，每一次训练我们可以使用不同的数据子集，这样做既可以减少计算开销，又可以最大化地学习到数据集的总体特性。 tf.InteractiveSession这里，我们使用更加方便的InteractiveSession类。通过它，我们可以更加灵活地构建我们的代码。它能让我们在运行图的时候，插入一些计算图，这些计算图是由某些操作（operations）构成的。这对于工作在交互式环境中的人们来说非常便利，比如使用IPython。如果你没有使用InteractiveSession，那么你需要在启动 session 之前构建整个计算图，然后启动该计算图。12import tensorflow as tfsess = tf.InteractiveSession() Evaluating Our Modeltf.argmax1correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) tf.argmax 是一个非常有用的函数，它能给出某个 tensor 对象在某一维上的其数据最大值所在的索引值。由于标签向量是由 0 和 1组成，因此最大值 1 所在的索引位置就是类别标签，比如 tf.argmax(y, 1) 返回的是模型对任一输入 x 预测到的标签值，而 tf.argmax(y_, 1) 代表正确的标签，我们可以用 tf.equal 来检测我们的预测是否真实标签匹配（索引位置一样表示匹配）。其返回的是给我们一组布尔值，为了确定正确预测项的比例，我们可以把布尔值转换成浮点数，然后取平均值。例如，[True, False, True, True] 会变成[1, 0, 1, 1] ，取平均值后得到 0.75。 1accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float")) tf.cast 数据类型转换。 这个最终结果值应该大约是 91%。 这个结果好吗？嗯，并不太好。事实上，这个结果是很差的。这是因为我们仅仅使用了一个非常简单的模型。不过，做一些小小的改进，我们就可以得到97％的正确率。最好的模型甚至可以获得超过99.7％的准确率！（想了解更多信息，可以看看这个关于各种模型的性能对比列表。)]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F03%2F01%2F%3CTensorFlow%3E%20Dropout%20Introduction%2F</url>
    <content type="text"><![CDATA[Dropout开篇明义，dropout 是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个 mini-batch 都在训练不同的网络。 Dropout 是 CNN 中防止过拟合提高效果的一个大杀器，但对于其为何有效，却众说纷纭。在下读到两篇代表性的论文，代表两种不同的观点，特此分享给大家。 组合派参考文献中第一篇中的观点，Hinton 老大爷提出来的，关于 Hinton 在深度学习界的地位我就不再赘述了，光是这地位，估计这一派的观点就是“武当少林”了。注意，派名是我自己起的，各位勿笑。 观点该论文从神经网络的难题出发，一步一步引出 dropout 为何有效的解释。大规模的神经网络有两个缺点： 费时 容易过拟合 这两个缺点真是抱在深度学习大腿上的两个大包袱，一左一右，相得益彰，额不，臭气相投。过拟合是很多机器学习的通病，过拟合了，得到的模型基本就废了。而为了解决过拟合问题，一般会采用 ensemble 方法，即训练多个模型做组合，此时，费时就成为一个大问题，不仅训练起来费时，测试起来多个模型也很费时。总之，几乎形成了一个死锁。 Dropout 的出现很好的可以解决这个问题，每次做完 dropout，相当于从原始的网络中找到一个更瘦的网络，如下图所示： 因而，对于一个有 $N$ 个节点的神经网络，有了 dropout 后，就可以看做是 $2^n$ 个模型的集合了，但此时要训练的参数数目却是不变的，这就解脱了费时的问题。 动机论虽然直观上看 dropout 是 ensemble 在分类性能上的一个近似，然而实际中，dropout 毕竟还是在一个神经网络上进行的，只训练出了一套模型参数。那么他到底是因何而有效呢？这就要从动机上进行分析了。论文中作者对dropout 的动机做了一个十分精彩的类比： 在自然界中，在中大型动物中，一般是有性繁殖，有性繁殖是指后代的基因从父母两方各继承一半。但是从直观上看，似乎无性繁殖更加合理，因为无性繁殖可以保留大段大段的优秀基因。而有性繁殖则将基因随机拆了又拆，破坏了大段基因的联合适应性。 但是自然选择中毕竟没有选择无性繁殖，而选择了有性繁殖，须知物竞天择，适者生存。我们先做一个假设，那就是基因的力量在于混合的能力而非单个基因的能力。不管是有性繁殖还是无性繁殖都得遵循这个假设。为了证明有性繁殖的强大，我们先看一个概率学小知识。 比如要搞一次恐怖袭击，两种方式： 集中 50 人，让这 50 个人准确分工，搞一次大爆破。 将 50 人分成10组，每组 5 人，分头行事，去随便什么地方搞点动作，成功一次就算。 哪一个成功的概率比较大？ 显然是后者。因为将一个大团队作战变成了游击战。 那么，类比过来，有性繁殖的方式不仅仅可以将优秀的基因传下来，还可以降低基因之间的联合适应性，使得复杂的大段大段基因联合适应性变成比较小的一个一个小段基因的联合适应性。 Dropout 也能达到同样的效果，它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。 个人补充一点：那就是植物和微生物大多采用无性繁殖，因为他们的生存环境的变化很小，因而不需要太强的适应新环境的能力，所以保留大段大段优秀的基因适应当前环境就足够了。而高等动物却不一样，要准备随时适应新的环境，因而将基因之间的联合适应性变成一个一个小的，更能提高生存的概率。 Dropout带来的模型的变化而为了达到 ensemble 的特性，有了 dropout 后，神经网络的训练和预测就会发生一些变化。 训练层面 无可避免的，训练网络的每个单元要添加一道概率流程。 对应的公式变化如下如下： 没有 dropout 的神经网络$$\begin{align}z{i}^{(l+1)} &amp; = w{i}^{(l+1)}y^{l} + b{i}^{(l+1)} \cry{i}^{(l+1)} &amp; = f(z_{i}^{(l+1)})\end{align}$$​ 有 dropout 的神经网络$$\begin{align}r{i}^{(l)} &amp; \sim Bernoulli(p) \cr\tilde{y}^{(l)} &amp; = r^{(l)}*y^{(l)}\crz{i}^{(l+1)} &amp; = w{i}^{(l+1)} \tilde{y}^{l} + b{i}^{(l+1)} \cry{i}^{(l+1)} &amp; = f(z{i}^{(l+1)})\end{align}$$​ 测试层面 预测的时候，每一个单元的参数要预乘以 $p$。 论文中的其他技术点 防止过拟合的方法： 提前终止（当验证集上的效果变差的时候） L1 和 L2 正则化加权 soft weight sharing dropout dropout率的选择 经过交叉验证，隐含节点 dropout 率等于 0.5 的时候效果最好，原因是 0.5 的时候 dropout 随机生成的网络结构最多。 Dropout 也可以被用作一种添加噪声的方法，直接对 input 进行操作。输入层设为更接近 1 的数。使得输入变化不会太大（0.8）。 训练过程 对参数 w 的训练进行球形限制(max-normalization)，对 dropout 的训练非常有用。 球形半径 c 是一个需要调整的参数。可以使用验证集进行参数调优。 Dropout 自己虽然也很牛，但是 dropout、max-normalization、large decaying learning rates and high momentum 组合起来效果更好，比如 max-norm regularization 就可以防止大的 learning rate 导致的参数 blow up。 使用 pretraining 方法也可以帮助 dropout 训练参数，在使用 dropout 时，要将所有参数都乘以 $\frac{1}{p}$。 部分实验结论 该论文的实验部分很丰富，有大量的评测数据。 maxout 神经网络中得另一种方法，Cifar-10上超越 dropout 文本分类上，dropout 效果提升有限，分析原因可能是 Reuters-RCV1 数据量足够大，过拟合并不是模型的主要问题 dropout 与其他 standerd regularizers 的对比 L2 weight decay lasso KL-sparsity max-norm regularization dropout 特征学习 标准神经网络，节点之间的相关性使得他们可以合作去 fix 其他节点中得噪声，但这些合作并不能在 unseen data 上泛化，于是，过拟合，dropout 破坏了这种相关性。在 autoencoder 上，有 dropout 的算法更能学习有意义的特征（不过只能从直观上，不能量化）。 产生的向量具有稀疏性。 保持隐含节点数目不变，dropout 率变化；保持激活的隐节点数目不变，隐节点数目变化。 数据量小的时候，dropout 效果不好，数据量大了，dropout 效果好 模型均值预测 使用 weight-scaling 来做预测的均值化 使用 mente-carlo 方法来做预测。即对每个样本根据 dropout 率先 sample 出来 k 个 net，然后做预测，k 越大，效果越好。 Multiplicative Gaussian Noise 使用高斯分布的 dropou t而不是伯努利模型 dropout dropout 的缺点就在于训练时间是没有 dropout 网络的 2-3 倍。 &gt; 进一步需要了解的知识点 dropout RBM Marginalizing Dropout 具体来说就是将随机化的 dropout 变为确定性的，比如对于 Logistic 回归，其 dropout 相当于加了一个正则化项 Bayesian neural network 对稀疏数据特别有用，比如 medical diagnosis, genetics, drug discovery and other computational biology applications 噪声派参考文献中第二篇论文中得观点，也很强有力。 观点观点十分明确，就是对于每一个 dropout 后的网络，进行训练时，相当于做了 Data Augmentation，因为，总可以找到一个样本，使得在原始的网络上也能达到 dropout 单元后的效果。 比如，对于某一层，dropout 一些单元后，形成的结果是(1.5,0,2.5,0,1,2,0)，其中 0 是被 drop 的单元，那么总能找到一个样本，使得结果也是如此。这样，每一次 dropout 其实都相当于增加了样本。 稀疏性知识点A首先，先了解一个知识点： When the data points belonging to a particular class are distributed along a linear manifold, or sub-space, of the input space, it is enough to learn a single set of features which can span the entire manifold. But when the data is distributed along a highly non-linear and discontinuous manifold, the best way to represent such a distribution is to learn features which can explicitly represent small local regions of the input space, effectively “tiling” the space to define non-linear decision boundaries. 大致含义就是： 在线性空间中，学习一个整个空间的特征集合是足够的，但是当数据分布在非线性不连续的空间中得时候，则学习局部空间的特征集合会比较好。 知识点B假设有一堆数据，这些数据由 $M$ 个不同的非连续性簇表示，给定 $K$ 个数据。那么一个有效的特征表示是将输入的每个簇映射为特征以后，簇之间的重叠度最低。使用 $A$ 来表示每个簇的特征表示中激活的维度集合。重叠度是指两个不同的簇的 $A{i}$ 和 $A{j}$ 之间的 Jaccard 相似度最小，那么： 当 $K$ 足够大时，即便 $A$ 也很大，也可以学习到最小的重叠度 当 $K$ 小，$M$ 大时，学习到最小的重叠度的方法就是减小 $A$ 的大小，也就是稀疏性 上述的解释可能是有点太专业化，比较拗口。主旨意思是这样，我们要把不同的类别区分出来，就要是学习到的特征区分度比较大，在数据量足够的情况下不会发生过拟合的行为，不用担心。但当数据量小的时候，可以通过稀疏性，来增加特征的区分度。 因而有意思的假设来了，使用了 dropout 后，相当于得到更多的局部簇，同等的数据下，簇变多了，因而为了使区分性变大，就使得稀疏性变大。 为了验证这个数据，论文还做了一个实验，如下图： 该实验使用了一个模拟数据，即在一个圆上，有 15000 个点，将这个圆分为若干个弧，在一个弧上的属于同一个类，一共 10 个类，即不同的弧也可能属于同一个类。改变弧的大小，就可以使属于同一类的弧变多。 实验结论就是当弧长变大时，簇数目变少，稀疏度变低。与假设相符合。 个人观点：该假设不仅仅解释了 dropout 何以导致稀疏性，还解释了 dropout 因为使局部簇的更加显露出来，而根据知识点 A 可得，使局部簇显露出来是 dropout 能防止过拟合的原因，而稀疏性只是其外在表现。 论文中的其他技术知识点 将 dropout 映射回得样本训练一个完整的网络，可以达到 dropout 的效果。 Dropout 由固定值变为一个区间，可以提高效果。 将 dropout 后的表示映射回输入空间时，并不能找到一个样本 x* 使得所有层都能满足 dropout 的结果，但可以为每一层都找到一个样本，这样，对于每一个 dropout，都可以找到一组样本可以模拟结果。 dropout对应的还有一个dropConnect，公式如下： dropout$$h{n} = \overrightarrow{w{n}}^{T}(\overrightarrow{r} \odot \overrightarrow{x}) + b_{n}$$​ dropConnect$$h{n} = (\overrightarrow{r{n}} \odot \overrightarrow{w{n}})^{T}\overrightarrow{x} + b{n}$$​ 试验中，纯二值化的特征的效果也非常好，说明了稀疏表示在进行空间分区的假设是成立的，一个特征是否被激活表示该样本是否在一个子空间中。 参考文献[1]. Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: A simple way to prevent neural networks from overfitting[J]. The Journal of Machine Learning Research, 2014, 15(1): 1929-1958. [2]. Dropout as data augmentation. http://arxiv.org/abs/1506.08700]]></content>
  </entry>
</search>