<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[「TensorFlow」 Use WeChat to Monitor Your Network]]></title>
    <url>%2F2017%2F03%2F13%2F%3CTensorflow%3E%20Use%20WeChat%20to%20Monitor%20Your%20Network%2F</url>
    <content type="text"><![CDATA[平时，大家自己的机器模型在训练期间（特别是深度网络），训练时间通常几小时到十几小时不等，甚至可能会花上好几天，那么在这段时间，你们又会干些什么事情呢？ 作为程序员，这里提供一个「有趣的」方式，用你的微信来监控你的模型在训练期间的一举一动。 &lt;!-- more --&gt; 大概的效果是： 程序用到的主角是 Python 中的微信个人号接口 itchat。What's itchat? （itchat 的介绍及安装过程） 这次，我们要监控的模型是先前提到过的 基于 MNIST 手写体数据集的「CNN」模型 。 注意： 文章要求读者事先下载安装好 itchat。 文章不会详细介绍 TensorFlow 以及 Tensorboard 的知识。 Environment OS: macOS Sierra 10.12.x Python Version: 3.4.x TensorFlow: 1.0 itchat: 1.2.3 Code Use WeChat to Monitor Your Network（tensorboard 绘图） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259# 基于 MNIST 数据集 的 「CNN」（tensorboard 绘图）from tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tfimport numpy as npimport scipy# Import itchat &amp; threadingimport itchatimport threading# Create a running status flaglock = threading.Lock()running = False# Parameterslearning_rate = 0.001training_iters = 200000batch_size = 128display_step = 10def weight_variable(shape): initial = tf.truncated_normal(shape, stddev = 0.1) return tf.Variable(initial) def bias_variable(shape): initial = tf.constant(0.1, shape = shape) return tf.Variable(initial)def conv2d(x, W, strides=1): return tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')def max_pool_2x2(x, k=2): return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME') def variable_summaries(var): """Attach a lot of summaries to a Tensor (for TensorBoard visualization).""" with tf.name_scope('summaries'): mean = tf.reduce_mean(var) tf.summary.scalar('mean', mean) with tf.name_scope('stddev'): stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean))) tf.summary.scalar('stddev', stddev) tf.summary.scalar('max', tf.reduce_max(var)) tf.summary.scalar('min', tf.reduce_min(var)) tf.summary.histogram('histogram', var)def add_layer(input_tensor, weights_shape, biases_shape, layer_name, act = tf.nn.relu, flag = 1): """Reusable code for making a simple neural net layer. It does a matrix multiply, bias add, and then uses relu to nonlinearize. It also sets up name scoping so that the resultant graph is easy to read, and adds a number of summary ops.""" with tf.name_scope(layer_name): with tf.name_scope('weights'): weights = weight_variable(weights_shape) variable_summaries(weights) with tf.name_scope('biases'): biases = bias_variable(biases_shape) variable_summaries(biases) with tf.name_scope('Wx_plus_b'): if flag == 1: preactivate = tf.add(conv2d(input_tensor, weights), biases) else: preactivate = tf.add(tf.matmul(input_tensor, weights), biases) tf.summary.histogram('pre_activations', preactivate) if act == None: outputs = preactivate else: outputs = act(preactivate, name = 'activation') tf.summary.histogram('activation', outputs) return outputsdef nn_train(wechat_name, param): global lock, running # Lock with lock: running = True # 参数 learning_rate, training_iters, batch_size, display_step = param # Import data mnist_data_path = 'MNIST_data/' mnist = input_data.read_data_sets(mnist_data_path, one_hot = True) # Network Parameters n_input = 28*28 # MNIST data input (img shape: 28*28) n_classes = 10 # MNIST total classes (0-9 digits) dropout = 0.75 # Dropout, probability to keep units with tf.name_scope('Input'): x = tf.placeholder(tf.float32, [None, n_input], name = 'input_x') y_ = tf.placeholder(tf.float32, [None, n_classes], name = 'target_y') keep_prob = tf.placeholder(tf.float32, name = 'keep_prob') #dropout (keep probability) def cnn_net(x, weights, biases, dropout): # Reshape input picture x_image = tf.reshape(x, [-1, 28, 28 ,1]) # First Convolutional Layer conv_1 = add_layer(x_image, weights['conv1_w'], biases['conv1_b'], 'First_Convolutional_Layer', flag = 1) # First Pooling Layer pool_1 = max_pool_2x2(conv_1) # Second Convolutional Layer conv_2 = add_layer(pool_1, weights['conv2_w'], biases['conv2_b'], 'Second_Convolutional_Layer', flag = 1) # Second Pooling Layer pool_2 = max_pool_2x2(conv_2) # Densely Connected Layer pool_2_flat = tf.reshape(pool_2, [-1, weight_variable(weights['dc1_w']).get_shape().as_list()[0]]) dc_1 = add_layer(pool_2_flat, weights['dc1_w'], biases['dc1_b'], 'Densely_Connected_Layer', flag = 0) # Dropout dc_1_drop = tf.nn.dropout(dc_1, keep_prob) # Readout Layer y = add_layer(dc_1_drop, weights['out_w'], biases['out_b'], 'Readout_Layer', flag = 0) return y # Store layers weight &amp; bias weights = &#123; # 5x5 conv, 1 input, 32 outputs 'conv1_w': [5, 5, 1, 32], # 5x5 conv, 32 inputs, 64 outputs 'conv2_w': [5, 5, 32, 64], # fully connected, 7*7*64 inputs, 1024 outputs 'dc1_w': [7*7*64, 1024], # 1024 inputs, 10 outputs (class prediction) 'out_w': [1024, n_classes] &#125; biases = &#123; 'conv1_b': [32], 'conv2_b': [64], 'dc1_b': [1024], 'out_b': [n_classes] &#125; y = cnn_net(x, weights, biases, dropout) # Optimizer with tf.name_scope('cost'): cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = y)) tf.summary.scalar('cost', cost) tf.summary.histogram('cost', cost) # Train with tf.name_scope('train'): optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Test with tf.name_scope('accuracy'): with tf.name_scope('correct_prediction'): correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) with tf.name_scope('accuracy'): accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) tf.summary.scalar('accuracy', accuracy) sess = tf.InteractiveSession() merged = tf.summary.merge_all() train_writer = tf.summary.FileWriter('train/', sess.graph) test_writer = tf.summary.FileWriter('test/') tf.global_variables_initializer().run() # Train the model, and also write summaries. # Every 10th step, measure test-set accuracy, and write test summaries # All other steps, run train_step on training data, &amp; add training summaries # Keep training until reach max iterations print('Wait for lock') with lock: run_state = running print('Start') step = 1 while step * batch_size &lt; training_iters and run_state: batch_x, batch_y = mnist.train.next_batch(batch_size) # Run optimization op (backprop) sess.run(optimizer, feed_dict = &#123;x: batch_x, y_: batch_y, keep_prob: dropout&#125;) if step % display_step == 0: # Record execution stats run_options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE) run_metadata = tf.RunMetadata() summary, _ = sess.run([merged, optimizer], feed_dict = &#123;x: batch_x, y_: batch_y, keep_prob: 1.&#125;, options = run_options, run_metadata = run_metadata) train_writer.add_run_metadata(run_metadata, 'step %d' % step) train_writer.add_summary(summary, step) print('Adding run metadata for', step) summary, loss, acc = sess.run([merged, cost, accuracy], feed_dict = &#123;x: batch_x, y_: batch_y, keep_prob: 1.&#125;) print("Iter" + str(step*batch_size) + ", Minibatch Loss=" + \ "&#123;:.6f&#125;".format(loss) + ", Training Accuracy=" + \ "&#123;:.5f&#125;".format(acc)) itchat.send("Iter" + str(step*batch_size) + ", Minibatch Loss=" + \ "&#123;:.6f&#125;".format(loss) + ", Training Accuracy=" + \ "&#123;:.5f&#125;".format(acc), 'filehelper') else: summary, _ = sess.run([merged, optimizer], feed_dict = &#123;x: batch_x, y_: batch_y, keep_prob: 1.&#125;) train_writer.add_summary(summary, step) step += 1 with lock: run_state = running print("Optimization Finished!") itchat.send("Optimization Finished!", 'filehelper') # Calculate accuracy for 256 mnist test images summary, acc = sess.run([merged, accuracy], feed_dict = &#123;x: mnist.test.images[:256], y_: mnist.test.labels[:256], keep_prob: 1.&#125; ) text_writer.add_summary(summary) print("Testing Accuracy:", acc) itchat.send("Testing Accuracy: %s" % acc, wechat_name) @itchat.msg_register([itchat.content.TEXT])def chat_trigger(msg): global lock, running, learning_rate, training_iters, batch_size, display_step if msg['Text'] == u'开始': print('Starting') with lock: run_state = running if not run_state: try: threading.Thread(target=nn_train, args=(msg['FromUserName'], (learning_rate, training_iters, batch_size, display_step))).start() except: msg.reply('Running') elif msg['Text'] == u'停止': print('Stopping') with lock: running = False elif msg['Text'] == u'参数': itchat.send('lr=%f, ti=%d, bs=%d, ds=%d'%(learning_rate, training_iters, batch_size, display_step),msg['FromUserName']) else: try: param = msg['Text'].split() key, value = param print(key, value) if key == 'lr': learning_rate = float(value) elif key == 'ti': training_iters = int(value) elif key == 'bs': batch_size = int(value) elif key == 'ds': display_step = int(value) except: passif __name__ == '__main__': itchat.auto_login(hotReload=True) itchat.run() 大家可以看到，我对先前的代码进行了一些修改。 下面我会对代码中用到 itchat 的部分进行一些简短的说明。 代码部分截图： 说明： 首先我导入了 itchat 和 threading。 在原先所有 print 消息的地方，都添加了 itchat.send() 来输出我们的模型训练日志。 加了一个带锁的状态量 running 用来做为发送微信消息的运行开关。 写了一个 itchat 的 handler（就是上图）。其作用就是当程序运行，我们需要在微信中，对自己的微信号发送「开始」，模型才会开始训练，为了防止信息阻塞，所以要用到 threading 将其放在另一个线程当中。在训练的过程中，如果我们觉得结果已到达我们自己的预期，可以微信发送「停止」来停止模型的训练过程。 另外，脚本刚开始运行时，程序会弹出一个包含二维码的图片，我们需要通过微信来扫描该二维码，来登陆微信并启动 itchat 的服务。 程序是包含了 Tensorboard 绘图的，所以等模型训练好，我们依然是可以通过 Tensorboard 来更加详细地查看我们模型的训练过程。 至此，我们就可以一边通过微信来监控我们的模型训练过程，一边与身边的朋友们谈笑风生了。 如果看过 itchat 那个连接的读者，可以了解到 itchat 同样是可以发送图片信息的，所以我们可以写额外的脚本在训练的过程中每隔 100 次迭代， plot 到目前为止 loss，acc 等指标的趋势图。在此，我就不再进行拓展了。 关于各个模块的作用，以及各个变量的意义，我在此就不再赘述了。 如果有读者对于 CNN 卷积神经网络有些陌生或者是遗忘，可以参考我的另外一篇文章 CNN on TensorFlow。 如果读者对 Tensorboard 有所遗忘，可以参考我的另一篇文章 「TensorFlow 1.0」 Tensorboard。]]></content>
      <categories>
        <category>TensorFlow</category>
        <category>WeChat</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>TensorFlow</tag>
        <tag>WeChat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[「Computer Vision」 JPEG Compression]]></title>
    <url>%2F2016%2F12%2F14%2F%3CComputer%20Vision%3E%20JPEG%20Compression%2F</url>
    <content type="text"><![CDATA[Preview 本文介绍了关于信息隐藏中的图像 JPEG 压缩标准方法，JPEG 是一种针对照片视频而广泛使用的一种 有损压缩 标准方法。 &lt;!-- more --&gt; Related 颜色模式转换: RBG to YUV 采样 &amp; 分块 DCT 离散余弦变换 量化 DPCM 差分编码（对 DC 系数） RLE 游程编码（对 AC 系数）：Zig-Zag 扫描 熵编码：Huffman 编码 颜色模式转换 JPEG 采用的是 YCrCb 颜色空间，而 BMP 采用的是 RGB 颜色空间，要想对 BMP 图片进行压缩，首先需要进行颜色空间的转换。YCrCb 颜色空间中，$Y$ 代表亮度，$Cr$，$Cb$ 则代表色度和饱和度（也有人将 $Cb$，$Cr$ 两者统称为色度），三者通常以 $Y$，$U$，$V$ 来表示，即用 $U$ 代表 $Cb$，用 $V$ 代表 $Cr$。RGB 和 YCrCb 之间的转换关系如下所示： $$ \begin{align} Y &amp;= &amp;0.299R &amp;+ 0.587G + 0.114B \cr Cb &amp;= &amp;-0.168R &amp;- 0.331G + 0.500B + 128 \cr Cr &amp;= &amp;0.500R &amp;- 0.418G - 0.081B + 128 \end{align} $$ 采样 研究发现，人眼对亮度变换的敏感度要比对色彩变换的敏感度高出很多。 因此，我们可以认为 Y 分量要比 Cb，Cr 分量重要的多。在 BMP 图片中，RGB 三个分量各采用一个字节进行采样，也就是我们常听到的 RGB-888 的模式；而 JPEG 图片中，通常采用两种采样方式：YUV-411 和 YUV-422，它们所代表的意义是 Y，Cb，Cr 三个分量的数据取样比例一般是 4：1：1 或者 4：2：2（4：1：1 含义就是：在 $2\times 2$ 的单元中，本应分别有 4 个 Y，4 个 U，4 个 V 值，用 12 个字节进行存储。经过 4:1:1 采样处理后，每个单元中的值分别有 4 个 Y、1 个 U、1 个 V，只要用 6 个字节就可以存储了） 这样的采样方式，虽然损失了一定的精度但也在人眼不太察觉到的范围内减小了数据的存储量。当然，JPEG 格式里面也允许将每个点的 U，V 值都记录下来。 分块 由于后面的 DCT 变换是是对 $8 \times 8$ 的子块进行处理的，因此，在进行 DCT 变换之前必须把源图象数据进行分块。源图象中每点的 3 个分量是交替出现的，先要把这 3 个分量分开，存放到 3 张表中去。然后由左及右，由上到下依次读取 $8 \times 8$ 的子块，存放在长度为 64 的表中，即可以进行 DCT 变换。 注意，编码时，程序从源数据中读取一个 $8 \times 8$ 的数据块后，进行 DCT 变换，量化，编码，然后再读取、处理下一个 $8 \times 8$ 的数据块。 JPEG 编码是以每 $8 \times 8$ 个点为一个单位进行处理的. 所以如果原始图片的长宽不是 8 的倍数， 都需要先补成 8 的倍数， 使其可以进行一块块的处理。将原始图像数据分为 $8 \times 8$ 的数据单元矩阵之后，还必须将每个数值减去 128，然后一一带入 DCT 变换公式，即可达到 DCT 变换的目的。图像的数据值必须减去 128，是因为 DCT 公式所接受的数字范围是 -128 到 127 之间。 DCT 离散余弦变化 DCT（Discrete Cosine Transform，离散余弦变换），是码率压缩中常用的一种变换编码方法。任何连续的实对称函数的傅里叶变换中只含有余弦项，因此，余弦变换同傅里叶变换一样具有明确的物理意义。DCT 是先将整体图像分成 $N \times N$ 的像素块，然后针对 $N \times N$ 的像素块逐一进行 DCT 操作。需要提醒的是，JPEG 的编码过程需要进行正向离散余弦变换，而解码过程则需要反向离散余弦变换。 二维 DCT 变换公式： $$ F(u,v)=c(u)c(v)\sum_{i=0}^{N-1} \sum_{j=0}^{N-1} f(i,j) $$ $$ c(u)= \begin{cases} \sqrt{\frac 1N},&amp; u = 0 \cr \sqrt{2 \over N},&amp; u \neq 0 \cr \end{cases} $$ ### 二维 DCT 逆变换公式： $$ f(i,j)=\sum_{u=0}^{N-1}\sum_{v=0}^{N-1}c(u)c(v)F(u,v)\cos \left[\frac{(i+0.5)\pi}{N}u\right]\cos \left[\frac{(j+0.5)\pi}{N}v\right] $$ $$ c(u)= \begin{cases} \sqrt{\frac 1N},&amp; u = 0 \cr \sqrt{2 \over N},&amp; u \neq 0 \cr \end{cases} $$ 这里的 N 是水平、垂直方向的像素数目，一般取值为 8。$8 \times 8$ 的二维像素块经过 DCT 操作之后，就得到了 $8 \times 8$ 的变换系数矩阵。这些系数，都有具体的物理含义。 例如，U=0，V=0 时的 F(0,0) 是原来的 64 个数据的均值，相当于直流分量，也有人称之为 DC 系数或者直流系数。随着 U，V 的增加，相另外的 63 个系数则代表了水平空间频率和垂直空间频率分量（高频分量）的大小，多半是一些接近于 0 的正负浮点数，我们称之为交流系数 AC。 DCT 变换后的 $8 \times 8$ 的系数矩阵中，低频分量集中在矩阵的左上角。高频成分则集中在右下角。这里，我们暂时先只考虑水平方向上一行数据（8 个像素）的情况时的 DCT 变换，从而来说明其物理意义。如下图所示： 原始的图像信号（最左边的波形）经过 DCT 变换之后变成了 8 个波，其中第一个波为直流成分，其余 7 个为交流成分。可见图像信号被分解为直流成分和一些从低频到高频的各种余弦成分。而 DCT 系数只表示了该种成分所占原图像信号的份额大小。显然，恢复图像信息可以表示为下面的式子： $$ F(n) = C(n) \cdot E(n)。 $$ 这里，E(n) 是一个基底，C(n) 是 DCT 系数，F(n) 则是图像信号；如果考虑垂直方向的变化，那就需要一个二维的基底。大学里面的信号处理，傅里叶变换等课程上也讲过，任何信号都可以被分解为基波和不同幅度的谐波的组合，而 DCT 变换的物理意义也正是如此。 由于大多数图像的高频分量比较小，相应的图像高频分量的 DCT 系数经常接近于 0，再加上高频分量中只包含了图像的细微的细节变化信息，而人眼对这种高频成分的失真不太敏感，所以，可以考虑将这一些高频成分予以抛弃，从而降低需要传输的数据量。这样一来，传送 DCT 变换系数的所需要的编码长度要远远小于传送图像像素的编码长度。到达接收端之后通过反离散余弦变换就可以得到原来的数据，虽然这么做存在一定的失真，但人眼是可接受的，而且对这种微小的变换是不敏感的。所谓 JPEG 的有损压缩，损的是量化过程中的高频部分，romove 50% 的高频信息可能对于编码信息只损失了 5%。]]></content>
      <categories>
        <category>Computer Vision</category>
      </categories>
      <tags>
        <tag>Computer Vision</tag>
        <tag>Python</tag>
      </tags>
  </entry>
</search>